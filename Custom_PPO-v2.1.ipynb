{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Storage 2.0\n",
    "Numpy Version with full-batch return of epoch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward_sum(rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return sum(discounted_rewards)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage2:\n",
    "\n",
    "    def __init__(self, observation_dimension, size):\n",
    "        self.observations = np.zeros((size, observation_dimension), dtype=np.float32)\n",
    "        self.actions = np.zeros(size, dtype=np.int32)\n",
    "        self.rewards = np.zeros(size, dtype=np.float32)\n",
    "        self.episode_return = np.zeros(size, dtype=np.float32)\n",
    "        self.baseline_estimates = np.zeros(size, dtype=np.float32)\n",
    "        self.pointer_start, self.pointer_end= 0,0\n",
    "        \n",
    "\n",
    "    def store(self, observation, action, reward, baseline_estimate):\n",
    "        self.observations[self.pointer_end] = observation\n",
    "        self.actions[self.pointer_end] = action\n",
    "        self.rewards[self.pointer_end] = reward\n",
    "        self.baseline_estimates[self.pointer_end] = baseline_estimate\n",
    "        self.pointer_end += 1\n",
    "\n",
    "    def conclude_episode(self, last_value = 0):\n",
    "        indexes = slice(self.pointer_start, self.pointer_end)\n",
    "        rewards_total = np.append(self.rewards[indexes], last_value) # maybe weglassen?\n",
    "        baseline_estimates_total = np.append(self.baseline_estimates[indexes], last_value) # den maybe auch?\n",
    "        self.episode_return = discounted_reward_sum(self.rewards[indexes])\n",
    "        self.pointer_start = self.pointer_end\n",
    "\n",
    "    def get_episodes(self,actor):\n",
    "        self.pointer_start, self.pointer_end = 0,0\n",
    "\n",
    "        return self.observations, self.actions, self.rewards, np.mean(self.episode_return), self.baseline_estimates, actor.get_prob(self.actions, self.observations)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, actionspace, struct=[256,128,64]):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionspace = actionspace\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(self.actionspace, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    @tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "    #@tf.function\n",
    "\n",
    "    ## logits nur an stelle action zur√ºck\n",
    "    @tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "        #logits = self.call(observation)\n",
    "        logits = self(observation)\n",
    "        # tf.print(type(logits))\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "        # tf.print(action)\n",
    "        return logits, action\n",
    "\n",
    "\n",
    "    #@tf.function\n",
    "    def get_prob(self,actions, states):\n",
    "        # actions = actions.flatten()\n",
    "\n",
    "        logits = self.call(states)\n",
    "        #probs = np.ones_like(actions)\n",
    "        \n",
    "        # print(f'Indexes: {actions, type(actions)}: logits. {logits}, dtype {logits.dtype}')\n",
    "        # print(f'Indexes: {len(actions)}: logits. {len(logits)}')\n",
    "        \n",
    "        logits_flat = tf.squeeze(logits)\n",
    "        # print(logits_flat)\n",
    "\n",
    "        ind_1d = tf.range(len(logits_flat))\n",
    "        test = tf.stack([ind_1d, actions], axis=1)\n",
    "\n",
    "        new_probs = tf.gather_nd(logits_flat, test)\n",
    "        \n",
    "        return new_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    @tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "  \n",
    "    '''\n",
    "\n",
    "    def __init__(self, env_name, render=False, steps_per_epoch=1000, epochs=100, actor_structure=[256,128,64], critic_structure=[256,128,64]):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        \n",
    "        Args:\n",
    "        env_name(): String Name of the Environment Passed\n",
    "        render(): Boolean determining if env should be rendered during training\n",
    "        steps_per_epoch(): how many steps/frame the agent should take during each Epoch of training; Default=1000\n",
    "        epochs(): How many epochs of training should the agent do; Default=100\n",
    "        actor_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        critic_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        '''\n",
    "        # create environemt\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_dimensions = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # set Hyperparameters\n",
    "        self.lr = 3e-4\n",
    "        self.clip_ratio = 0.2\n",
    "        self.c_1 = 0.5\n",
    "        self.optimizer = Adam()\n",
    "        self.render = render\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.actor_struct = actor_structure\n",
    "        self.critic_struct = critic_structure\n",
    "        \n",
    "        # create models and temporary storage\n",
    "        self.actor = Actor(self.num_actions,self.actor_struct)\n",
    "        self.critic = Critic(self.critic_struct)\n",
    "        self.storage = Storage2(self.observation_dimensions, size=steps_per_epoch)\n",
    "\n",
    "\n",
    "    def collect_train_data(self, epoch):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        \n",
    "\n",
    "        # Initialize values for return, length and episodes\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "\n",
    "        # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "        #  allows takes on action in a state and saves the information in storage object\n",
    "        for t in tqdm_notebook(range(self.steps_per_epoch), desc = 'Epoch:' + str(epoch)):\n",
    "\n",
    "            # Toggles displaying of environment\n",
    "            if self.render or epoch == self.epochs-1 and self.epochs != 1:\n",
    "                self.env.render()\n",
    "\n",
    "            # Reshaping observation to fit as input for Actor network (policy)\n",
    "            observation = observation.reshape(1,-1)\n",
    "            \n",
    "            # Obtain action and logits for this observation by our actor\n",
    "            logits, action = self.actor.sample_action(observation)\n",
    "            \n",
    "            # Take action in environment and obtain the rewards for it\n",
    "            # Variable done represents wether agent has finished \n",
    "            # The last variable would be diagnostic information, not needed for training\n",
    "            observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "\n",
    "            # Sum up rewards over this episode and count amount of frames\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the Base-Estimate from the Critics network\n",
    "            base_estimate = self.critic(observation)\n",
    "\n",
    "            # Store Variables collected in this timestep t\n",
    "            self.storage.store(observation, action, reward, base_estimate)\n",
    "            # Save the new state of our agent\n",
    "            observation = observation_new\n",
    "            \n",
    "            # Check if terminal state is reached in environment\n",
    "            if done or (t == self.steps_per_epoch - 1):\n",
    "                # Save information about episode\n",
    "                self.storage.conclude_episode()\n",
    "                # Refresh environment and reset return and length value\n",
    "                observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "\n",
    "        # obtain all episodes saved in storage\n",
    "        # episodes, amount_episodes = self.storage.get_episodes()\n",
    "\n",
    "\n",
    "    def actor_loss_fun(self,probs_old, probs_new, rewards, b_estimates, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "        ratio = self.get_ratio(probs_old,probs_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        \n",
    "        #l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "\n",
    "        l2 = tf.clip_by_value(ratio, clip_value_min=1-clip_param, clip_value_max=1+clip_param) * advantage\n",
    "        #l1 = np.array(l1, dtype=\"float32\")\n",
    "        #l2 = np.array(l2, dtype=\"float32\")\n",
    "        \n",
    "\n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        #actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "        #l1 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l1]), dtype=tf.float32)\n",
    "        #l2 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l2]), dtype=tf.float32)\n",
    "        return l1, l2\n",
    "\n",
    "\n",
    "    def train_step(self, observations, actions, optimizer, rewards_old, probs_old, baseline_estimates,returns, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            \n",
    "            probs_new = self.actor.get_prob(actions, observations)\n",
    "            # print(f'Probs Old: {probs_old}, oftype: {type(probs_old)}. Probs New: {probs_new}, ofType {type(probs_new)}')\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen f√ºr den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()            \n",
    "            critic_loss = tf.reduce_mean((returns - self.critic(observations)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(probs_old, probs_new, rewards_old, baseline_estimates, clip_param)\n",
    "\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "            #critic_loss = tf.cast(critic_loss, dtype=tf.float32)\n",
    "            #print(f'Critics loss:{type(critic_loss)}. Actor Loss {actor_loss.dtype}')\n",
    "\n",
    "            #print('Actor weights')\n",
    "            #print(print(self.actor.layers[0].weights))\n",
    "\n",
    "            #print('actor')\n",
    "            #print(actor_loss)\n",
    "            #print(type(actor_loss))\n",
    "            #print('critic')\n",
    "            #print(critic_loss)\n",
    "            #print(type(critic_loss))\n",
    "\n",
    "            #actor_loss = tf.convert_to_tensor(actor_loss, dtype=tf.float32)\n",
    "\n",
    "            # print(actor_loss)\n",
    "            # print(critic_loss)\n",
    "            # print('actor')\n",
    "            # print(self.actor.trainable_variables)\n",
    "            # print('critic')\n",
    "            #print(self.critic.trainable_variables)\n",
    "            a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            c_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            # print(a_gradients)\n",
    "            # print(c_gradients)\n",
    "\n",
    "            #print(tape)\n",
    "            #print('Actor loss')\n",
    "            #print(actor_loss)\n",
    "            #print('Trainable Weights')\n",
    "            #print(self.actor.trainable_weights)\n",
    "        \n",
    "        #print(f'Gradients Actor: {a_gradients}. Gradients Critic: {c_gradients}')\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        # del tape\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def update_policy(self, episodes, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                        [look at single one]\n",
    "        '''\n",
    "        # for epoch in training_iteratins:\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in tqdm_notebook(episodes):\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                # States\n",
    "                episode[0],\n",
    "                # Actions\n",
    "                episode[1],\n",
    "                #optimizer (Adam)\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2],\n",
    "                # Rewards\n",
    "                episode[3],\n",
    "                clip_param,\n",
    "                c_1,\n",
    "                c_2,\n",
    "                episode[6]\n",
    "            )\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "            return train_losses_actor, train_losses_critic\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # # Estimated Value of the current situtation from the critics network\n",
    "        # b_estimates = self.episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_ratio(self, probs_old, probs_new):\n",
    "        log_probs_old = tf.nn.log_softmax(probs_old)\n",
    "        log_probs_new = tf.nn.log_softmax(probs_new)\n",
    "\n",
    "        # ratio = tf.exp(log_probs_new-log_probs_old)\n",
    "        #ratio = tf.exp(probs_new-probs_new)\n",
    "        ratio = tf.divide(log_probs_new,log_probs_old)\n",
    "        return ratio\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for epoch in tqdm_notebook(range(self.epochs), desc = 'Epochs'):\n",
    "            \n",
    "            self.collect_train_data((epoch))\n",
    "            observations, actions, rewards, returns, baseline_estimates, probs = self.storage.get_episodes(self.actor)\n",
    "            self.train_step(observations, actions, self.optimizer, rewards, probs, baseline_estimates,returns, self.clip_ratio)\n",
    "            #print(observations, actions, rewards, returns, baseline_estimates, probs)\n",
    "            print(f'Return: {np.mean(returns)}')\n",
    "            #print(data)\n",
    "            #self.update_policy(data, self.optimizer, self.clip_ratio)\n",
    "        print(self.actor.trainable_variables)\n",
    "        print(self.critic.trainable_variables)\n",
    "        self.env.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 17:12:34.987221: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-09 17:12:34.987306: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad76e8b19924a50ad744c96aadb7216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7673b79d9a5940a2b1aa0419b16aee85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 17:12:35.480982: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-04-09 17:12:35.481607: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-09 17:12:35.481698: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 17:12:35.558657: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 17:12:43.821210: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 17:12:43.879749: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 17:12:43.924131: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 17:12:45.134034: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 17:12:45.262715: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -2774.4591319226665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c88f6768c2405c8a4d13b671ce8152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -4676.504014997554\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847c2510fe3c423e8f79acbbf0eb510d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:2:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -7943.694642010811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46387fbd24547d3bf21693ec0cae32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:3:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -1903.0171753221427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cea5ba86fd4271b72863eb7dbd7626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:4:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -1253.7566815941932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c945c6f0f5b542c8b096533d39b22efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:5:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -1622.3278040004245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec700179bb14718bf369a3b7cc60006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:6:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -20.97350343076418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f75252545984fada5c4682112580a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:7:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -101.52291818916815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6aab92b9e0e4ca08e54383b9cdc1f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:8:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -4897.8960047000855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164fd87317e246b7a9ef46556cac3b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:9:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: 415.1947104350876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239456c5149f4775a514131146e77a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -622.6389877410418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4620af1b80ee47d18cbb1470f375310d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:11:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -2624.8872324911435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83782b2697d945169570e74054c025af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:12:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: 160.2467110616285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada2e87054af453494d8ec534a32e0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:13:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -49.64934272384537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bd7438345c4f78bc70a80e48a1931a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:14:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -108.81403674473816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318df882eece472dafeeae250483c153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:15:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -1150.1427751286838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7719ee0db966445689c2299cf1d1a87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:16:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -528.6665509486959\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d24d843c2f84ef59fc577845fefd0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:17:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -309.6650765461923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0fc5e44f1e4eebbd2a3e954ca491ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:18:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -13.52161814753123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfedca78ecf4163a7c8c9fa4796de15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:19:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: -534.8320024434419\n",
      "[<tf.Variable 'dense/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[-1.50827961e-02,  2.61323154e-02, -7.41020124e-03, ...,\n",
      "        -3.02780565e-04,  1.01455664e-02, -6.42375136e-03],\n",
      "       [ 1.39157055e-02, -2.89546279e-03, -1.34565653e-02, ...,\n",
      "        -9.69316158e-03, -1.22106774e-02, -5.69066219e-03],\n",
      "       [ 1.13433087e-02, -2.46069301e-03,  3.77112701e-05, ...,\n",
      "        -4.51337919e-03, -9.15297493e-03,  1.25844693e-02],\n",
      "       ...,\n",
      "       [-3.18263611e-03, -2.21650768e-03, -3.32938624e-03, ...,\n",
      "        -6.06468786e-03,  5.53688593e-03, -4.57641715e-03],\n",
      "       [-4.10830975e-03,  1.60619318e-02, -2.41473187e-02, ...,\n",
      "        -1.19672650e-02,  1.00186327e-02, -2.61615263e-03],\n",
      "       [-7.56196585e-03,  8.51930212e-03,  1.23066241e-02, ...,\n",
      "        -8.17059819e-03,  6.53973129e-03, -1.62330773e-02]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([-3.92505014e-03,  1.07309362e-03, -7.86275417e-03,  1.34982681e-03,\n",
      "       -4.99548251e-03, -4.52147657e-03,  2.09336076e-03, -4.55326866e-03,\n",
      "        8.09935227e-05,  3.70721309e-03, -6.72935043e-03, -6.24672370e-03,\n",
      "       -8.07795487e-03, -3.27777787e-04, -3.68229253e-03, -8.20483256e-04,\n",
      "       -3.11763212e-03, -2.80665606e-03, -5.67039195e-03,  7.92843581e-04,\n",
      "       -1.27086490e-02, -4.56582347e-05,  5.88651863e-04,  3.27462121e-03,\n",
      "       -9.52813029e-03, -5.34373010e-03, -1.04144048e-02,  4.21567471e-04,\n",
      "       -1.20490845e-02, -1.94401166e-03, -7.21824635e-03, -2.54297303e-03,\n",
      "       -6.31261338e-03, -4.47372848e-04, -9.49561596e-03, -1.01309863e-03,\n",
      "       -1.31051603e-03, -3.59749218e-04, -6.58081053e-03, -2.38346448e-03,\n",
      "       -2.83190212e-03,  5.46501717e-03, -1.62852847e-03, -6.27783453e-03,\n",
      "       -5.48718357e-03,  3.93312912e-05, -6.80887327e-03, -3.04856896e-03,\n",
      "       -6.75354665e-03, -2.83423625e-03, -5.17494325e-03, -4.44031088e-04,\n",
      "       -6.84920046e-03, -4.45218757e-03, -4.49975953e-03, -7.10009644e-03,\n",
      "        1.33638084e-03, -5.31454803e-03, -4.95846104e-03, -6.37897430e-03,\n",
      "       -3.19896126e-03, -3.44703998e-03, -3.25131696e-03, -6.64566737e-03,\n",
      "       -6.57605007e-04, -3.37073510e-03, -8.59014399e-04, -1.72719255e-03,\n",
      "       -1.13819365e-03, -9.43570747e-04, -9.29948466e-04, -5.94707811e-03,\n",
      "        3.17726331e-03, -7.07884040e-03, -9.41494852e-03, -2.78131058e-03,\n",
      "       -1.35209202e-03, -5.04309684e-03,  1.72932283e-03, -6.52908871e-04,\n",
      "        9.03117761e-04,  2.39228248e-03,  2.84640980e-03, -7.89728668e-03,\n",
      "       -7.95999367e-04,  3.57346755e-04, -6.20758627e-03, -5.63502451e-03,\n",
      "       -5.00470307e-03, -4.06718766e-03, -4.43346333e-03, -5.21486485e-03,\n",
      "        2.02309201e-03, -7.89220538e-03, -1.05607593e-02, -6.22827234e-03,\n",
      "       -8.89261370e-04,  1.16430072e-03, -3.28212162e-03, -7.49319419e-03,\n",
      "       -3.64858104e-04, -5.56972250e-03, -3.31109064e-03,  1.99010712e-03,\n",
      "       -5.35401562e-03, -3.40790430e-04, -1.49543595e-03, -4.12760722e-03,\n",
      "        1.78569625e-03, -3.25903227e-03, -1.35103636e-03, -1.00175096e-02,\n",
      "        3.37594817e-03, -4.69890377e-03, -1.08645502e-02, -1.20799961e-02,\n",
      "        1.94907223e-03, -5.29570098e-04, -9.09694762e-04,  4.19886364e-03,\n",
      "       -4.38010041e-03,  7.18565937e-03, -4.04820731e-03, -4.76866355e-03,\n",
      "        1.42349303e-03,  2.09810701e-03, -6.26641652e-03, -2.01056781e-03,\n",
      "       -4.52266866e-03, -2.72856047e-03,  2.75514694e-03, -2.87607755e-03,\n",
      "       -6.57596672e-03,  5.71415201e-03, -1.56493788e-03,  1.66712259e-03,\n",
      "       -3.24773369e-03, -4.05603176e-04, -4.08874219e-03,  5.87941334e-03,\n",
      "        1.13207661e-03, -2.40816362e-03, -1.16864545e-03, -1.12675549e-02,\n",
      "       -4.18755831e-03, -2.70107551e-03, -9.52374097e-03, -4.21465002e-03,\n",
      "       -9.94408852e-04, -4.17643413e-03,  2.37896596e-03, -4.25308570e-03,\n",
      "       -3.11156781e-03, -6.57763099e-03, -5.66908764e-03,  2.93229730e-03,\n",
      "       -6.44608960e-03,  4.89729876e-03, -1.10936421e-03, -1.13643764e-03,\n",
      "        5.70008357e-04, -5.14559122e-03,  1.27841136e-04, -4.97776736e-03,\n",
      "       -2.53574736e-03, -1.17779047e-04, -2.84653739e-03, -5.63787471e-04,\n",
      "       -6.65991614e-03,  6.87268609e-03, -4.72641876e-03, -2.48921663e-03,\n",
      "       -2.58981553e-03, -5.34070889e-03, -1.02461837e-02, -5.36057586e-03,\n",
      "       -1.51659260e-04,  1.37269439e-03, -1.78726448e-03, -6.65546162e-04,\n",
      "       -6.47815969e-03, -8.87323476e-05, -1.51175552e-03,  2.85778218e-03,\n",
      "       -4.65932302e-03, -4.95191384e-03,  1.40717335e-03, -1.43506192e-03,\n",
      "       -2.60096785e-05, -1.21691066e-03, -4.30982048e-03,  5.37033379e-03,\n",
      "       -2.54902552e-04, -7.74118723e-03, -3.12656048e-03, -7.05761416e-03,\n",
      "       -5.84821275e-04,  8.40517052e-04, -5.00910124e-03, -1.63496810e-03,\n",
      "        1.17223861e-03, -9.67566390e-03, -2.67653493e-03, -1.04069738e-02,\n",
      "       -1.41873013e-03, -9.92072932e-03,  1.20994076e-03, -3.38572380e-03,\n",
      "       -5.78683382e-03, -1.17083592e-03,  1.70811510e-03,  8.32632504e-05,\n",
      "       -8.96677282e-03, -2.23058640e-04,  1.06228027e-03, -4.57787421e-03,\n",
      "       -4.16538725e-03, -3.19674308e-03, -6.38912572e-03, -2.65828101e-03,\n",
      "       -7.10331229e-03, -1.29273103e-03, -1.07646186e-03, -1.18022761e-03,\n",
      "       -1.91428338e-03, -8.61008000e-03, -9.52262990e-03, -9.66466870e-03,\n",
      "       -1.90289761e-03,  4.23245085e-03, -7.64817558e-03, -7.29435077e-03,\n",
      "       -4.93089901e-03, -8.17172602e-03, -4.30339307e-04,  1.90801104e-03,\n",
      "       -9.03763250e-03, -5.48493676e-03, -4.11096821e-03, -5.98189095e-03,\n",
      "       -6.95616845e-03, -3.71261756e-03,  2.40217405e-03, -8.99529643e-03,\n",
      "       -2.56185979e-03, -6.27398305e-03,  3.62993858e-04, -3.01558920e-03,\n",
      "       -4.83306218e-03,  2.60069990e-03,  2.62407283e-03,  7.29672192e-03,\n",
      "       -5.39764576e-03, -6.01971149e-03, -9.43254214e-03, -9.91879776e-03],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[-0.00866829, -0.01243407, -0.00837128, ..., -0.00663488,\n",
      "        -0.01003639,  0.00170753],\n",
      "       [-0.00480025, -0.00794952,  0.007815  , ...,  0.00413023,\n",
      "        -0.00470044,  0.0120613 ],\n",
      "       [ 0.01025265,  0.00594277, -0.0035304 , ..., -0.00168241,\n",
      "         0.01051866, -0.00060546],\n",
      "       ...,\n",
      "       [-0.0128411 , -0.00120271,  0.00664912, ..., -0.00123393,\n",
      "        -0.01816658, -0.00347458],\n",
      "       [ 0.0030449 , -0.0074036 ,  0.00061619, ..., -0.00138852,\n",
      "         0.02346893, -0.01733389],\n",
      "       [-0.00075469,  0.00593688, -0.0055686 , ..., -0.02037199,\n",
      "         0.0081601 , -0.01546721]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-4.9602571e-03,  9.9451048e-04, -4.4613816e-03,  2.1003278e-03,\n",
      "       -7.1852407e-03, -5.1668966e-03, -8.8349869e-04, -4.3994881e-04,\n",
      "       -1.8877574e-03,  4.4528153e-03, -5.1657003e-03, -4.7901147e-03,\n",
      "       -3.3798055e-03, -5.5069262e-03, -6.0399878e-03, -1.3389064e-03,\n",
      "       -5.9638233e-03, -6.0393559e-03, -5.1267110e-03, -6.1912737e-03,\n",
      "       -5.1089851e-03, -5.1547945e-03, -4.3596728e-03,  6.7037261e-05,\n",
      "       -3.7754064e-03, -2.1584688e-03, -5.4531829e-03,  2.4000953e-03,\n",
      "       -3.7497617e-03,  5.4938742e-04, -5.1627983e-03, -3.8569795e-03,\n",
      "        1.3615770e-03, -9.8658248e-04, -4.7135656e-03, -4.9821180e-03,\n",
      "       -5.0381245e-03, -5.2559054e-03, -4.1752816e-03, -7.1894038e-03,\n",
      "        1.0087671e-03, -9.9862111e-04, -9.6227677e-04,  1.4503138e-03,\n",
      "        1.6789216e-03, -6.9720703e-03,  2.2924184e-03,  6.8148942e-04,\n",
      "        3.2836973e-04,  6.9378468e-04, -3.5884434e-03,  2.3721009e-03,\n",
      "       -4.1061752e-03,  3.8040674e-03, -5.1650358e-03, -5.1528295e-03,\n",
      "       -7.0552072e-03, -1.4833601e-03, -2.3594055e-04,  5.9190771e-04,\n",
      "       -4.8363398e-04, -6.6707195e-03, -3.6956454e-03, -1.0426719e-03,\n",
      "       -3.5511665e-03,  4.2433539e-04, -5.1607531e-03, -4.8498721e-03,\n",
      "       -5.4808529e-03, -2.9526041e-03,  1.4040667e-03, -6.9210408e-03,\n",
      "       -5.3653456e-03, -3.9611035e-03, -7.2209043e-03,  6.8021670e-04,\n",
      "       -5.8946493e-03, -4.9250685e-03, -4.3405927e-04,  4.0880940e-03,\n",
      "        1.2921411e-04, -5.1591704e-03, -5.2419798e-03,  1.0559174e-03,\n",
      "        4.5341652e-04, -3.6219156e-03, -5.9422087e-03, -4.3788832e-03,\n",
      "       -5.2485489e-03, -1.6387572e-04,  2.3477979e-03, -5.8583892e-03,\n",
      "       -1.6835054e-03, -5.9697372e-03, -5.4364153e-03,  5.7847956e-03,\n",
      "       -5.1654363e-03, -6.1813705e-03, -3.5686558e-03, -5.9320214e-03,\n",
      "       -5.1518423e-03, -5.1746205e-03,  3.6698545e-04,  1.4524254e-03,\n",
      "        1.6232956e-03, -7.0545739e-03,  2.1760813e-03,  6.0966928e-03,\n",
      "       -5.1667560e-03, -5.1665115e-03, -2.7450621e-03,  3.4654259e-03,\n",
      "       -2.9767121e-04,  1.0936105e-04, -5.0944639e-03, -5.2335788e-03,\n",
      "       -7.8490684e-03,  8.2782196e-04, -6.6990983e-03, -5.4987138e-03,\n",
      "       -5.1631141e-03, -4.8055989e-03, -5.1638512e-03, -2.8829178e-04,\n",
      "       -5.1586316e-03, -5.5726967e-03,  3.5678565e-03, -6.2001026e-03],\n",
      "      dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[-0.00181486,  0.00570045,  0.02486853, ..., -0.00953625,\n",
      "         0.02181218,  0.01077233],\n",
      "       [-0.016392  , -0.00657063,  0.01914155, ...,  0.0016454 ,\n",
      "        -0.01104155,  0.00517254],\n",
      "       [-0.00016149, -0.01687564, -0.00225523, ...,  0.01389964,\n",
      "        -0.00609525,  0.00668536],\n",
      "       ...,\n",
      "       [-0.00463148,  0.00905463,  0.01052901, ...,  0.00359864,\n",
      "         0.00601003, -0.01019318],\n",
      "       [-0.01334459,  0.02282201,  0.00515957, ...,  0.00386963,\n",
      "         0.00476372,  0.00509155],\n",
      "       [ 0.00341783,  0.00238152,  0.00762032, ...,  0.00250812,\n",
      "        -0.00286713,  0.00145551]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([ 1.2496981e-03, -5.1670824e-03, -5.1620281e-03, -5.1535666e-03,\n",
      "       -5.1513296e-03, -5.1675867e-03, -5.1676743e-03, -5.1673632e-03,\n",
      "       -5.1674824e-03, -5.1665190e-03, -1.3039304e-03, -5.1669981e-03,\n",
      "       -5.1672165e-03, -5.1677320e-03, -5.1676808e-03,  1.6007696e-03,\n",
      "       -3.4011812e-03,  3.0724714e-03, -4.2548086e-03,  3.9632726e-03,\n",
      "       -5.1675723e-03, -3.6632849e-04, -4.6275202e-03,  1.8421522e-03,\n",
      "        2.1251661e-03,  2.3960276e-03, -5.1677283e-03,  2.5019222e-03,\n",
      "       -4.6284483e-03, -3.2509349e-03, -5.1676561e-03, -5.0822957e-03,\n",
      "        2.9305837e-03, -1.2135364e-03,  5.7716789e-03, -5.1596267e-03,\n",
      "       -5.1675914e-03, -5.1676668e-03, -5.1677786e-03,  1.6460555e-03,\n",
      "        4.6159155e-03, -5.1674261e-03,  4.5410834e-05, -5.1666130e-03,\n",
      "       -5.1675634e-03, -5.1676650e-03,  1.9777585e-03,  3.7152322e-03,\n",
      "       -4.6278965e-03,  3.4214540e-03, -5.1673311e-03, -5.1677749e-03,\n",
      "       -4.0317848e-03, -5.1295757e-03, -5.1675998e-03,  3.8157173e-03,\n",
      "       -5.1677232e-03, -5.1655341e-03, -3.0942578e-03, -5.1677753e-03,\n",
      "        3.1172989e-03,  2.8623333e-03, -5.1675383e-03, -5.1677353e-03],\n",
      "      dtype=float32)>, <tf.Variable 'dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\n",
      "array([[-0.14284626, -0.21394773, -0.16795497, -0.16266914],\n",
      "       [ 0.04606876,  0.11113653, -0.2923198 , -0.18205   ],\n",
      "       [-0.20887102,  0.07388441,  0.24824789, -0.06834249],\n",
      "       [ 0.25586474, -0.10446791, -0.04354231,  0.04733283],\n",
      "       [ 0.2658287 ,  0.14498506, -0.23800163, -0.25908598],\n",
      "       [ 0.24260853, -0.16401342, -0.22616524,  0.0840838 ],\n",
      "       [ 0.21111576,  0.21931934, -0.08629234, -0.27433774],\n",
      "       [-0.03211346,  0.13387285, -0.28397277, -0.02861261],\n",
      "       [-0.03324446, -0.09057361, -0.2528695 , -0.08240537],\n",
      "       [ 0.05048993,  0.0704155 ,  0.2744188 , -0.17428607],\n",
      "       [ 0.09785174, -0.06245432,  0.20371422,  0.287364  ],\n",
      "       [-0.23057072, -0.0539191 ,  0.14754118,  0.14477345],\n",
      "       [ 0.10488471, -0.1178658 ,  0.1900914 ,  0.19713882],\n",
      "       [ 0.1384901 , -0.23551673, -0.06609432,  0.12411302],\n",
      "       [ 0.15839583,  0.0634305 ,  0.13022307,  0.24608499],\n",
      "       [-0.16612248, -0.00324614,  0.07062791, -0.2390096 ],\n",
      "       [-0.15382282, -0.22319663, -0.28855005, -0.21965079],\n",
      "       [ 0.20127144, -0.18256606, -0.23177198,  0.17347474],\n",
      "       [-0.1194354 , -0.14089604, -0.2417946 ,  0.25587228],\n",
      "       [-0.03267236, -0.02236624,  0.0990035 , -0.26842377],\n",
      "       [ 0.22135405, -0.06877261, -0.10727807, -0.0262646 ],\n",
      "       [ 0.03639983,  0.00746304,  0.25738135,  0.1409951 ],\n",
      "       [ 0.12482413, -0.02567796, -0.17754205, -0.20344481],\n",
      "       [-0.07031699, -0.1802986 , -0.03285992, -0.00416383],\n",
      "       [ 0.16228835, -0.18943515, -0.00982244,  0.2420279 ],\n",
      "       [-0.11878707,  0.0535147 ,  0.19048683, -0.264195  ],\n",
      "       [-0.04933001, -0.01690717,  0.22481984,  0.25277787],\n",
      "       [-0.00257569, -0.2049524 , -0.00752708,  0.27045223],\n",
      "       [ 0.23972778,  0.08063427, -0.28030598,  0.21850957],\n",
      "       [-0.03514571,  0.27005273,  0.00975064,  0.21548487],\n",
      "       [-0.29107302, -0.02124254,  0.11396323,  0.12175044],\n",
      "       [ 0.19254027,  0.19471426, -0.11568364,  0.22209635],\n",
      "       [ 0.18840519, -0.01946917,  0.03970036,  0.18532377],\n",
      "       [-0.19631393, -0.20598182, -0.0891793 ,  0.06122963],\n",
      "       [ 0.17944416,  0.07787056,  0.08202464, -0.13037856],\n",
      "       [ 0.245561  ,  0.29287514,  0.20266704, -0.15013216],\n",
      "       [-0.27430335, -0.10803243,  0.00742068,  0.2547308 ],\n",
      "       [-0.16405985, -0.04087094, -0.10090356,  0.1604459 ],\n",
      "       [ 0.22886851,  0.22998911, -0.19787185, -0.03341023],\n",
      "       [-0.12053746, -0.20632844, -0.01355683, -0.08984733],\n",
      "       [ 0.260365  ,  0.10105588,  0.21013188, -0.26941243],\n",
      "       [ 0.19486548,  0.23551324, -0.26491955,  0.08326804],\n",
      "       [-0.1470084 , -0.08334056, -0.00286854, -0.11264936],\n",
      "       [-0.0527604 ,  0.05979698,  0.2690471 , -0.28821686],\n",
      "       [ 0.16408014,  0.06800925,  0.09876455, -0.02299847],\n",
      "       [-0.04610353,  0.2226847 ,  0.22207905, -0.10578391],\n",
      "       [-0.24143454, -0.27448732, -0.0685214 , -0.23108894],\n",
      "       [-0.01085958, -0.15579355,  0.04712059, -0.28891277],\n",
      "       [ 0.27339876, -0.07500288, -0.25433388, -0.15337339],\n",
      "       [-0.18534926, -0.15221332, -0.15599157, -0.24196053],\n",
      "       [ 0.06689966, -0.06821167, -0.16745432,  0.05267728],\n",
      "       [-0.24674095, -0.22486901,  0.20334306,  0.27895635],\n",
      "       [-0.24401665,  0.06646251,  0.06576509, -0.06021002],\n",
      "       [-0.25845787, -0.1695095 ,  0.23822373,  0.2814929 ],\n",
      "       [ 0.03504892, -0.09824468, -0.19435853,  0.01241131],\n",
      "       [ 0.10973263, -0.29383528,  0.11977413, -0.1414648 ],\n",
      "       [ 0.00263771,  0.2946756 , -0.23832147, -0.04988255],\n",
      "       [ 0.2473981 ,  0.29211566, -0.07634671, -0.13112676],\n",
      "       [-0.14826958,  0.16300526,  0.18413042,  0.00390219],\n",
      "       [ 0.01240219,  0.21714589, -0.1837331 ,  0.06650929],\n",
      "       [ 0.09204128,  0.10827206,  0.1801351 ,  0.02861996],\n",
      "       [ 0.07804323, -0.28267458,  0.2304983 , -0.00029541],\n",
      "       [ 0.0118332 ,  0.21588081,  0.1345445 ,  0.11552742],\n",
      "       [-0.19229117,  0.0015738 , -0.1821771 , -0.08589406]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([ 0.006018  , -0.0033082 ,  0.00227729, -0.00473966], dtype=float32)>]\n",
      "[<tf.Variable 'dense_4/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[-0.01150676, -0.01628095, -0.02554838, ..., -0.01324601,\n",
      "        -0.01515191,  0.01370214],\n",
      "       [-0.01714197,  0.01658283,  0.00281947, ...,  0.01480585,\n",
      "         0.01283836,  0.00515737],\n",
      "       [ 0.01219957,  0.00876868, -0.01547644, ..., -0.01684333,\n",
      "        -0.00856872,  0.00129319],\n",
      "       ...,\n",
      "       [-0.00563183, -0.00839029, -0.01046205, ...,  0.03554573,\n",
      "         0.00937207, -0.00940846],\n",
      "       [ 0.01999827,  0.01069507,  0.0170498 , ...,  0.00579896,\n",
      "        -0.01157315,  0.00421044],\n",
      "       [-0.02337377,  0.0195796 ,  0.02184546, ..., -0.00603786,\n",
      "         0.01393201, -0.01103567]], dtype=float32)>, <tf.Variable 'dense_4/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([-1.85672531e-03,  1.43255098e-02,  1.34967081e-02,  1.42295090e-02,\n",
      "       -1.88446755e-03,  8.61099921e-03,  1.39421225e-02,  1.28768114e-02,\n",
      "        1.43591911e-02,  9.48563591e-03,  7.92081002e-03,  1.00482525e-02,\n",
      "        6.00933377e-03,  1.40556600e-02,  1.23936031e-02,  1.37031469e-02,\n",
      "        1.36459768e-02,  1.29454946e-02,  1.18973851e-02,  1.34192035e-02,\n",
      "       -2.11384217e-03,  8.68660863e-03,  9.20333434e-03,  1.39160929e-02,\n",
      "        1.43514825e-02,  1.34019507e-02,  1.36197768e-02,  1.35980463e-02,\n",
      "        1.43656721e-02,  1.02275508e-02,  8.52489565e-03,  1.23700788e-02,\n",
      "        3.05945962e-03,  1.20037766e-02,  1.43374754e-02,  1.38077578e-02,\n",
      "        1.22761093e-02,  1.41745526e-02,  4.19111410e-03,  1.20837539e-02,\n",
      "        1.42644728e-02,  1.43279405e-02,  1.19668152e-02,  1.38638793e-02,\n",
      "        1.07702250e-02,  6.52023731e-03,  4.51372284e-03,  1.41712185e-02,\n",
      "        1.39933480e-02,  1.40859773e-02,  1.42139457e-02,  1.39658013e-02,\n",
      "       -6.82365242e-03,  5.18417126e-03,  9.65832546e-03,  1.41139794e-02,\n",
      "        1.43447081e-02,  1.21853882e-02,  4.14185738e-03,  1.34922527e-02,\n",
      "        4.72574448e-03,  1.20361391e-02,  1.30320126e-02,  1.41870193e-02,\n",
      "        6.87036244e-03,  1.19701335e-02,  1.40738990e-02,  1.19501920e-02,\n",
      "        1.30166961e-02,  6.62709074e-03, -4.04532766e-03,  1.03063276e-02,\n",
      "        1.34961540e-02,  1.42637361e-02,  1.42184328e-02, -3.83874727e-03,\n",
      "       -3.76398000e-03, -7.92216416e-03,  1.41910082e-02,  1.24933524e-02,\n",
      "        1.20611517e-02,  2.69089942e-03,  1.42970355e-02,  1.41676040e-02,\n",
      "        7.98050687e-03,  1.24554457e-02,  8.64509959e-03,  1.20880622e-02,\n",
      "        9.65982396e-03,  3.78193287e-03,  6.04609307e-03,  1.40705779e-02,\n",
      "        1.42682167e-02,  1.17292479e-02,  1.39538003e-02,  1.19022382e-02,\n",
      "        1.38673261e-02,  1.02876434e-02,  1.41072292e-02,  1.41456677e-02,\n",
      "        1.40424529e-02,  1.40823592e-02,  1.45690236e-02,  6.46045245e-03,\n",
      "        1.37680806e-02,  1.34539856e-02,  1.06112063e-02,  1.06648235e-02,\n",
      "        1.33940792e-02,  1.45899216e-02,  1.23604769e-02,  1.38948336e-02,\n",
      "        1.39922770e-02,  9.43584740e-03,  1.19395824e-02,  9.12144082e-04,\n",
      "        1.39016462e-02,  1.18753240e-02,  3.86510626e-03,  1.41329085e-02,\n",
      "       -3.81234754e-03,  1.45617155e-02,  1.25311576e-02,  1.16705028e-02,\n",
      "        1.43327164e-02,  1.16954828e-02,  1.42503595e-02,  1.38570303e-02,\n",
      "        1.15425065e-02,  1.05433827e-02,  1.27473129e-02,  1.42244399e-02,\n",
      "        1.40321860e-02,  1.41503364e-02,  1.41019728e-02,  6.02505496e-03,\n",
      "        1.41843129e-02,  1.39312651e-02,  2.31926027e-03,  1.42861418e-02,\n",
      "        1.41741410e-02,  1.07548665e-02,  1.40819997e-02,  1.25299692e-02,\n",
      "        1.02555966e-02,  1.42289745e-02,  1.42680556e-02,  1.18381577e-02,\n",
      "        1.39380442e-02,  1.44803151e-02,  8.93083494e-03,  5.35171526e-03,\n",
      "        1.41630964e-02, -6.31684205e-03,  3.74229997e-03,  9.84736625e-03,\n",
      "        1.34639051e-02,  1.27894934e-02,  1.37152122e-02,  8.77547823e-03,\n",
      "        5.03256358e-03,  1.40003618e-02,  9.66889411e-03,  1.42228063e-02,\n",
      "        1.18946368e-02,  6.96164044e-03,  1.42162926e-02,  1.43435067e-02,\n",
      "        6.36463100e-03, -7.69961532e-03,  1.30501660e-02,  1.37349749e-02,\n",
      "        9.88288317e-03,  1.24611510e-02,  4.16312832e-03,  9.16647445e-03,\n",
      "        1.38148731e-02,  1.37020331e-02,  1.38252703e-02,  1.44311553e-02,\n",
      "        1.37452632e-02,  1.99299608e-03,  1.42005170e-02, -7.82580953e-03,\n",
      "        1.16547635e-02,  5.69702499e-03,  1.08677782e-02,  1.41545553e-02,\n",
      "        5.17768320e-03,  1.19723585e-02,  7.48661812e-03,  1.41157284e-02,\n",
      "       -5.24849165e-04,  1.08259637e-02,  1.36249177e-02,  1.41001493e-02,\n",
      "        1.42551772e-02,  6.39671134e-03,  2.25023390e-03,  1.18781896e-02,\n",
      "        1.11339279e-02,  1.46518266e-02, -1.94861495e-03,  9.83581878e-03,\n",
      "        1.42776882e-02, -3.10498202e-04,  1.22480253e-02,  3.08638485e-03,\n",
      "        1.41093619e-02,  1.04243234e-02,  1.33129293e-02,  1.22609306e-02,\n",
      "        5.34341764e-03,  1.23443417e-02,  1.42113194e-02,  1.12149995e-02,\n",
      "        1.43848164e-02,  1.40962955e-02,  1.41245099e-02,  1.18117733e-02,\n",
      "        1.39256343e-02,  1.23981861e-02,  1.11168548e-02,  1.24470228e-02,\n",
      "        1.42348278e-02,  1.15597425e-02,  1.40661020e-02,  1.41379479e-02,\n",
      "        1.40062291e-02,  9.44761373e-03,  9.26499162e-03,  1.33778742e-02,\n",
      "        1.46332113e-02,  1.16969924e-02,  1.41231874e-02,  1.22935036e-02,\n",
      "        1.15105705e-02,  1.42248627e-02,  1.37881069e-02,  1.40858172e-02,\n",
      "        1.42257139e-02,  1.10234143e-02,  1.12275332e-02,  1.39357755e-02,\n",
      "        1.40692424e-02,  1.40262153e-02,  9.95181687e-03,  5.38752973e-03,\n",
      "        3.96743417e-03,  1.03840362e-02,  1.43219912e-02,  1.11941732e-02,\n",
      "       -8.01466595e-05,  1.40916342e-02,  9.69936699e-03,  2.25983863e-03],\n",
      "      dtype=float32)>, <tf.Variable 'dense_5/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[-1.2079841e-02, -1.4544869e-02,  1.1915026e-02, ...,\n",
      "         1.1460223e-02, -1.5123795e-02, -4.6008360e-03],\n",
      "       [-1.7414995e-02, -1.0439195e-02,  1.3316876e-02, ...,\n",
      "        -9.7005311e-03, -2.9712478e-03, -1.2135013e-02],\n",
      "       [ 4.6114479e-03, -7.6874980e-04,  3.5375703e-02, ...,\n",
      "         9.5985839e-03, -1.1038554e-02,  9.2942920e-04],\n",
      "       ...,\n",
      "       [ 1.0108699e-02, -9.8821996e-03,  6.4503355e-03, ...,\n",
      "         2.2056540e-02, -6.1984849e-03, -1.5369566e-02],\n",
      "       [ 1.8857474e-03,  1.7833943e-04,  2.3898216e-02, ...,\n",
      "         1.6829737e-02, -2.0031488e-02, -6.1120051e-03],\n",
      "       [-2.3582196e-02, -1.6836436e-02, -2.1132076e-02, ...,\n",
      "         4.2403994e-05, -5.6416309e-03, -3.1564817e-02]], dtype=float32)>, <tf.Variable 'dense_5/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-0.0048505 , -0.00663453,  0.0132449 ,  0.01358936, -0.00485073,\n",
      "        0.01402713, -0.00485068, -0.0046914 , -0.00485072,  0.01404832,\n",
      "        0.01351915, -0.00485072, -0.00485073,  0.0135861 ,  0.01403871,\n",
      "        0.01411503, -0.00485072, -0.00575675, -0.00526005,  0.01368119,\n",
      "       -0.00485073,  0.01356941,  0.01052327,  0.01423242,  0.01156928,\n",
      "        0.00211588,  0.01370952,  0.01379655,  0.01342409,  0.00841139,\n",
      "       -0.00482176, -0.00329029, -0.00485072,  0.01392281,  0.01328202,\n",
      "       -0.00473616,  0.0133513 ,  0.00596579, -0.00485073,  0.01356049,\n",
      "        0.0137219 ,  0.0132372 ,  0.01399107,  0.01194444,  0.01077371,\n",
      "        0.00900215,  0.01312713,  0.01390058,  0.00538073, -0.00485072,\n",
      "       -0.00485067,  0.01403124,  0.01366189,  0.01352893, -0.00570185,\n",
      "       -0.00479505, -0.00484993,  0.01360975,  0.01235834,  0.00365754,\n",
      "        0.01158583,  0.0140484 , -0.00485065, -0.00485373,  0.01327833,\n",
      "        0.01336749,  0.01405003,  0.01339736,  0.01377088,  0.00607292,\n",
      "        0.        , -0.00485072,  0.01070933,  0.01212522, -0.00485226,\n",
      "       -0.00456057,  0.00612847, -0.00485066, -0.0048507 , -0.00083381,\n",
      "        0.01190389, -0.00485073, -0.00485072, -0.00632528, -0.00565221,\n",
      "       -0.00598566,  0.01347503,  0.01417965, -0.00485073, -0.00497905,\n",
      "        0.01340091, -0.00779726,  0.01174467, -0.00484248, -0.00535426,\n",
      "        0.01414915,  0.01342671,  0.01444697,  0.01368102,  0.01340044,\n",
      "        0.01350361, -0.00512884,  0.01214414,  0.01374385,  0.01101615,\n",
      "       -0.00486051,  0.01106481, -0.00485073, -0.00496909, -0.0048507 ,\n",
      "        0.01333998, -0.00527648,  0.01391826,  0.01324937,  0.01350294,\n",
      "       -0.00485073, -0.00485072,  0.01392429, -0.00609493,  0.01352919,\n",
      "       -0.00485072, -0.00547216, -0.00485072,  0.01394985,  0.01332668,\n",
      "        0.01180332, -0.00485073, -0.00592684], dtype=float32)>, <tf.Variable 'dense_6/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.01334018, -0.00884562,  0.00409406, ..., -0.00668425,\n",
      "         0.00056724,  0.01890808],\n",
      "       [ 0.01216225,  0.01423354,  0.00894221, ...,  0.01307416,\n",
      "        -0.00781025,  0.01156607],\n",
      "       [ 0.03441361,  0.02872219,  0.00471243, ...,  0.02454694,\n",
      "        -0.00303964, -0.02366024],\n",
      "       ...,\n",
      "       [ 0.02212235,  0.01217722,  0.02104365, ...,  0.01684701,\n",
      "        -0.01976759, -0.00772868],\n",
      "       [ 0.00291078, -0.00537547, -0.00242802, ..., -0.01166733,\n",
      "        -0.00276422, -0.02867401],\n",
      "       [ 0.00986442,  0.02076346,  0.00159701, ...,  0.0076737 ,\n",
      "        -0.00855654,  0.01024893]], dtype=float32)>, <tf.Variable 'dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([ 0.01013897,  0.01243096,  0.01224247,  0.01241348, -0.00485073,\n",
      "       -0.00485073,  0.01202946,  0.01243871,  0.01167948, -0.00485362,\n",
      "       -0.00485073,  0.01178438, -0.00485073,  0.0120489 ,  0.01205185,\n",
      "       -0.00485073, -0.00456059, -0.00485073,  0.01242871,  0.01235216,\n",
      "       -0.00485073,  0.01175588,  0.01168035, -0.00485073, -0.00485073,\n",
      "       -0.00485073,  0.01236512,  0.01242404,  0.01198076, -0.00485073,\n",
      "        0.01232062,  0.01240979,  0.01260536,  0.        ,  0.01226119,\n",
      "        0.01243071, -0.00485073, -0.00485073, -0.00485072,  0.01240649,\n",
      "       -0.00485073,  0.01226738, -0.00485073, -0.00485073,  0.01240219,\n",
      "        0.01189613, -0.00485073, -0.00485073, -0.00485073,  0.01227258,\n",
      "       -0.00485073, -0.00485073, -0.00485073,  0.01218867,  0.01249351,\n",
      "       -0.00485073, -0.00485073, -0.00485073, -0.00485073, -0.00485073,\n",
      "        0.01170581,  0.01367946, -0.00485073,  0.        ], dtype=float32)>, <tf.Variable 'dense_7/kernel:0' shape=(64, 1) dtype=float32, numpy=\n",
      "array([[-0.15861408],\n",
      "       [-0.2323665 ],\n",
      "       [-0.18533719],\n",
      "       [-0.18118186],\n",
      "       [ 0.04205395],\n",
      "       [ 0.10932528],\n",
      "       [-0.3131547 ],\n",
      "       [-0.20067753],\n",
      "       [-0.23151997],\n",
      "       [ 0.06595094],\n",
      "       [ 0.2442011 ],\n",
      "       [-0.07893401],\n",
      "       [ 0.25690395],\n",
      "       [-0.1210624 ],\n",
      "       [-0.05865675],\n",
      "       [ 0.04358444],\n",
      "       [ 0.26842245],\n",
      "       [ 0.1471668 ],\n",
      "       [-0.2603594 ],\n",
      "       [-0.28232613],\n",
      "       [ 0.23857714],\n",
      "       [-0.17821056],\n",
      "       [-0.25049752],\n",
      "       [ 0.08622546],\n",
      "       [ 0.2076456 ],\n",
      "       [ 0.22432488],\n",
      "       [-0.10718686],\n",
      "       [-0.29156312],\n",
      "       [-0.05091323],\n",
      "       [ 0.12843387],\n",
      "       [-0.30792284],\n",
      "       [-0.03881866],\n",
      "       [-0.05255504],\n",
      "       [-0.08907223],\n",
      "       [-0.27696803],\n",
      "       [-0.09381537],\n",
      "       [ 0.05016174],\n",
      "       [ 0.07215407],\n",
      "       [ 0.27118742],\n",
      "       [-0.19718727],\n",
      "       [ 0.08492558],\n",
      "       [-0.07731652],\n",
      "       [ 0.20160453],\n",
      "       [ 0.2963942 ],\n",
      "       [-0.2501284 ],\n",
      "       [-0.06743212],\n",
      "       [ 0.1470181 ],\n",
      "       [ 0.14097555],\n",
      "       [ 0.10511566],\n",
      "       [-0.13662332],\n",
      "       [ 0.19035123],\n",
      "       [ 0.19476743],\n",
      "       [ 0.13184498],\n",
      "       [-0.26005813],\n",
      "       [-0.08660519],\n",
      "       [ 0.1272661 ],\n",
      "       [ 0.15215373],\n",
      "       [ 0.05526203],\n",
      "       [ 0.12320267],\n",
      "       [ 0.25205165],\n",
      "       [-0.1913853 ],\n",
      "       [-0.01741985],\n",
      "       [ 0.06827139],\n",
      "       [-0.24170662]], dtype=float32)>, <tf.Variable 'dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([-0.01238673], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent(env_name='LunarLander-v2', render=True,epochs=20)\n",
    "ppo_agent.run()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
