{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Storage 2.0\n",
    "Numpy Version with full-batch return of epoch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward_sum(rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return sum(discounted_rewards)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "\n",
    "    def __init__(self, observation_dimension, size):\n",
    "        self.observations = np.zeros((size, observation_dimension), dtype=np.float32)\n",
    "        self.actions = np.zeros(size, dtype=np.int32)\n",
    "        self.rewards = np.zeros(size, dtype=np.float32)\n",
    "        self.episode_return = np.zeros(size, dtype=np.float32)\n",
    "        self.baseline_estimates = np.zeros(size, dtype=np.float32)\n",
    "        self.pointer_start, self.pointer_end= 0,0\n",
    "        \n",
    "\n",
    "    def store(self, observation, action, reward, baseline_estimate):\n",
    "        self.observations[self.pointer_end] = observation\n",
    "        self.actions[self.pointer_end] = action\n",
    "        self.rewards[self.pointer_end] = reward\n",
    "        self.baseline_estimates[self.pointer_end] = baseline_estimate\n",
    "        self.pointer_end += 1\n",
    "\n",
    "    def conclude_episode(self, last_value = 0):\n",
    "        indexes = slice(self.pointer_start, self.pointer_end)\n",
    "        rewards_total = np.append(self.rewards[indexes], last_value) # maybe weglassen?\n",
    "        baseline_estimates_total = np.append(self.baseline_estimates[indexes], last_value) # den maybe auch?\n",
    "        self.episode_return = discounted_reward_sum(self.rewards[indexes])\n",
    "        self.pointer_start = self.pointer_end\n",
    "\n",
    "    def get_episodes(self,actor):\n",
    "        self.pointer_start, self.pointer_end = 0,0\n",
    "\n",
    "        return self.observations, self.actions, self.rewards, np.mean(self.episode_return), self.baseline_estimates, actor.get_prob(self.actions, self.observations)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, actionspace, struct=[256,128,64]):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "\n",
    "        Args:\n",
    "        actionspace(): number of possible actions the agent can take\n",
    "        struct(): Width of layers in the network - must be of length = 3 \n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionspace = actionspace\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(self.actionspace, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    @tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #@tf.function\n",
    "\n",
    "    ## logits nur an stelle action zur√ºck\n",
    "    @tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "\n",
    "        logits = self(observation)\n",
    "\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "        return logits, action\n",
    "\n",
    "\n",
    "    #@tf.function\n",
    "    def get_prob(self,actions, states):\n",
    "\n",
    "        logits = self.call(states)\n",
    "        logits_flat = tf.squeeze(logits)\n",
    "\n",
    "\n",
    "        index_first_dim = tf.range(len(logits_flat))\n",
    "        index_2D = tf.stack([index_first_dim, actions], axis=1)\n",
    "\n",
    "        new_probs = tf.gather_nd(logits_flat, index_2D)\n",
    "        \n",
    "        return new_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "\n",
    "        Args:\n",
    "        struct(): Width of layers in the network - must be of length = 3 \n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    @tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "  \n",
    "    '''\n",
    "\n",
    "    def __init__(self, env_name, render=False, steps_per_epoch=1000, epochs=100, train_iterations=10, actor_structure=[256,128,64], critic_structure=[256,128,64]):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        \n",
    "        Args:\n",
    "        env_name(): String Name of the Environment Passed\n",
    "        render(): Boolean determining if env should be rendered during training\n",
    "        steps_per_epoch(): how many steps/frame the agent should take during each Epoch of training; Default=1000\n",
    "        epochs(): How many epochs of training should the agent do; Default=100\n",
    "        actor_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        critic_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        '''\n",
    "        # create environemt\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_dimensions = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # set Hyperparameters\n",
    "        self.lr = 3e-4\n",
    "        self.clip_ratio = 0.2\n",
    "        self.c_1 = 0.5\n",
    "        self.train_policy_iterations = train_iterations\n",
    "        self.optimizer = Adam(self.lr)\n",
    "        self.render = render\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.actor_struct = actor_structure\n",
    "        self.critic_struct = critic_structure\n",
    "        \n",
    "        # create models and temporary storage\n",
    "        self.actor = Actor(self.num_actions,self.actor_struct)\n",
    "        self.critic = Critic(self.critic_struct)\n",
    "        self.storage = Storage(self.observation_dimensions, size=steps_per_epoch)\n",
    "\n",
    "\n",
    "    def collect_train_data(self, epoch):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "\n",
    "        Args: \n",
    "        epoch(): Epoch inherited from wrapper function 'run' to correctly show epoch in TQDM Notebook\n",
    "        '''\n",
    "        observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        \n",
    "\n",
    "        # Initialize values for return, length and episodes\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "\n",
    "        # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "        #  allows takes on action in a state and saves the information in storage object\n",
    "        for t in tqdm_notebook(range(self.steps_per_epoch), desc = 'Epoch:' + str(epoch+1)):\n",
    "\n",
    "            # Toggles displaying of environment\n",
    "            if self.render:\n",
    "                self.env.render()\n",
    "\n",
    "            # Reshaping observation to fit as input for Actor network (policy)\n",
    "            observation = observation.reshape(1,-1)\n",
    "            \n",
    "            # Obtain action and logits for this observation by our actor\n",
    "            logits, action = self.actor.sample_action(observation)\n",
    "            \n",
    "            # Take action in environment and obtain the rewards for it\n",
    "            # Variable done represents wether agent has finished \n",
    "            # The last variable would be diagnostic information, not needed for training\n",
    "            observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "\n",
    "            # Sum up rewards over this episode and count amount of frames\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the Base-Estimate from the Critics network\n",
    "            base_estimate = self.critic(observation)\n",
    "\n",
    "            # Store Variables collected in this timestep t\n",
    "            self.storage.store(observation, action, reward, base_estimate)\n",
    "            # Save the new state of our agent\n",
    "            observation = observation_new\n",
    "            \n",
    "            # Check if terminal state is reached in environment\n",
    "            if done or (t == self.steps_per_epoch - 1):\n",
    "                # Save information about episode\n",
    "                self.storage.conclude_episode()\n",
    "                # Refresh environment and reset return and length value\n",
    "                num_episodes += 1\n",
    "                sum_length += episode_length\n",
    "                sum_return += episode_return\n",
    "                observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "\n",
    "        return num_episodes, sum_length/num_episodes, sum_return/num_episodes\n",
    "                \n",
    "\n",
    "\n",
    "    def actor_loss_fun(self,probs_old, probs_new, rewards, b_estimates, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "        ratio = self.get_ratio(probs_old,probs_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "    \n",
    "        l2 = tf.clip_by_value(ratio, clip_value_min=1-clip_param, clip_value_max=1+clip_param) * advantage\n",
    "\n",
    "        return l1, l2\n",
    "\n",
    "\n",
    "    def train_step(self, observations, actions, optimizer, rewards_old, probs_old, baseline_estimates,returns, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # obtain the probs of actions at observed states for the new policy\n",
    "            probs_new = self.actor.get_prob(actions, observations)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen f√ºr den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()            \n",
    "            critic_loss = tf.reduce_mean((returns - self.critic(observations)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(probs_old, probs_new, rewards_old, baseline_estimates, clip_param)\n",
    "\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "     \n",
    "        a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        c_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "    \n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        # del tape\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_ratio(self, probs_old, probs_new):\n",
    "        '''\n",
    "        Returns the ratio of the logarithmic probability given by the old policy and the  logarithmic probability given by the new policy for an action in a state\n",
    "\n",
    "        Args:\n",
    "        probs_old():\n",
    "        probs_new():\n",
    "        '''\n",
    "\n",
    "        log_probs_old = tf.nn.log_softmax(probs_old)\n",
    "        log_probs_new = tf.nn.log_softmax(probs_new)\n",
    "\n",
    "        ratio = tf.divide(log_probs_new,log_probs_old)\n",
    "        return ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, export = False):\n",
    "        '''\n",
    "        Wrapper funtion that can directly be called to train the agent.\n",
    "\n",
    "        Args:\n",
    "        export(bool): whether the Scores during Training should be exported and saved as a plot\n",
    "        '''\n",
    "        mean_returns = []\n",
    "        mean_lengths = []\n",
    "        num_episodes_total = 0\n",
    "        for epoch in tqdm_notebook(range(self.epochs), desc = 'Epochs'):\n",
    "            \n",
    "            num_episodes, mean_length, mean_return = self.collect_train_data((epoch))\n",
    "            observations, actions, rewards, returns, baseline_estimates, probs = self.storage.get_episodes(self.actor)\n",
    "            for i in range(self.train_policy_iterations):\n",
    "                self.train_step(observations, actions, self.optimizer, rewards, probs, baseline_estimates,returns, self.clip_ratio)\n",
    "            num_episodes_total += num_episodes\n",
    "            mean_returns.append(mean_return)\n",
    "            mean_lengths.append(mean_length)\n",
    "            print(f'Mean Return: {mean_return}. Average Length of Episode: {mean_length}. Total Episodes Trained: {num_episodes_total}')\n",
    "        \n",
    "        # export the training progress return values as png graph \n",
    "        if export:\n",
    "            plt.figure()\n",
    "            plt.ylabel('Average score per epoch')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.title(f'Model Structure: {self.actor_struct}, LR: {self.lr}, training iterations: {self.train_policy_iterations}')\n",
    "            plt.plot(range(self.epochs), mean_returns)\n",
    "            plt.savefig(f'Model Structure:{self.actor_struct}_LR:{self.lr}_training iterations:_{self.train_policy_iterations}_figure.png',facecolor='white', edgecolor='none')\n",
    "\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def test(self, length=100):\n",
    "        '''\n",
    "        Tests the agent on a number of episodes and return the average score.\n",
    "        Args: \n",
    "        length(int): number of episodes to test the agent over\n",
    "        export(bool): whether the \n",
    "        '''\n",
    "        reward_sum = 0\n",
    "        for i in tqdm_notebook(range(length), desc = 'Test Episodes'):\n",
    "            done = False\n",
    "            observation = self.env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                self.env.render()        \n",
    "                observation = observation.reshape(1,-1)\n",
    "                _ , action = self.actor.sample_action(observation)\n",
    "                observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "                reward_sum += reward\n",
    "                observation = observation_new\n",
    "        print(f'Average score over {length} episodes: {reward_sum/length}.')\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 23:19:51.507919: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-09 23:19:51.508038: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57355d61b79946e1817242e30adec73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ac8c94b2f84659af8ac9d68e162ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 23:19:51.670640: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-04-09 23:19:51.671686: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-09 23:19:51.671809: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 23:19:51.749441: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 23:19:55.806874: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 23:19:55.863258: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 23:19:55.908653: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 23:19:57.024557: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-09 23:19:57.080375: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Return: -182.2084788400984. Average Length of Episode: 90.9090909090909. Total Episodes Trained: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026d6930973d46e28a772d168556fa7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:2:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Return: -140.75764720822954. Average Length of Episode: 83.33333333333333. Total Episodes Trained: 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c812839e7c63468ca7856b71924e7028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Episodes:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over 20 episodes: -183.44262295687705.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent(env_name='LunarLander-v2', render=False,epochs=2)\n",
    "ppo_agent.run(export=True)\n",
    "ppo_agent.test(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Analysis for Hyperparameter Tuning \n",
    "Here we test out different Hyperparameters such as learning rate, model structure and training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures_models = np.array([[512,256,128],[256,128,64],[128,64,32]])\n",
    "learninig_rates = np.array([1e-4,3e-4])\n",
    "trainig_iterations = np.array([10,20,50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
