{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Environment\n",
    "import gym\n",
    "\n",
    "# Further support\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('training_graphs')\n",
    "    print('Successfully created folder for graphic exports.')\n",
    "except FileExistsError:\n",
    "    print('Folder alerady exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Storage 2.0\n",
    "Numpy Version with full-batch return of epoch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward_sum(rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return sum(discounted_rewards)   \n",
    "\n",
    "\n",
    "def get_advantage(rewards, b_estimates, gamma = 0.99):\n",
    "    '''\n",
    "    Computes Advantage for action in state.\n",
    "\n",
    "    Args:\n",
    "    rewards(float): Reward for action.\n",
    "    gamma(float): Discount factor.\n",
    "    b_estimates(float): Baseline Estimates.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Get discounted sum of rewards \n",
    "    disc_sum = discounted_reward(rewards, gamma)\n",
    "\n",
    "    # Convert lists to np arrays and flatten\n",
    "    disc_sum_np = np.array(disc_sum, dtype=np.float32)\n",
    "\n",
    "    # substract arrays to obtain advantages\n",
    "    advantages = np.subtract(disc_sum_np, b_estimates, dtype=np.float32)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "\n",
    "\n",
    "def discounted_reward(rewards, gamma = 0.99):\n",
    "    '''\n",
    "    weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "    Args:\n",
    "    rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "    gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "    '''\n",
    "    # To select the next reward\n",
    "    i = 0\n",
    "    discounted_rewards = []\n",
    "\n",
    "    # Iterates through every reward and appends a discounted version to the output\n",
    "    for r in rewards:\n",
    "        disc = 0\n",
    "        for t in rewards[i:-1]:\n",
    "            discount_t = gamma ** t\n",
    "            disc += t * discount_t\n",
    "        i += 1\n",
    "        discounted_rewards.append(disc)\n",
    "\n",
    "    # returns list of discounted rewards.\n",
    "    return discounted_rewards   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "\n",
    "    def __init__(self, observation_dimension, size):\n",
    "        self.observations = np.zeros((size, observation_dimension), dtype=np.float32)\n",
    "        self.actions = np.zeros(size, dtype=np.int32)\n",
    "        self.rewards = np.zeros(size, dtype=np.float32)\n",
    "        self.episode_return = np.zeros(size, dtype=np.float32)\n",
    "        self.baseline_estimates = np.zeros(size, dtype=np.float32)\n",
    "        self.advantages = np.zeros(size, dtype=np.float32)\n",
    "        \n",
    "        self.pointer_start, self.pointer_end= 0,0\n",
    "        \n",
    "\n",
    "    def store(self, observation, action, reward, baseline_estimate):\n",
    "        self.observations[self.pointer_end] = observation\n",
    "        self.actions[self.pointer_end] = action\n",
    "        self.rewards[self.pointer_end] = reward\n",
    "        self.baseline_estimates[self.pointer_end] = baseline_estimate\n",
    "        self.pointer_end += 1\n",
    "\n",
    "    def conclude_episode(self, last_value = 0):\n",
    "        indexes = slice(self.pointer_start, self.pointer_end)\n",
    "        self.episode_return = discounted_reward_sum(self.rewards[indexes])\n",
    "        self.advantages[indexes] = get_advantage(self.rewards[indexes], self.baseline_estimates[indexes])\n",
    "        self.pointer_start = self.pointer_end\n",
    "\n",
    "    def get_episodes(self,actor):\n",
    "        self.pointer_start, self.pointer_end = 0,0\n",
    "\n",
    "        return self.observations, self.actions, self.rewards, np.mean(self.episode_return), self.baseline_estimates, actor.get_prob(self.actions, self.observations), self.advantages\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, actionspace, struct=[256,128,64]):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "\n",
    "        Args:\n",
    "        actionspace(): number of possible actions the agent can take\n",
    "        struct(): Width of layers in the network - must be of length = 3 \n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionspace = actionspace\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(self.actionspace, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    @tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #@tf.function\n",
    "\n",
    "    ## logits nur an stelle action zurück\n",
    "    @tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "\n",
    "        logits = self(observation)\n",
    "\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "        return logits, action\n",
    "\n",
    "\n",
    "    #@tf.function\n",
    "    def get_prob(self,actions, states):\n",
    "\n",
    "        logits = self.call(states)\n",
    "        logits_flat = tf.squeeze(logits)\n",
    "\n",
    "\n",
    "        index_first_dim = tf.range(len(logits_flat))\n",
    "        index_2D = tf.stack([index_first_dim, actions], axis=1)\n",
    "\n",
    "        new_probs = tf.gather_nd(logits_flat, index_2D)\n",
    "        \n",
    "        return new_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "\n",
    "        Args:\n",
    "        struct(): Width of layers in the network - must be of length = 3 \n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    @tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "  \n",
    "    '''\n",
    "\n",
    "    def __init__(self, env_name, render=False, steps_per_epoch=1000, epochs=100, train_iterations=10,learning_rate=3e-4, actor_structure=[256,128,64], critic_structure=[256,128,64]):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        \n",
    "        Args:\n",
    "        env_name(): String Name of the Environment Passed\n",
    "        render(): Boolean determining if env should be rendered during training\n",
    "        steps_per_epoch(): how many steps/frame the agent should take during each Epoch of training; Default=1000\n",
    "        epochs(): How many epochs of training should the agent do; Default=100\n",
    "        actor_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        critic_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        '''\n",
    "        # create environemt\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_dimensions = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # set Hyperparameters\n",
    "        self.lr = learning_rate\n",
    "        self.clip_ratio = 0.2\n",
    "        self.c_1 = 0.5\n",
    "        self.train_policy_iterations = train_iterations\n",
    "        self.optimizer = Adam(self.lr)\n",
    "        self.render = render\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.actor_struct = actor_structure\n",
    "        self.critic_struct = critic_structure\n",
    "        \n",
    "        # create models and temporary storage\n",
    "        self.actor = Actor(self.num_actions,self.actor_struct)\n",
    "        self.critic = Critic(self.critic_struct)\n",
    "        self.storage = Storage(self.observation_dimensions, size=steps_per_epoch)\n",
    "\n",
    "\n",
    "    def collect_train_data(self, epoch):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "\n",
    "        Args: \n",
    "        epoch(): Epoch inherited from wrapper function 'run' to correctly show epoch in TQDM Notebook\n",
    "        '''\n",
    "        observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        \n",
    "\n",
    "        # Initialize values for return, length and episodes\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "\n",
    "        # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "        #  allows takes on action in a state and saves the information in storage object\n",
    "        for t in tqdm_notebook(range(self.steps_per_epoch), desc = 'Epoch:' + str(epoch+1)):\n",
    "\n",
    "            # Toggles displaying of environment\n",
    "            if self.render:\n",
    "                self.env.render()\n",
    "\n",
    "            # Reshaping observation to fit as input for Actor network (policy)\n",
    "            observation = observation.reshape(1,-1)\n",
    "            \n",
    "            # Obtain action and logits for this observation by our actor\n",
    "            logits, action = self.actor.sample_action(observation)\n",
    "            \n",
    "            # Take action in environment and obtain the rewards for it\n",
    "            # Variable done represents wether agent has finished \n",
    "            # The last variable would be diagnostic information, not needed for training\n",
    "            observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "\n",
    "            # Sum up rewards over this episode and count amount of frames\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the Base-Estimate from the Critics network\n",
    "            base_estimate = self.critic(observation)\n",
    "\n",
    "            # Store Variables collected in this timestep t\n",
    "            self.storage.store(observation, action, reward, base_estimate)\n",
    "            # Save the new state of our agent\n",
    "            observation = observation_new\n",
    "            \n",
    "            # Check if terminal state is reached in environment\n",
    "            if done or (t == self.steps_per_epoch - 1):\n",
    "                # Save information about episode\n",
    "                self.storage.conclude_episode()\n",
    "                # Refresh environment and reset return and length value\n",
    "                num_episodes += 1\n",
    "                sum_length += episode_length\n",
    "                sum_return += episode_return\n",
    "                observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "\n",
    "        return num_episodes, sum_length/num_episodes, sum_return/num_episodes\n",
    "                \n",
    "\n",
    "\n",
    "    def actor_loss_fun(self,probs_old, probs_new, rewards, b_estimates, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "        ratio = self.get_ratio(probs_old, probs_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "    \n",
    "        l2 = tf.clip_by_value(ratio, clip_value_min=1-clip_param, clip_value_max=1+clip_param) * advantage\n",
    "\n",
    "        return l1, l2\n",
    "\n",
    "\n",
    "    def train_step(self, observations, actions, optimizer, rewards_old, probs_old,returns, advantages, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # obtain the probs of actions at observed states for the new policy\n",
    "            probs_new = self.actor.get_prob(actions, observations)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen für den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "            entropy = K.mean((-(probs_new * K.log(probs_new)) + 1e-10)) * c_2\n",
    "            #print(entropy)\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()            \n",
    "            critic_loss = tf.reduce_mean((returns - self.critic(observations)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(probs_old, probs_new, rewards_old, self.critic(observations), clip_param)\n",
    "\n",
    "\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "     \n",
    "        a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        c_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "    \n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_rewards(rewards)\n",
    "\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum,b_estimates)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "\n",
    "    def discounted_rewards(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        for i in reversed(range(0,len(rewards))):\n",
    "            running_add = running_add * gamma + rewards[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_ratio(self, probs_old, probs_new):\n",
    "        '''\n",
    "        Returns the ratio of the logarithmic probability given by the old policy and the  logarithmic probability given by the new policy for an action in a state\n",
    "\n",
    "        Args:\n",
    "        probs_old():\n",
    "        probs_new():\n",
    "        '''\n",
    "\n",
    "        log_probs_old = tf.nn.log_softmax(probs_old)\n",
    "        log_probs_new = tf.nn.log_softmax(probs_new)\n",
    "\n",
    "        ratio = tf.divide(log_probs_new,log_probs_old)\n",
    "        return ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, export = False):\n",
    "        '''\n",
    "        Wrapper funtion that can directly be called to train the agent.\n",
    "\n",
    "        Args:\n",
    "        export(bool): whether the Scores during Training should be exported and saved as a plot\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "        mean_returns = []\n",
    "        mean_lengths = []\n",
    "        num_episodes_total = 0\n",
    "        for epoch in tqdm_notebook(range(self.epochs), desc = 'Epochs'):\n",
    "            \n",
    "            num_episodes, mean_length, mean_return = self.collect_train_data((epoch))\n",
    "            observations, actions, rewards, returns, baseline_estimates, probs, advantages = self.storage.get_episodes(self.actor)\n",
    "            for i in range(self.train_policy_iterations):\n",
    "                self.train_step(observations, actions, self.optimizer, rewards, probs,returns, advantages, self.clip_ratio)\n",
    "            num_episodes_total += num_episodes\n",
    "            mean_returns.append(mean_return)\n",
    "            mean_lengths.append(mean_length)\n",
    "            print(f'Mean Return: {mean_return}. Average Length of Episode: {mean_length}. Total Episodes Trained: {num_episodes_total}')\n",
    "        \n",
    "        trainings_duration = (time.time() - start_time)\n",
    "        trainings_duration_minutes, rest_seconds = (trainings_duration / 60), (trainings_duration % 60)\n",
    "        time_formatted = f' Time trained : {int(trainings_duration_minutes)}:{int(rest_seconds)} m.'\n",
    "        print(time_formatted)\n",
    "\n",
    "        # export the training progress return values as png graph \n",
    "        if export:\n",
    "            plt.figure(figsize=(10,6), dpi=300)\n",
    "            plt.ylabel('Average score per epoch')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.title(f'Model Structure: {self.actor_struct}, LR: {self.lr}, training iterations: {self.train_policy_iterations}, {time_formatted}', loc='left')\n",
    "            plt.plot(range(self.epochs), mean_returns)\n",
    "            plt.savefig(f'training_graphs/Model Structure:{self.actor_struct} LR:{self.lr} training_iterations:{self.train_policy_iterations} figure.png',facecolor='white', edgecolor='none')\n",
    "\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def test(self, length=100):\n",
    "        '''\n",
    "        Tests the agent on a number of episodes and return the average score.\n",
    "        Args: \n",
    "        length(int): number of episodes to test the agent over\n",
    "        export(bool): whether the \n",
    "        '''\n",
    "        reward_sum = 0\n",
    "        for i in tqdm_notebook(range(length), desc = 'Test Episodes'):\n",
    "            done = False\n",
    "            observation = self.env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                self.env.render()        \n",
    "                observation = observation.reshape(1,-1)\n",
    "                _ , action = self.actor.sample_action(observation)\n",
    "                observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "                reward_sum += reward\n",
    "                observation = observation_new\n",
    "        print(f'Average score over {length} episodes: {reward_sum/length}.')\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppo_agent = Agent(env_name='LunarLander-v2', render=False,epochs=20)\n",
    "ppo_agent.run()\n",
    "ppo_agent.test(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Analysis for Hyperparameter Tuning \n",
    "Here we test out different Hyperparameters such as learning rate, model structure and training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures_models = np.array([[512,256,128],[256,128,64],[128,64,32]])\n",
    "learninig_rates = np.array([1e-4,3e-4])\n",
    "trainig_iterations = np.array([10,20,50])\n",
    "\n",
    "# # Test different model Structs (Niki: 1&2, Tom: 3)\n",
    "# ppo_agent1 = Agent('LunarLander-v2', epochs= 300, train_iterations=trainig_iterations[0], actor_structure=structures_models[0], learning_rate=learninig_rates[1])\n",
    "# ppo_agent1.run(export=True)\n",
    "# ppo_agent1.test(length=50)\n",
    "\n",
    "# ppo_agent2 = Agent('LunarLander-v2', epochs= 300, train_iterations=trainig_iterations[0], actor_structure=structures_models[1], learning_rate=learninig_rates[1])\n",
    "# ppo_agent2.run(export=True)\n",
    "# ppo_agent2.test(length=50)\n",
    "\n",
    "# ppo_agent3 = Agent('LunarLander-v2', epochs= 300, train_iterations=trainig_iterations[0], actor_structure=structures_models[2], learning_rate=learninig_rates[1])\n",
    "# ppo_agent3.run(export=True)\n",
    "# ppo_agent3.test(length=50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
