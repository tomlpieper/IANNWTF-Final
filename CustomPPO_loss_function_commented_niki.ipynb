{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        # Outputs from the actor network, an action is sampled from (Probabilities)\n",
    "        self.logits = []\n",
    "        # Outputs from the crtitics network (Values)\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) \n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collected values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards)])\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, actionspace):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionspace = actionspace\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(self.actionspace, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "    #@tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "        #logits = self.call(observation)\n",
    "        logits = self(observation)\n",
    "    # tf.print(type(logits))\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    # tf.print(action)\n",
    "        return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "  \n",
    "    '''\n",
    "\n",
    "    def __init__(self, env_name, render=False, steps_per_epoch=1000, epochs=100):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        \n",
    "        Args:\n",
    "        env_name(): String Name of the Environment Passed\n",
    "        render(): Boolean determining if env should be rendered during training\n",
    "        steps_per_epoch(): how many steps/frame the agent should take during each Epoch of training; Default=1000\n",
    "        epochs(): How many epochs of training should the agent do; Default=100\n",
    "        '''\n",
    "        # create environemt\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_dimensions = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.actor = Actor(self.num_actions)\n",
    "        self.critic = Critic()\n",
    "        self.storage = Storage()\n",
    "        # set Hyperparameters\n",
    "        self.lr = 3e-4\n",
    "        self.clip_ratio = 0.2\n",
    "        self.c_1 = 0.5\n",
    "        self.optimizer = Adam()\n",
    "        self.render = render\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        \n",
    "\n",
    "\n",
    "    def collect_train_data(self):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        for epoch in tqdm_notebook(range(self.epochs), desc = 'Epochs'):\n",
    "\n",
    "            # Initialize values for return, length and episodes\n",
    "            sum_return = 0\n",
    "            sum_length = 0\n",
    "            num_episodes = 0\n",
    "\n",
    "            # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "            #  allows takes on action in a state and saves the information in storage object\n",
    "            for t in tqdm_notebook(range(self.steps_per_epoch), desc = 'Epoch:' + str(epoch)):\n",
    "\n",
    "                # Toggles displaying of environment\n",
    "                if self.render or epoch == self.epochs-1 and self.epochs != 1:\n",
    "                    self.env.render()\n",
    "\n",
    "                # Reshaping observation to fit as input for Actor network (policy)\n",
    "                observation = observation.reshape(1,-1)\n",
    "                \n",
    "                # Obtain action and logits for this observation by our actor\n",
    "                logits, action = self.actor.sample_action(observation)\n",
    "                \n",
    "                # Take action in environment and obtain the rewards for it\n",
    "                # Variable done represents wether agent has finished \n",
    "                # The last variable would be diagnostic information, not needed for training\n",
    "                observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "\n",
    "                # Sum up rewards over this episode and count amount of frames\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "\n",
    "                # Get the Base-Estimate from the Critics network\n",
    "                base_estimate = self.critic(observation)\n",
    "\n",
    "                # Store Variables collected in this timestep t\n",
    "                self.storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "                # Save the new state of our agent\n",
    "                observation = observation_new\n",
    "                \n",
    "                # Check if terminal state is reached in environment\n",
    "                if done:\n",
    "                    # Save information about episode\n",
    "                    self.storage.conclude_episode()\n",
    "                    # Refresh environment and reset return and length value\n",
    "                    observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "\n",
    "            # obtain all episodes saved in storage\n",
    "            # episodes, amount_episodes = self.storage.get_episodes()\n",
    "\n",
    "\n",
    "    def actor_loss_fun(self, actions, logits_old, logits_new, rewards, b_estimates_new, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "        ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates_new)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "        #l1 = np.array(l1, dtype=\"float32\")\n",
    "        #l2 = np.array(l2, dtype=\"float32\")\n",
    "        \n",
    "\n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        #actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "        l1 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l1]), dtype=tf.float32)\n",
    "        l2 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l2]), dtype=tf.float32)\n",
    "        return tf.convert_to_tensor(l1, dtype=tf.float32), tf.convert_to_tensor(l2, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape() as tape, tf.GradientTape() as tape2:\n",
    "            # print(self.actor.trainable_variables())\n",
    "            # Obtain action and logits for this state selected by policy\n",
    "            #print(f' Observation shape/type {observation}')\n",
    "            #print(f'Trainables: {self.actor.layers[0].weights}')\n",
    "\n",
    "\n",
    "            # logits_new, actions_new = sample_action(states)\n",
    "            logits_new = []\n",
    "            b_estimates_new = []\n",
    "\n",
    "            # Compute values with updated critic network\n",
    "            # b_estimates_new = critic(states)\n",
    "\n",
    "            # till we work with np arrays we need to sample each action for this by looping through it\n",
    "            for i in states:\n",
    "                logits, _ = self.actor.sample_action(i)\n",
    "                logits_new.append(logits)\n",
    "                b_estimate = self.critic(i)\n",
    "                \n",
    "                b_estimates_new.append(b_estimate)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen fÃ¼r den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()\n",
    "            #print('Weewoo')\n",
    "            #print(tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print('type critic')\n",
    "            print(type((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "\n",
    "            critic_loss = tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            #print('minimum')\n",
    "            #print(-tf.reduce_mean(tf.minimum(l1, l2)))\n",
    "            #print(type(tf.minimum(l1,l2)))\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "            #critic_loss = tf.cast(critic_loss, dtype=tf.float32)\n",
    "            #print(f'Critics loss:{type(critic_loss)}. Actor Loss {actor_loss.dtype}')\n",
    "\n",
    "            #print('Actor weights')\n",
    "            #print(print(self.actor.layers[0].weights))\n",
    "\n",
    "            #print('actor')\n",
    "            #print(actor_loss)\n",
    "            #print(type(actor_loss))\n",
    "            #print('critic')\n",
    "            #print(critic_loss)\n",
    "            #print(type(critic_loss))\n",
    "\n",
    "            actor_loss = tf.convert_to_tensor(actor_loss, dtype=tf.float32)\n",
    "\n",
    "            print(actor_loss)\n",
    "            print(critic_loss)\n",
    "            print('actor')\n",
    "            print(self.actor.trainable_variables)\n",
    "            print('critic')\n",
    "            print(self.critic.trainable_variables)\n",
    "            a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            c_gradients = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            print(a_gradients)\n",
    "            print(c_gradients)\n",
    "\n",
    "            #print(tape)\n",
    "            #print('Actor loss')\n",
    "            #print(actor_loss)\n",
    "            #print('Trainable Weights')\n",
    "            #print(self.actor.trainable_weights)\n",
    "        \n",
    "        #print(f'Gradients Actor: {a_gradients}. Gradients Critic: {c_gradients}')\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def update_policy(self, episodes, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                        [look at single one]\n",
    "        '''\n",
    "        # for epoch in training_iteratins:\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in tqdm_notebook(episodes):\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                # States\n",
    "                episode[0],\n",
    "                # Actions\n",
    "                episode[1],\n",
    "                #optimizer (Adam)\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2],\n",
    "                # Rewards\n",
    "                episode[3],\n",
    "                clip_param,\n",
    "                c_1,\n",
    "                c_2 \n",
    "            )\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "            return train_losses_actor, train_losses_critic\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # # Estimated Value of the current situtation from the critics network\n",
    "        # b_estimates = self.episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "    ## get ratio lutsch noch ARSCH, das Ding verarscht mich anders\n",
    "\n",
    "    def get_ratio_episode(self, actions, logits_old, logits_new): \n",
    "        r = []\n",
    "        for a, o, n in zip(actions, logits_old, logits_new):\n",
    "            o = tf.convert_to_tensor(o)\n",
    "            n = tf.convert_to_tensor(n)\n",
    "            #print(f'A: {a} O: {type(o)} N: {type(n)}')\n",
    "\n",
    "            #get the Logarithmic version of all logits for computational efficiency\n",
    "            log_prob_old = tf.nn.log_softmax(o)\n",
    "            log_prob_new = tf.nn.log_softmax(n)\n",
    "\n",
    "            # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "            logprobability_old = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_old, axis=1\n",
    "            )\n",
    "            logprobability_new = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_new, axis=1\n",
    "            )\n",
    "            # get the ratio of new over old prob\n",
    "            ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "            r.append(ratio)\n",
    "        return r\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.collect_train_data()\n",
    "        data, _ = self.storage.get_episodes()\n",
    "        #print(data)\n",
    "        self.update_policy(data, self.optimizer, self.clip_ratio)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a053e78fc8b541a7a1de1c4404c5e9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97fb929534c4ab7a688c6bb436bc4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704b5e99e26e458ca3b238697099319d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type critic\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[[ 2.184144   1.3503168  4.229442  ... 11.596705   7.184944   5.525864 ]]\n",
      "\n",
      " [[ 2.184139   1.3503127  4.2294345 ... 11.596719   7.184953   5.525873 ]]\n",
      "\n",
      " [[ 2.1841278  1.350304   4.22942   ... 11.596742   7.1849737  5.52589  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.1840985  1.350281   4.2293787 ... 11.596811   7.185027   5.525937 ]]\n",
      "\n",
      " [[ 2.1840942  1.3502777  4.229373  ... 11.596821   7.1850343  5.5259438]]\n",
      "\n",
      " [[ 2.1840975  1.3502802  4.229378  ... 11.596812   7.185028   5.525938 ]]], shape=(55, 1, 55), dtype=float32)\n",
      "tf.Tensor(71.18381, shape=(), dtype=float32)\n",
      "tf.Tensor(7.645477, shape=(), dtype=float32)\n",
      "actor\n",
      "[<tf.Variable 'actor/dense/kernel:0' shape=(8, 128) dtype=float32, numpy=\n",
      "array([[-1.1308892e-02, -1.7355127e-02,  1.4617484e-02, ...,\n",
      "         7.7918665e-03, -1.2149482e-03,  1.1708942e-02],\n",
      "       [-5.6339898e-03,  1.1871940e-02, -3.2164247e-03, ...,\n",
      "         1.5997957e-02, -8.8565255e-05,  7.6529449e-03],\n",
      "       [ 7.4402371e-05,  9.2703337e-03, -2.2002612e-03, ...,\n",
      "         2.3353538e-02,  5.5125421e-03,  1.6654283e-02],\n",
      "       ...,\n",
      "       [-1.5461081e-02, -1.3530107e-03,  1.4057566e-02, ...,\n",
      "         1.6799183e-03,  4.7297953e-03,  1.0336874e-02],\n",
      "       [ 9.0182861e-03, -6.8201423e-03,  1.6932411e-02, ...,\n",
      "        -3.8476523e-03, -3.7253860e-03, -8.6598042e-03],\n",
      "       [ 2.1894097e-04, -1.8342618e-02,  1.5183438e-02, ...,\n",
      "         8.4689986e-03,  5.7382844e-03,  3.3946114e-03]], dtype=float32)>, <tf.Variable 'actor/dense/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[ 2.2657779e-03, -1.7283890e-02, -4.3896073e-03, ...,\n",
      "         9.4167178e-04,  1.8003359e-02, -9.1539761e-03],\n",
      "       [-2.7941731e-03, -2.9509424e-03, -5.6459224e-03, ...,\n",
      "         2.6931071e-03,  1.3929374e-02, -3.8496732e-06],\n",
      "       [ 4.3166704e-03,  1.6031874e-02, -1.6114162e-02, ...,\n",
      "         3.1683287e-03,  2.6926197e-02, -4.7587580e-03],\n",
      "       ...,\n",
      "       [-5.4885107e-03,  8.3536776e-03, -1.0122832e-02, ...,\n",
      "        -1.7820664e-02, -1.8281270e-02, -6.0325861e-03],\n",
      "       [-1.1064194e-02,  2.0068783e-02,  6.9486629e-03, ...,\n",
      "         7.1593933e-03,  3.8889369e-03, -5.3896448e-03],\n",
      "       [-1.8809100e-03, -7.6064006e-03,  1.3267115e-02, ...,\n",
      "         3.4782130e-03, -3.6236126e-04, -9.2724450e-03]], dtype=float32)>, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.0118261 ,  0.01489335, -0.00536757, ..., -0.01612636,\n",
      "        -0.00727129,  0.00395092],\n",
      "       [ 0.00620107, -0.00555016, -0.01394565, ...,  0.014705  ,\n",
      "         0.01103642,  0.00129992],\n",
      "       [ 0.00195672, -0.00530112, -0.0067015 , ...,  0.00082253,\n",
      "         0.00058043,  0.00864953],\n",
      "       ...,\n",
      "       [-0.00789423,  0.01028978,  0.00268362, ..., -0.00884427,\n",
      "         0.00767427,  0.01623529],\n",
      "       [-0.0014378 ,  0.0129225 ,  0.00302604, ..., -0.01237711,\n",
      "        -0.00741295,  0.00687513],\n",
      "       [-0.00180506, -0.01141407,  0.00332733, ..., -0.02268454,\n",
      "         0.00279521, -0.00713064]], dtype=float32)>, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\n",
      "array([[-0.02158797, -0.07005644,  0.1206525 ,  0.0365369 ],\n",
      "       [ 0.18660891, -0.06472568, -0.16434792, -0.03862917],\n",
      "       [-0.2661875 , -0.11951412,  0.1785737 ,  0.10809135],\n",
      "       [ 0.240453  , -0.2750672 , -0.13723223,  0.14678413],\n",
      "       [ 0.25027835, -0.13852471, -0.12773119,  0.01944196],\n",
      "       [ 0.00147703,  0.03144774,  0.18875831,  0.22032857],\n",
      "       [ 0.21957308,  0.05811724, -0.13629265, -0.1511785 ],\n",
      "       [-0.20247859, -0.00369778,  0.2371782 ,  0.10556936],\n",
      "       [-0.2376012 , -0.17829262, -0.05658454, -0.0329048 ],\n",
      "       [ 0.23479265,  0.06495458,  0.07955492,  0.12124676],\n",
      "       [-0.11125916, -0.04413527,  0.16306004, -0.12030703],\n",
      "       [-0.18300807, -0.12298696,  0.25498873, -0.01637203],\n",
      "       [-0.2599401 , -0.14809488,  0.09497434, -0.02876723],\n",
      "       [ 0.26280648, -0.22155036,  0.227103  , -0.19129029],\n",
      "       [-0.2747692 , -0.2207333 , -0.04827406, -0.19222832],\n",
      "       [-0.13614683,  0.2253288 ,  0.1821236 ,  0.21527308],\n",
      "       [ 0.02762213, -0.0318478 ,  0.2834422 , -0.2557096 ],\n",
      "       [-0.02377769, -0.27592692,  0.23023987,  0.17955449],\n",
      "       [ 0.17044488, -0.21899578,  0.23448962,  0.22567242],\n",
      "       [-0.10451518, -0.2110897 , -0.2934195 ,  0.08735132],\n",
      "       [ 0.01416114, -0.2706916 ,  0.17301002, -0.20412892],\n",
      "       [-0.28120255,  0.11676139, -0.00532353, -0.14611827],\n",
      "       [-0.14638016,  0.1130845 , -0.20672593,  0.26536858],\n",
      "       [-0.22903329, -0.04051316, -0.00305903,  0.29281092],\n",
      "       [ 0.16146919, -0.00070664,  0.00197634, -0.2966369 ],\n",
      "       [ 0.08681753, -0.07763796,  0.2741887 , -0.24430914],\n",
      "       [ 0.21542966,  0.12947473,  0.18409666,  0.0149481 ],\n",
      "       [-0.17874232,  0.2561506 , -0.05561967, -0.03979835],\n",
      "       [ 0.11216828,  0.2116636 , -0.0645283 ,  0.15592349],\n",
      "       [-0.09696263, -0.16443184,  0.07529533, -0.09715711],\n",
      "       [ 0.25344998, -0.03239664, -0.21323244,  0.20436382],\n",
      "       [-0.25758415,  0.20445311,  0.19473568,  0.12740457],\n",
      "       [-0.07417367,  0.17925054, -0.19096658,  0.23697805],\n",
      "       [ 0.17919558,  0.2916116 ,  0.00252199,  0.10983044],\n",
      "       [-0.10092463, -0.2968236 , -0.15595104,  0.11739743],\n",
      "       [ 0.05228981, -0.15935186,  0.26877135,  0.1012589 ],\n",
      "       [-0.29042473, -0.13362022,  0.15681258,  0.21364486],\n",
      "       [ 0.12989044,  0.22284108, -0.11519355,  0.19709966],\n",
      "       [ 0.08760038, -0.27823058,  0.02493829, -0.08668673],\n",
      "       [ 0.12108457, -0.16910708,  0.10571155, -0.22515726],\n",
      "       [-0.11943579, -0.00284383,  0.22702181, -0.06079845],\n",
      "       [ 0.13620609,  0.24687487, -0.06993058, -0.04499179],\n",
      "       [-0.22452909,  0.1907573 , -0.10236576, -0.17476007],\n",
      "       [ 0.129051  , -0.21703744, -0.1347139 , -0.14070578],\n",
      "       [-0.03918433, -0.16551398, -0.08996709, -0.10164595],\n",
      "       [ 0.29064655, -0.06536901, -0.00285989, -0.02880201],\n",
      "       [-0.16466221,  0.06134015, -0.26548764,  0.11443838],\n",
      "       [-0.1457658 ,  0.00475648,  0.24825889, -0.13648535],\n",
      "       [ 0.08494809, -0.25637484,  0.1826942 , -0.04963148],\n",
      "       [-0.11487161, -0.04434252,  0.10491064, -0.26543918],\n",
      "       [ 0.00058171,  0.02626464,  0.0354214 , -0.13539733],\n",
      "       [-0.00948277, -0.27872935,  0.12518665, -0.08339965],\n",
      "       [ 0.1053969 , -0.28872246,  0.02776739,  0.2543745 ],\n",
      "       [-0.0369314 ,  0.03190628,  0.09900963, -0.00712848],\n",
      "       [-0.10224204,  0.07233664, -0.21551457,  0.06509438],\n",
      "       [ 0.18081611, -0.27546707, -0.28287494,  0.15028629],\n",
      "       [-0.1606039 ,  0.16512665,  0.0204159 , -0.07033597],\n",
      "       [ 0.28629142, -0.24938324, -0.03003052, -0.19869992],\n",
      "       [-0.15548794,  0.06056374, -0.02504247, -0.11444908],\n",
      "       [-0.23291966, -0.2887753 ,  0.02025867,  0.06195474],\n",
      "       [-0.11439279,  0.1726465 , -0.20391476, -0.07539561],\n",
      "       [ 0.22371233,  0.08779812,  0.26678282, -0.24339506],\n",
      "       [ 0.22273314,  0.28431982, -0.2908014 ,  0.2913713 ],\n",
      "       [-0.27033517, -0.11323628, -0.01478845, -0.03955027]],\n",
      "      dtype=float32)>, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "critic\n",
      "[<tf.Variable 'critic/dense_4/kernel:0' shape=(8, 128) dtype=float32, numpy=\n",
      "array([[ 0.00180935,  0.03409453,  0.00270764, ..., -0.01713218,\n",
      "        -0.00153701, -0.02081805],\n",
      "       [-0.01074689, -0.00521406,  0.01294268, ..., -0.00812024,\n",
      "         0.00276553, -0.00538763],\n",
      "       [-0.0006649 ,  0.00039341, -0.00484779, ..., -0.00352984,\n",
      "        -0.00912932,  0.00860144],\n",
      "       ...,\n",
      "       [-0.0050434 ,  0.01641911, -0.00234771, ...,  0.00869241,\n",
      "         0.01544451, -0.01431786],\n",
      "       [ 0.00487787,  0.00881763, -0.01700454, ...,  0.00880557,\n",
      "         0.01381362, -0.00929718],\n",
      "       [ 0.00255231, -0.00655538,  0.00711972, ..., -0.00802004,\n",
      "         0.00527294,  0.00452909]], dtype=float32)>, <tf.Variable 'critic/dense_4/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_5/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[-0.00231491,  0.00071918,  0.00018816, ..., -0.00068034,\n",
      "        -0.00221476,  0.00343599],\n",
      "       [ 0.01480083,  0.0157796 , -0.01826198, ...,  0.00361824,\n",
      "        -0.00255027, -0.00306962],\n",
      "       [ 0.01987266, -0.00974116,  0.0017666 , ...,  0.01291076,\n",
      "         0.00233625, -0.00300335],\n",
      "       ...,\n",
      "       [-0.0095742 ,  0.01563353,  0.00344008, ..., -0.01015553,\n",
      "         0.02311411, -0.00642549],\n",
      "       [ 0.00445191, -0.00675452, -0.01260975, ...,  0.03632493,\n",
      "        -0.0148974 ,  0.00959322],\n",
      "       [-0.00263191,  0.00210176,  0.00179283, ...,  0.01124489,\n",
      "         0.00941002, -0.01134286]], dtype=float32)>, <tf.Variable 'critic/dense_5/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_6/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.01600041, -0.0069973 ,  0.00584872, ..., -0.00662256,\n",
      "         0.00258515, -0.00511604],\n",
      "       [-0.01584217, -0.00027101, -0.00178092, ..., -0.0035335 ,\n",
      "         0.00161922,  0.01648252],\n",
      "       [-0.00726093,  0.01500833, -0.00916257, ..., -0.00240837,\n",
      "        -0.00166485,  0.01106702],\n",
      "       ...,\n",
      "       [-0.00085057, -0.0163225 ,  0.00069592, ...,  0.00382438,\n",
      "        -0.00962556,  0.00436535],\n",
      "       [-0.01664725, -0.00530935,  0.00257922, ..., -0.00071445,\n",
      "        -0.02224498, -0.00132334],\n",
      "       [ 0.00283315,  0.00993836,  0.01290617, ..., -0.00841133,\n",
      "         0.00389276, -0.01149031]], dtype=float32)>, <tf.Variable 'critic/dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_7/kernel:0' shape=(64, 1) dtype=float32, numpy=\n",
      "array([[-0.02208054],\n",
      "       [-0.07165489],\n",
      "       [ 0.12340537],\n",
      "       [ 0.03737056],\n",
      "       [ 0.19086668],\n",
      "       [-0.06620249],\n",
      "       [-0.16809776],\n",
      "       [-0.03951055],\n",
      "       [-0.27226096],\n",
      "       [-0.12224102],\n",
      "       [ 0.18264815],\n",
      "       [ 0.11055762],\n",
      "       [ 0.24593931],\n",
      "       [-0.2813433 ],\n",
      "       [-0.1403634 ],\n",
      "       [ 0.15013322],\n",
      "       [ 0.25598884],\n",
      "       [-0.14168537],\n",
      "       [-0.13064557],\n",
      "       [ 0.01988557],\n",
      "       [ 0.00151074],\n",
      "       [ 0.03216526],\n",
      "       [ 0.19306514],\n",
      "       [ 0.22535568],\n",
      "       [ 0.22458303],\n",
      "       [ 0.05944327],\n",
      "       [-0.13940237],\n",
      "       [-0.15462786],\n",
      "       [-0.20709844],\n",
      "       [-0.00378215],\n",
      "       [ 0.24258977],\n",
      "       [ 0.10797808],\n",
      "       [-0.24302244],\n",
      "       [-0.18236063],\n",
      "       [-0.0578756 ],\n",
      "       [-0.03365555],\n",
      "       [ 0.2401498 ],\n",
      "       [ 0.06643662],\n",
      "       [ 0.08137009],\n",
      "       [ 0.12401319],\n",
      "       [-0.11379772],\n",
      "       [-0.04514229],\n",
      "       [ 0.1667805 ],\n",
      "       [-0.12305202],\n",
      "       [-0.1871837 ],\n",
      "       [-0.1257931 ],\n",
      "       [ 0.26080668],\n",
      "       [-0.01674557],\n",
      "       [-0.26587102],\n",
      "       [-0.1514739 ],\n",
      "       [ 0.09714133],\n",
      "       [-0.02942359],\n",
      "       [ 0.26880282],\n",
      "       [-0.22660537],\n",
      "       [ 0.23228472],\n",
      "       [-0.19565488],\n",
      "       [-0.2810385 ],\n",
      "       [-0.22576967],\n",
      "       [-0.0493755 ],\n",
      "       [-0.19661431],\n",
      "       [-0.13925323],\n",
      "       [ 0.23047   ],\n",
      "       [ 0.18627903],\n",
      "       [ 0.22018486]], dtype=float32)>, <tf.Variable 'critic/dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "[None, None, None, None, None, None, None, None]\n",
      "[<tf.Tensor: shape=(8, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  0.0000000e+00,  1.0076451e-04, ...,\n",
      "         0.0000000e+00, -1.7708480e-05,  6.2811872e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  3.1536166e-03, ...,\n",
      "         0.0000000e+00, -1.3622598e-03,  1.5051573e-03],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  4.4427166e-04, ...,\n",
      "         0.0000000e+00, -1.1104095e-04,  2.5892135e-04],\n",
      "       ...,\n",
      "       [ 0.0000000e+00,  0.0000000e+00, -7.3511532e-04, ...,\n",
      "         0.0000000e+00,  8.7548935e-05, -4.5106048e-04],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
      "array([ 0.0000000e+00,  0.0000000e+00,  2.2408185e-03,  0.0000000e+00,\n",
      "        0.0000000e+00,  0.0000000e+00,  7.0566376e-04, -1.4050006e-03,\n",
      "        1.5673270e-03,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "       -5.7823047e-05,  0.0000000e+00,  1.0263523e-03,  1.6413367e-03,\n",
      "        0.0000000e+00, -1.8799718e-03,  0.0000000e+00, -1.3477544e-04,\n",
      "        3.5997962e-03,  1.6791845e-03, -1.3862986e-03, -6.4685679e-05,\n",
      "       -2.8425683e-03,  0.0000000e+00,  2.8422745e-03,  5.1272695e-04,\n",
      "       -1.4480846e-03, -1.0649288e-03,  9.9256216e-04, -2.2286720e-05,\n",
      "        0.0000000e+00,  0.0000000e+00,  2.2110357e-03,  4.6358735e-04,\n",
      "        0.0000000e+00,  0.0000000e+00,  5.9459355e-05,  7.4552826e-04,\n",
      "        0.0000000e+00, -4.7866881e-05,  3.7958787e-04, -2.3203327e-03,\n",
      "       -1.6442883e-03,  2.8541069e-03, -2.0561665e-03,  1.6131783e-03,\n",
      "       -5.6188629e-04,  6.0674036e-04, -5.0314766e-04,  3.0602943e-03,\n",
      "        3.3844905e-03, -6.0533821e-06,  2.1271452e-03, -4.5141000e-03,\n",
      "        0.0000000e+00,  1.2134210e-03,  1.4315482e-03,  0.0000000e+00,\n",
      "        1.4976424e-03,  0.0000000e+00,  1.2971954e-03, -1.0963596e-04,\n",
      "       -2.9486499e-05, -7.0036278e-04, -2.3356841e-04,  0.0000000e+00,\n",
      "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "        1.2105763e-03,  1.6898974e-03,  1.8756698e-03,  1.0986952e-03,\n",
      "        0.0000000e+00,  0.0000000e+00,  1.3236093e-03,  1.4656510e-03,\n",
      "        0.0000000e+00,  8.4364868e-04, -1.1992110e-03,  0.0000000e+00,\n",
      "        1.2930871e-03,  1.2158521e-03,  0.0000000e+00, -3.2585519e-04,\n",
      "        9.1685686e-04,  1.3937225e-03, -1.7200829e-04, -2.3962560e-04,\n",
      "        2.9687933e-03,  1.8881785e-03,  2.8540537e-04,  0.0000000e+00,\n",
      "        0.0000000e+00, -4.9654837e-04, -2.1638961e-03,  1.7154688e-03,\n",
      "       -1.9326861e-04,  0.0000000e+00,  2.5991832e-03,  2.1893878e-03,\n",
      "       -1.0160946e-03,  2.2681493e-03, -8.3190302e-04,  3.2138801e-04,\n",
      "        0.0000000e+00,  1.4613532e-03,  7.7333179e-04,  2.6553389e-04,\n",
      "       -5.6592061e-04,  5.3222320e-04, -3.2222862e-03,  0.0000000e+00,\n",
      "       -3.6836611e-03,  1.0207685e-03, -3.2259810e-03,  2.1784671e-04,\n",
      "        0.0000000e+00,  0.0000000e+00, -1.4474322e-03,  4.4396863e-04,\n",
      "        0.0000000e+00,  0.0000000e+00, -9.2868088e-04,  1.1011390e-03],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 1.3496300e-04, -7.6252320e-05,  0.0000000e+00, ...,\n",
      "         3.6359215e-04,  2.1707070e-04, -8.8022774e-05],\n",
      "       ...,\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [-4.3409377e-06, -1.8222458e-05,  0.0000000e+00, ...,\n",
      "         2.4217050e-05,  1.2766742e-05, -2.1292464e-05],\n",
      "       [ 3.9357703e-05,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.2354043e-05,  3.7832189e-05,  2.5682859e-06]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
      "array([ 0.00605102, -0.00669866,  0.        ,  0.00460401,  0.        ,\n",
      "        0.        ,  0.        ,  0.02856985,  0.00441365,  0.        ,\n",
      "        0.02171404,  0.00897372,  0.00224033, -0.01402466, -0.01926275,\n",
      "       -0.01017113,  0.        ,  0.        ,  0.        , -0.01871499,\n",
      "        0.02171139, -0.01519779,  0.        ,  0.01574417,  0.        ,\n",
      "        0.        , -0.00660977,  0.03123634,  0.0330013 ,  0.00765251,\n",
      "       -0.01251352, -0.00033475, -0.01181523,  0.        , -0.00469803,\n",
      "        0.        ,  0.0125311 ,  0.00092793,  0.        ,  0.02500691,\n",
      "       -0.02691674,  0.02799572,  0.0069072 ,  0.01864638,  0.0449512 ,\n",
      "        0.        ,  0.        , -0.0007588 ,  0.04825513,  0.00242316,\n",
      "       -0.02887921, -0.00899472,  0.        , -0.01298408,  0.01888582,\n",
      "       -0.00248541, -0.00498891,  0.        ,  0.05010533, -0.01046023,\n",
      "        0.01655602,  0.00428224,  0.00032756,  0.00563096, -0.01735048,\n",
      "        0.03907935,  0.01036271,  0.        , -0.01636803,  0.05427622,\n",
      "        0.01779671, -0.01149353,  0.00741219,  0.        ,  0.03821654,\n",
      "        0.02098907, -0.01571997,  0.01602715,  0.        , -0.02379524,\n",
      "        0.        , -0.01978625,  0.0083738 ,  0.02400526,  0.00149753,\n",
      "        0.02299739,  0.        , -0.02596577, -0.02211387,  0.0044561 ,\n",
      "       -0.00021661,  0.00103871,  0.        ,  0.        ,  0.02228062,\n",
      "        0.00301418,  0.        , -0.0054558 ,  0.        ,  0.03142634,\n",
      "        0.        ,  0.        ,  0.0102399 , -0.04910106,  0.01170072,\n",
      "        0.02238803,  0.        ,  0.03307826,  0.02504109,  0.03632051,\n",
      "        0.        ,  0.01580516,  0.00016747,  0.03430689,  0.02065153,\n",
      "        0.        ,  0.        , -0.01610158, -0.00472369, -0.01388787,\n",
      "        0.        ,  0.02076967, -0.04183382,  0.00047035,  0.00468636,\n",
      "        0.02056196,  0.01232008, -0.00721888], dtype=float32)>, <tf.Tensor: shape=(128, 64), dtype=float32, numpy=\n",
      "array([[-2.2817288e-05,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.7461137e-04,  0.0000000e+00,  2.2753161e-04],\n",
      "       [-1.2916582e-06,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.6997468e-07,  0.0000000e+00,  1.2880282e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       ...,\n",
      "       [-3.6221805e-05,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         2.3370740e-04,  0.0000000e+00,  3.6120016e-04],\n",
      "       [-1.7637007e-04,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.5177026e-03,  0.0000000e+00,  1.7587438e-03],\n",
      "       [-2.5130143e-05,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.5426641e-04,  0.0000000e+00,  2.5059516e-04]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
      "array([-0.07174219,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "       -0.21509942,  0.        , -0.04434731, -0.30559155,  0.        ,\n",
      "        0.        ,  0.22206023,  0.6683256 , -0.39888602,  0.        ,\n",
      "        0.31928724,  0.        , -0.22599041, -0.08489609,  0.06461049,\n",
      "        0.00303438,  0.01140092,  0.        ,  0.        ,  0.729696  ,\n",
      "        0.        ,  0.        , -0.32884598, -0.6728866 ,  0.        ,\n",
      "        0.7882019 ,  0.        , -0.53119063,  0.        , -0.18804446,\n",
      "        0.        ,  0.29792318,  0.05887087,  0.2643807 ,  0.06593417,\n",
      "       -0.3697418 ,  0.        ,  0.        , -0.39981005, -0.37596628,\n",
      "        0.        ,  0.8473907 ,  0.        ,  0.        , -0.4295181 ,\n",
      "        0.31562322,  0.        ,  0.        ,  0.        ,  0.75471956,\n",
      "        0.        ,  0.        , -0.73355144,  0.        , -0.2671443 ,\n",
      "       -0.45244962,  0.6262891 ,  0.        ,  0.7154058 ], dtype=float32)>, <tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n",
      "array([[6.4930000e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.4700883e-04],\n",
      "       [0.0000000e+00],\n",
      "       [9.5376836e-06],\n",
      "       [6.6607448e-05],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.0615052e-04],\n",
      "       [9.2379858e-05],\n",
      "       [3.0029654e-05],\n",
      "       [0.0000000e+00],\n",
      "       [1.2751555e-04],\n",
      "       [0.0000000e+00],\n",
      "       [3.8550446e-05],\n",
      "       [1.3895311e-05],\n",
      "       [4.7659595e-04],\n",
      "       [9.5487907e-05],\n",
      "       [1.7501494e-06],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [2.9326946e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [5.8390451e-05],\n",
      "       [5.6581397e-04],\n",
      "       [0.0000000e+00],\n",
      "       [1.1458590e-03],\n",
      "       [0.0000000e+00],\n",
      "       [4.1202900e-05],\n",
      "       [0.0000000e+00],\n",
      "       [2.1275293e-04],\n",
      "       [0.0000000e+00],\n",
      "       [4.0750332e-05],\n",
      "       [3.3407046e-06],\n",
      "       [4.2110658e-04],\n",
      "       [1.6436056e-06],\n",
      "       [4.0797345e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [2.4077334e-04],\n",
      "       [6.3766674e-05],\n",
      "       [0.0000000e+00],\n",
      "       [4.0647824e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [3.8158658e-05],\n",
      "       [5.0134317e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [6.5671839e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [3.1996574e-04],\n",
      "       [0.0000000e+00],\n",
      "       [4.4763397e-05],\n",
      "       [3.8881516e-04],\n",
      "       [2.2467035e-04],\n",
      "       [0.0000000e+00],\n",
      "       [5.1912089e-04]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.2491145], dtype=float32)>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/2511223439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mppo_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/4223816070.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/4223816070.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, episodes, optimizer, clip_param, c_1, c_2)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;31m# Compute train losses and action by chosen by policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             actor_loss, critic_loss = self.train_step(\n\u001b[0m\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# States\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mepisode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/4223816070.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[1;32m     76\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0']."
     ]
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent(env_name='LunarLander-v2', render=False,epochs=1)\n",
    "ppo_agent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
