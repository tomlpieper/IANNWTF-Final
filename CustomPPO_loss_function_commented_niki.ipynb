{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        # Outputs from the actor network, an action is sampled from (Probabilities)\n",
    "        self.logits = []\n",
    "        # Outputs from the crtitics network (Values)\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) \n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collected values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards)])\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, actionspace, struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionspace = actionspace\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(self.actionspace, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "    #@tf.function\n",
    "\n",
    "    ## logits nur an stelle action zurück\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "        #logits = self.call(observation)\n",
    "        logits = self(observation)\n",
    "        # tf.print(type(logits))\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "        # tf.print(action)\n",
    "        return logits, action\n",
    "\n",
    "\n",
    "    def return_prob()\n",
    "    # prob of action, -> states und action, returns prob of this action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "  \n",
    "    '''\n",
    "\n",
    "    def __init__(self, env_name, render=False, steps_per_epoch=1000, epochs=100, actor_structure=[256,128,64], critic_structure=[256,128,64]):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        \n",
    "        Args:\n",
    "        env_name(): String Name of the Environment Passed\n",
    "        render(): Boolean determining if env should be rendered during training\n",
    "        steps_per_epoch(): how many steps/frame the agent should take during each Epoch of training; Default=1000\n",
    "        epochs(): How many epochs of training should the agent do; Default=100\n",
    "        actor_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        critic_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        '''\n",
    "        # create environemt\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_dimensions = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # set Hyperparameters\n",
    "        self.lr = 3e-4\n",
    "        self.clip_ratio = 0.2\n",
    "        self.c_1 = 0.5\n",
    "        self.optimizer = Adam()\n",
    "        self.render = render\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.actor_struct = actor_structure\n",
    "        self.critic_struct = critic_structure\n",
    "        \n",
    "        # create models and temporary storage\n",
    "        self.actor = Actor(self.num_actions,self.actor_struct)\n",
    "        self.critic = Critic(self.critic_struct)\n",
    "        self.storage = Storage()\n",
    "\n",
    "\n",
    "    def collect_train_data(self, epoch):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        \n",
    "\n",
    "        # Initialize values for return, length and episodes\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "\n",
    "        # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "        #  allows takes on action in a state and saves the information in storage object\n",
    "        for t in tqdm_notebook(range(self.steps_per_epoch), desc = 'Epoch:' + str(epoch)):\n",
    "\n",
    "            # Toggles displaying of environment\n",
    "            if self.render or epoch == self.epochs-1 and self.epochs != 1:\n",
    "                self.env.render()\n",
    "\n",
    "            # Reshaping observation to fit as input for Actor network (policy)\n",
    "            observation = observation.reshape(1,-1)\n",
    "            \n",
    "            # Obtain action and logits for this observation by our actor\n",
    "            logits, action = self.actor.sample_action(observation)\n",
    "            \n",
    "            # Take action in environment and obtain the rewards for it\n",
    "            # Variable done represents wether agent has finished \n",
    "            # The last variable would be diagnostic information, not needed for training\n",
    "            observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "\n",
    "            # Sum up rewards over this episode and count amount of frames\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the Base-Estimate from the Critics network\n",
    "            base_estimate = self.critic(observation)\n",
    "\n",
    "            # Store Variables collected in this timestep t\n",
    "            self.storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "            # Save the new state of our agent\n",
    "            observation = observation_new\n",
    "            \n",
    "            # Check if terminal state is reached in environment\n",
    "            if done:\n",
    "                # Save information about episode\n",
    "                self.storage.conclude_episode()\n",
    "                # Refresh environment and reset return and length value\n",
    "                observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "\n",
    "        # obtain all episodes saved in storage\n",
    "        # episodes, amount_episodes = self.storage.get_episodes()\n",
    "\n",
    "\n",
    "    def actor_loss_fun(self, actions, logits_old, logits_new, rewards, b_estimates_new, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "        ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates_new)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        \n",
    "        #l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "\n",
    "        l2 = tf.clip_by_value(ratio, clip_value_min=1-clip_param, clip_value_max=1+clip_param) * advantage\n",
    "        #l1 = np.array(l1, dtype=\"float32\")\n",
    "        #l2 = np.array(l2, dtype=\"float32\")\n",
    "        \n",
    "\n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        #actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "        #l1 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l1]), dtype=tf.float32)\n",
    "        #l2 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l2]), dtype=tf.float32)\n",
    "        return l1, l2\n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # print(self.actor.trainable_variables())\n",
    "            # Obtain action and logits for this state selected by policy\n",
    "            #print(f' Observation shape/type {observation}')\n",
    "            #print(f'Trainables: {self.actor.layers[0].weights}')\n",
    "\n",
    "\n",
    "            # logits_new, actions_new = sample_action(states)\n",
    "            logits_new = []\n",
    "            b_estimates_new = []\n",
    "\n",
    "            # Compute values with updated critic network\n",
    "            # b_estimates_new = critic(states)\n",
    "\n",
    "            # till we work with np arrays we need to sample each action for this by looping through it\n",
    "            for i in states:\n",
    "                logits, _ = self.actor.sample_action(i)\n",
    "                logits_new.append(logits)\n",
    "                b_estimate = self.critic(i)\n",
    "                \n",
    "                b_estimates_new.append(b_estimate)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen für den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()\n",
    "            #print('Weewoo')\n",
    "            #print(tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print('type critic')\n",
    "            print(type((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "\n",
    "            critic_loss = tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "            #critic_loss = tf.cast(critic_loss, dtype=tf.float32)\n",
    "            #print(f'Critics loss:{type(critic_loss)}. Actor Loss {actor_loss.dtype}')\n",
    "\n",
    "            #print('Actor weights')\n",
    "            #print(print(self.actor.layers[0].weights))\n",
    "\n",
    "            #print('actor')\n",
    "            #print(actor_loss)\n",
    "            #print(type(actor_loss))\n",
    "            #print('critic')\n",
    "            #print(critic_loss)\n",
    "            #print(type(critic_loss))\n",
    "\n",
    "            actor_loss = tf.convert_to_tensor(actor_loss, dtype=tf.float32)\n",
    "\n",
    "            print(actor_loss)\n",
    "            print(critic_loss)\n",
    "            print('actor')\n",
    "            print(self.actor.trainable_variables)\n",
    "            print('critic')\n",
    "            print(self.critic.trainable_variables)\n",
    "            a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            c_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            print(a_gradients)\n",
    "            print(c_gradients)\n",
    "\n",
    "            #print(tape)\n",
    "            #print('Actor loss')\n",
    "            #print(actor_loss)\n",
    "            #print('Trainable Weights')\n",
    "            #print(self.actor.trainable_weights)\n",
    "        \n",
    "        #print(f'Gradients Actor: {a_gradients}. Gradients Critic: {c_gradients}')\n",
    "\n",
    "        # Update parameters\n",
    "        #optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        # del tape\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def update_policy(self, episodes, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                        [look at single one]\n",
    "        '''\n",
    "        # for epoch in training_iteratins:\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in tqdm_notebook(episodes):\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                # States\n",
    "                episode[0],\n",
    "                # Actions\n",
    "                episode[1],\n",
    "                #optimizer (Adam)\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2],\n",
    "                # Rewards\n",
    "                episode[3],\n",
    "                clip_param,\n",
    "                c_1,\n",
    "                c_2 \n",
    "            )\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "            return train_losses_actor, train_losses_critic\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # # Estimated Value of the current situtation from the critics network\n",
    "        # b_estimates = self.episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "    ## get ratio lutsch noch ARSCH, das Ding verarscht mich anders\n",
    "\n",
    "    ## gather function /gather_nd\n",
    "\n",
    "    def get_ratio_episode(self, actions, logits_old, logits_new): \n",
    "        r = []\n",
    "        for a, o, n in zip(actions, logits_old, logits_new):\n",
    "            o = tf.convert_to_tensor(o)\n",
    "            n = tf.convert_to_tensor(n)\n",
    "            #print(f'A: {a} O: {type(o)} N: {type(n)}')\n",
    "\n",
    "            #get the Logarithmic version of all logits for computational efficiency\n",
    "            log_prob_old = tf.nn.log_softmax(o)\n",
    "            log_prob_new = tf.nn.log_softmax(n)\n",
    "            \n",
    "            #  \n",
    "            # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "            logprobability_old = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_old, axis=1\n",
    "            )\n",
    "            logprobability_new = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_new, axis=1\n",
    "            )\n",
    "            # get the ratio of new over old prob\n",
    "            ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "            r.append(ratio)\n",
    "        return r\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for epoch in tqdm_notebook(range(self.epochs), desc = 'Epochs'):\n",
    "            self.collect_train_data((epoch))\n",
    "            data, _ = self.storage.get_episodes()\n",
    "            #print(data)\n",
    "            self.update_policy(data, self.optimizer, self.clip_ratio)\n",
    "        self.env.close()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-08 12:35:06.000097: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-08 12:35:06.000195: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39c55087fd5483da1ce782b5a2bf264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ff4d9913904b60876ad0284c6ac3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69af2481ba049d884e11206477622e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type critic\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[[ 1.9033396   0.6813695   3.5445788  ...  3.7188594   4.7414722\n",
      "   18.348448  ]]\n",
      "\n",
      " [[ 1.9033381   0.68136847  3.5445764  ...  3.718857    4.741469\n",
      "   18.34844   ]]\n",
      "\n",
      " [[ 1.9033431   0.6813714   3.5445836  ...  3.7188637   4.741476\n",
      "   18.34846   ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.9034073   0.6814099   3.5446708  ...  3.7189536   4.741578\n",
      "   18.34866   ]]\n",
      "\n",
      " [[ 1.9034429   0.68143106  3.5447195  ...  3.7190034   4.7416344\n",
      "   18.348768  ]]\n",
      "\n",
      " [[ 1.903469    0.6814466   3.5447552  ...  3.7190397   4.741676\n",
      "   18.348846  ]]], shape=(105, 1, 105), dtype=float32)\n",
      "tf.Tensor(103.683784, shape=(), dtype=float32)\n",
      "tf.Tensor(4.8978133, shape=(), dtype=float32)\n",
      "actor\n",
      "[<tf.Variable 'actor/dense/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[-0.00967695,  0.00185301,  0.00613167, ...,  0.01220629,\n",
      "        -0.00919568, -0.01075138],\n",
      "       [-0.00137531, -0.00837876, -0.00084755, ..., -0.01491074,\n",
      "         0.00147701,  0.00152492],\n",
      "       [ 0.00027184,  0.00635757,  0.00743306, ...,  0.0161585 ,\n",
      "        -0.01564773, -0.00862947],\n",
      "       ...,\n",
      "       [ 0.00139985, -0.01715652,  0.00108361, ...,  0.00285143,\n",
      "        -0.00348213,  0.01595145],\n",
      "       [ 0.00348064,  0.00861572, -0.00126302, ...,  0.00742477,\n",
      "         0.0145309 , -0.00017141],\n",
      "       [ 0.00483056,  0.00018159, -0.00120076, ..., -0.00555451,\n",
      "         0.01302919,  0.00327612]], dtype=float32)>, <tf.Variable 'actor/dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Variable 'actor/dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[-0.00023292,  0.01099315, -0.0019478 , ...,  0.00862446,\n",
      "        -0.0086223 ,  0.00126441],\n",
      "       [ 0.0139122 ,  0.00172158,  0.00116266, ...,  0.00344968,\n",
      "        -0.00239933,  0.01634763],\n",
      "       [ 0.0044331 ,  0.01436125, -0.00128357, ..., -0.02072755,\n",
      "         0.00589769, -0.00897394],\n",
      "       ...,\n",
      "       [-0.00632832,  0.01308886, -0.01326739, ...,  0.00524842,\n",
      "         0.00997104, -0.00431227],\n",
      "       [ 0.01419443, -0.00371235,  0.00123865, ..., -0.00181917,\n",
      "         0.00049144,  0.01350797],\n",
      "       [ 0.00142605,  0.0034694 , -0.00627253, ...,  0.0018999 ,\n",
      "         0.00885088,  0.00599304]], dtype=float32)>, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[-0.00918604,  0.00724981,  0.005373  , ..., -0.00941955,\n",
      "        -0.00235127, -0.00276949],\n",
      "       [ 0.00902192, -0.009705  , -0.00236268, ..., -0.01690155,\n",
      "        -0.00284953,  0.01215296],\n",
      "       [ 0.00767479, -0.00656839, -0.01503257, ..., -0.00963976,\n",
      "        -0.00320069, -0.00710616],\n",
      "       ...,\n",
      "       [-0.01524026,  0.00149814,  0.00678294, ..., -0.00248458,\n",
      "        -0.00401056,  0.0096963 ],\n",
      "       [ 0.00794898, -0.00830978,  0.00719765, ...,  0.00048114,\n",
      "         0.0080303 , -0.00856735],\n",
      "       [-0.01262985,  0.01773797, -0.00902007, ...,  0.00176028,\n",
      "        -0.00529999, -0.00333129]], dtype=float32)>, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\n",
      "array([[ 1.72452241e-01,  4.49679196e-02,  5.89698553e-02,\n",
      "        -2.69475281e-01],\n",
      "       [-7.18668103e-03,  7.11638629e-02, -6.32338375e-02,\n",
      "        -2.46881783e-01],\n",
      "       [-1.71266258e-01, -1.97944626e-01, -1.30808145e-01,\n",
      "         2.51316845e-01],\n",
      "       [ 1.21423393e-01, -2.78563559e-01,  1.71189129e-01,\n",
      "         9.23617482e-02],\n",
      "       [-9.54319835e-02,  1.74684644e-01,  1.44526780e-01,\n",
      "         4.58085835e-02],\n",
      "       [ 1.73666298e-01, -1.33436307e-01, -1.27119794e-01,\n",
      "         2.90360153e-01],\n",
      "       [-1.66590169e-01,  5.01673222e-02,  1.83072090e-02,\n",
      "         7.95781016e-02],\n",
      "       [ 2.78024971e-01, -1.48733959e-01,  1.33479297e-01,\n",
      "         2.11269319e-01],\n",
      "       [-1.41490117e-01,  1.17293745e-01, -6.26972318e-02,\n",
      "         1.67275846e-01],\n",
      "       [ 3.21923494e-02, -2.57887274e-01,  1.16133630e-01,\n",
      "        -6.83218241e-02],\n",
      "       [-1.50037706e-02,  2.86989927e-01, -1.17923021e-02,\n",
      "        -1.29387960e-01],\n",
      "       [ 1.30970448e-01,  2.73653567e-01,  1.47365272e-01,\n",
      "        -4.58910167e-02],\n",
      "       [ 1.03088200e-01,  1.66040808e-01,  1.39512300e-01,\n",
      "        -1.89878643e-02],\n",
      "       [ 2.22197771e-02,  6.37309253e-02, -2.84366995e-01,\n",
      "        -2.47833252e-01],\n",
      "       [ 2.59243011e-01, -1.99237674e-01, -6.53780848e-02,\n",
      "         4.26516533e-02],\n",
      "       [-2.61504650e-02, -3.65283489e-02, -2.88617074e-01,\n",
      "        -8.84827971e-03],\n",
      "       [-2.62181491e-01, -2.96640515e-01, -9.43482816e-02,\n",
      "         1.22391433e-01],\n",
      "       [-2.54697859e-01, -2.73887962e-01,  6.42698705e-02,\n",
      "        -1.90750420e-01],\n",
      "       [ 2.25189269e-01, -1.77327812e-01,  2.73019433e-01,\n",
      "        -7.21277297e-02],\n",
      "       [-7.99575448e-03, -2.77333409e-01, -4.45073843e-02,\n",
      "         5.16954064e-02],\n",
      "       [-1.82370275e-01,  9.77937877e-02,  2.61760175e-01,\n",
      "         1.37812734e-01],\n",
      "       [ 6.87905252e-02, -3.44826877e-02, -2.77298987e-02,\n",
      "         9.55528021e-02],\n",
      "       [-1.76880091e-01,  1.27764970e-01, -3.92883718e-02,\n",
      "         4.65219617e-02],\n",
      "       [-1.23959199e-01, -2.36783355e-01, -9.01142508e-02,\n",
      "         2.76194453e-01],\n",
      "       [-1.96789116e-01, -1.51438266e-01, -1.13751501e-01,\n",
      "         2.70449817e-01],\n",
      "       [ 1.32686228e-01, -2.23032147e-01, -1.37863666e-01,\n",
      "        -2.69879013e-01],\n",
      "       [-2.84999907e-01,  2.96308517e-01, -1.98188886e-01,\n",
      "        -2.60379672e-01],\n",
      "       [-1.43897966e-01, -8.48827809e-02, -1.31160259e-01,\n",
      "         2.51764655e-02],\n",
      "       [ 1.11702919e-01,  1.29904240e-01, -1.70336530e-01,\n",
      "        -5.64696640e-02],\n",
      "       [ 2.78306425e-01, -1.47099838e-01,  2.43684590e-01,\n",
      "         1.23057842e-01],\n",
      "       [-1.02452338e-02,  1.15441412e-01, -2.51498729e-01,\n",
      "         2.81471670e-01],\n",
      "       [-5.12409210e-03,  2.27153659e-01,  7.09730089e-02,\n",
      "        -6.84917867e-02],\n",
      "       [ 1.82604104e-01, -1.77087247e-01, -1.54854938e-01,\n",
      "        -1.08014792e-01],\n",
      "       [-2.19965875e-01, -1.89695984e-01,  7.83673227e-02,\n",
      "        -2.33239487e-01],\n",
      "       [ 8.37049484e-02, -1.31447151e-01, -2.84442484e-01,\n",
      "         2.66598105e-01],\n",
      "       [ 8.29050541e-02, -2.53881156e-01,  1.71559900e-01,\n",
      "        -2.05039978e-02],\n",
      "       [ 2.47666240e-03,  9.86852050e-02,  1.73943996e-01,\n",
      "        -1.38811246e-01],\n",
      "       [ 2.80219018e-01,  2.19784558e-01,  1.57480478e-01,\n",
      "         1.72118604e-01],\n",
      "       [-1.05123818e-01, -1.76352769e-01, -2.41009876e-01,\n",
      "         1.87492371e-03],\n",
      "       [ 1.91980720e-01, -2.11067170e-01,  2.52691805e-02,\n",
      "         7.57288933e-02],\n",
      "       [ 1.91052914e-01,  2.94723392e-01, -1.00010052e-01,\n",
      "         6.71970546e-02],\n",
      "       [-2.63043255e-01, -9.95786041e-02,  1.83437467e-02,\n",
      "        -2.27792725e-01],\n",
      "       [ 4.40610647e-02, -2.80066401e-01, -1.26420438e-01,\n",
      "        -9.71019268e-03],\n",
      "       [-1.55306205e-01, -1.80652931e-01, -1.66036844e-01,\n",
      "        -9.15421396e-02],\n",
      "       [ 1.33548379e-02,  1.71278864e-01,  1.25795782e-01,\n",
      "        -1.96931109e-01],\n",
      "       [-2.62118816e-01,  2.95319915e-01,  2.22064614e-01,\n",
      "        -2.39112094e-01],\n",
      "       [ 7.87355304e-02, -1.25797421e-01, -2.58348823e-01,\n",
      "        -1.36388749e-01],\n",
      "       [ 6.87256455e-02, -2.07609206e-01, -2.83729017e-01,\n",
      "        -2.99034119e-02],\n",
      "       [ 2.28962123e-01, -1.92539573e-01,  2.16926157e-01,\n",
      "        -2.27414459e-01],\n",
      "       [-2.79580563e-01,  2.29894042e-01,  8.18771422e-02,\n",
      "        -4.60458994e-02],\n",
      "       [-2.31558338e-01,  1.10920966e-02, -2.15393618e-01,\n",
      "         2.91942060e-01],\n",
      "       [-2.10953146e-01,  2.05324531e-01,  1.52951479e-03,\n",
      "        -2.59674311e-01],\n",
      "       [-5.67423999e-02,  2.34021902e-01,  1.05162561e-01,\n",
      "         8.30590725e-05],\n",
      "       [ 2.42824256e-01, -2.29259774e-01, -2.92403609e-01,\n",
      "         2.71042049e-01],\n",
      "       [-1.99518129e-01, -1.69790924e-01,  2.78286576e-01,\n",
      "         2.26985872e-01],\n",
      "       [ 4.10077572e-02, -2.65594274e-01, -1.97302490e-01,\n",
      "         1.65607333e-01],\n",
      "       [-1.16987526e-02, -2.91735888e-01,  2.11171865e-01,\n",
      "        -2.76837587e-01],\n",
      "       [-8.93926620e-02,  1.72299445e-02, -3.34620178e-02,\n",
      "         1.63965970e-01],\n",
      "       [-2.32790619e-01, -2.01857835e-01,  5.42218685e-02,\n",
      "        -2.06727684e-02],\n",
      "       [ 7.29393959e-02,  1.75270200e-01,  1.08606935e-01,\n",
      "         4.86035049e-02],\n",
      "       [ 2.94836283e-01, -4.55278456e-02, -2.74453759e-01,\n",
      "        -2.15772569e-01],\n",
      "       [ 2.71500349e-01, -1.09007493e-01,  2.25496948e-01,\n",
      "        -2.75396109e-02],\n",
      "       [-1.32031918e-01,  1.94169730e-01,  1.00086331e-02,\n",
      "        -1.98605597e-01],\n",
      "       [-1.31137386e-01, -1.09075263e-01, -1.09734401e-01,\n",
      "         7.86126554e-02]], dtype=float32)>, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "critic\n",
      "[<tf.Variable 'critic/dense_4/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[-0.00034328, -0.01593541,  0.00040663, ..., -0.00216859,\n",
      "         0.00111457,  0.00264102],\n",
      "       [-0.00520746, -0.00210193,  0.01076003, ...,  0.00154182,\n",
      "        -0.00769689,  0.02218007],\n",
      "       [-0.00714295, -0.01092701, -0.00457125, ...,  0.00438716,\n",
      "         0.00453609, -0.00770993],\n",
      "       ...,\n",
      "       [-0.00281693,  0.0011152 ,  0.00378724, ..., -0.01410227,\n",
      "         0.01845278, -0.00986574],\n",
      "       [-0.00354043, -0.00141311,  0.00530712, ...,  0.00831735,\n",
      "         0.00274292, -0.00319113],\n",
      "       [ 0.01020329,  0.0002724 , -0.0050024 , ...,  0.00483525,\n",
      "        -0.00812803,  0.0029249 ]], dtype=float32)>, <tf.Variable 'critic/dense_4/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Variable 'critic/dense_5/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[ 0.00758555,  0.02795609,  0.0162355 , ...,  0.01497811,\n",
      "         0.02057528, -0.01528008],\n",
      "       [ 0.01254435,  0.0012156 , -0.01220425, ..., -0.0033058 ,\n",
      "         0.00280348,  0.00497759],\n",
      "       [-0.00939803, -0.00224499,  0.00134593, ...,  0.02857227,\n",
      "        -0.010673  ,  0.00300759],\n",
      "       ...,\n",
      "       [ 0.00099175, -0.01358631, -0.0014785 , ..., -0.00399404,\n",
      "        -0.00072604,  0.01482483],\n",
      "       [ 0.00402997, -0.01474423,  0.00278246, ..., -0.00765895,\n",
      "        -0.00188383, -0.00744599],\n",
      "       [ 0.00527018, -0.00377781, -0.0050854 , ...,  0.01313733,\n",
      "         0.00099986,  0.00066603]], dtype=float32)>, <tf.Variable 'critic/dense_5/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_6/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.00410235,  0.00743987,  0.01437147, ..., -0.01282909,\n",
      "         0.00664487, -0.00995208],\n",
      "       [ 0.02122443, -0.00160497,  0.00774568, ...,  0.0137123 ,\n",
      "        -0.01027173, -0.01493635],\n",
      "       [-0.00174786, -0.00687793, -0.00851631, ...,  0.00976902,\n",
      "         0.00801691, -0.00814289],\n",
      "       ...,\n",
      "       [ 0.0080872 , -0.01308655, -0.00797384, ..., -0.00306896,\n",
      "         0.00728434,  0.01246273],\n",
      "       [-0.0081016 , -0.00591494, -0.00799858, ..., -0.00195679,\n",
      "        -0.01087105,  0.01384996],\n",
      "       [ 0.01317838, -0.00829819,  0.00096415, ...,  0.00466966,\n",
      "         0.0026533 ,  0.00780558]], dtype=float32)>, <tf.Variable 'critic/dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_7/kernel:0' shape=(64, 1) dtype=float32, numpy=\n",
      "array([[ 0.17638698],\n",
      "       [ 0.04599395],\n",
      "       [ 0.06031534],\n",
      "       [-0.27562377],\n",
      "       [-0.00735065],\n",
      "       [ 0.07278758],\n",
      "       [-0.06467661],\n",
      "       [-0.25251478],\n",
      "       [-0.17517397],\n",
      "       [-0.20246103],\n",
      "       [-0.13379273],\n",
      "       [ 0.25705105],\n",
      "       [ 0.12419385],\n",
      "       [-0.28491944],\n",
      "       [ 0.17509508],\n",
      "       [ 0.09446913],\n",
      "       [-0.0976094 ],\n",
      "       [ 0.17867035],\n",
      "       [ 0.14782438],\n",
      "       [ 0.04685375],\n",
      "       [ 0.17762879],\n",
      "       [-0.13648085],\n",
      "       [-0.13002022],\n",
      "       [ 0.29698515],\n",
      "       [-0.17039119],\n",
      "       [ 0.05131197],\n",
      "       [ 0.01872489],\n",
      "       [ 0.08139378],\n",
      "       [ 0.28436857],\n",
      "       [-0.15212756],\n",
      "       [ 0.13652483],\n",
      "       [ 0.21608973],\n",
      "       [-0.14471844],\n",
      "       [ 0.11996999],\n",
      "       [-0.06412776],\n",
      "       [ 0.17109251],\n",
      "       [ 0.03292686],\n",
      "       [-0.26377136],\n",
      "       [ 0.11878338],\n",
      "       [-0.06988068],\n",
      "       [-0.01534608],\n",
      "       [ 0.29353803],\n",
      "       [-0.01206139],\n",
      "       [-0.13234015],\n",
      "       [ 0.13395876],\n",
      "       [ 0.2798974 ],\n",
      "       [ 0.15072766],\n",
      "       [-0.04693809],\n",
      "       [ 0.10544032],\n",
      "       [ 0.16982928],\n",
      "       [ 0.14269549],\n",
      "       [-0.0194211 ],\n",
      "       [ 0.02272674],\n",
      "       [ 0.06518504],\n",
      "       [-0.29085526],\n",
      "       [-0.25348794],\n",
      "       [ 0.26515806],\n",
      "       [-0.2037836 ],\n",
      "       [-0.06686978],\n",
      "       [ 0.04362482],\n",
      "       [-0.02674714],\n",
      "       [-0.0373618 ],\n",
      "       [-0.29520234],\n",
      "       [-0.00905019]], dtype=float32)>, <tf.Variable 'critic/dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "[None, None, None, None, None, None, None, None]\n",
      "[<tf.Tensor: shape=(8, 256), dtype=float32, numpy=\n",
      "array([[-1.4082447e-04, -3.4864753e-04,  1.4430976e-03, ...,\n",
      "         6.3254527e-05,  0.0000000e+00,  6.1906096e-05],\n",
      "       [-3.6914580e-04, -7.0743251e-04, -3.4452318e-03, ...,\n",
      "         1.4091632e-04,  0.0000000e+00,  7.7591144e-04],\n",
      "       [-1.1934408e-04, -2.8408185e-04,  3.1981228e-03, ...,\n",
      "         6.8361347e-05,  0.0000000e+00, -2.4361994e-04],\n",
      "       ...,\n",
      "       [-1.2004534e-04, -1.9468220e-04, -5.1719847e-04, ...,\n",
      "         4.4225693e-05,  0.0000000e+00,  4.9888273e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)>, <tf.Tensor: shape=(256,), dtype=float32, numpy=\n",
      "array([-3.67186949e-05, -5.97681137e-05, -3.42779746e-03, -1.08503409e-04,\n",
      "        7.72753207e-04,  0.00000000e+00, -1.31944404e-03, -4.19473567e-04,\n",
      "       -1.54445006e-04,  5.26679039e-04, -5.40893350e-04,  2.02493451e-04,\n",
      "       -4.45337122e-04,  9.86907864e-04, -2.40015244e-04, -1.41832512e-03,\n",
      "        1.01019256e-03, -7.20461248e-04,  1.41371938e-03, -8.06116499e-04,\n",
      "        0.00000000e+00,  2.84859241e-04, -4.73569744e-05,  0.00000000e+00,\n",
      "        3.17422533e-03, -2.90767872e-04,  5.31498285e-04,  2.80669017e-04,\n",
      "        7.51796993e-04,  0.00000000e+00,  2.73617893e-03,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.45359702e-04,  2.54507735e-03,  9.38331534e-04,\n",
      "       -7.89813348e-04, -1.50089490e-03, -1.72217493e-03,  0.00000000e+00,\n",
      "        2.51718005e-03, -1.30222284e-03,  3.31611984e-04,  0.00000000e+00,\n",
      "       -1.56078150e-03,  0.00000000e+00, -6.54904230e-04, -3.53516359e-03,\n",
      "        0.00000000e+00,  1.22492958e-04, -2.33606523e-04,  6.15643279e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.43226900e-04, -5.65328955e-05,\n",
      "       -1.25083618e-03, -1.69326295e-03, -2.07388628e-04,  1.17484764e-04,\n",
      "       -2.75126018e-04,  1.13081544e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "        2.02453672e-03,  7.02240795e-04,  1.88716548e-03,  9.60591424e-05,\n",
      "        1.18438457e-03,  4.49614105e-04,  1.17816892e-03,  0.00000000e+00,\n",
      "        3.67874256e-03, -5.56472689e-03,  0.00000000e+00,  1.16128485e-05,\n",
      "        1.28271698e-03, -6.22233900e-04,  1.81721116e-03,  3.74107418e-04,\n",
      "        8.32370933e-05, -3.55402553e-05,  0.00000000e+00,  1.44879369e-03,\n",
      "        0.00000000e+00,  0.00000000e+00,  3.53324285e-04,  4.14486167e-05,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.87633413e-03,\n",
      "       -6.62502716e-04,  9.60247300e-04, -6.18190796e-04, -1.92340522e-03,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.84135705e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.70051877e-03, -9.20646999e-05,\n",
      "       -1.16297269e-05,  0.00000000e+00,  0.00000000e+00, -1.89171653e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -5.07113466e-04,\n",
      "       -8.33768165e-04, -4.51091444e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.02572376e-03,  0.00000000e+00, -1.20628654e-04,  3.15581652e-04,\n",
      "        0.00000000e+00, -2.15785200e-04, -6.78994867e-04,  3.47363809e-03,\n",
      "       -8.28512275e-05, -2.52061145e-04,  7.51058978e-05, -4.12764028e-03,\n",
      "        1.65383608e-04,  2.79888955e-05,  9.25823231e-04, -8.49540345e-04,\n",
      "        1.54547248e-04,  1.45912787e-03, -2.44371640e-03, -2.11184518e-03,\n",
      "        4.85856639e-04, -2.34837225e-03,  1.29990128e-03,  0.00000000e+00,\n",
      "        1.09102868e-03,  2.44553550e-03, -2.80737033e-04,  9.14597244e-04,\n",
      "       -1.09972607e-03,  5.62268469e-05,  3.12207558e-04,  0.00000000e+00,\n",
      "        7.03158323e-04,  2.66971090e-03,  0.00000000e+00, -1.18807133e-04,\n",
      "        0.00000000e+00,  1.35554816e-03,  0.00000000e+00, -4.82091634e-03,\n",
      "       -1.25597999e-03, -1.05959293e-03, -8.08175944e-04, -1.97101780e-03,\n",
      "        1.76609913e-03, -3.85098765e-03,  9.03772772e-04,  2.57213367e-04,\n",
      "        4.78184229e-04, -7.68156897e-05,  9.93880803e-06, -2.18456076e-03,\n",
      "        4.78445436e-04,  1.42052979e-03,  3.67346729e-05, -1.81785726e-03,\n",
      "        0.00000000e+00, -1.15825247e-03,  0.00000000e+00, -2.31005652e-05,\n",
      "       -4.72148622e-06, -1.30811019e-03,  7.91968887e-06, -1.80087960e-03,\n",
      "        2.10576202e-03, -9.69953195e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "        8.91454169e-04, -4.08372056e-04,  0.00000000e+00,  1.50861350e-04,\n",
      "        0.00000000e+00, -2.51874328e-03,  2.28472101e-03,  8.93862627e-04,\n",
      "        9.72040434e-05, -8.11747159e-04,  0.00000000e+00, -1.39317242e-03,\n",
      "        1.44990720e-03,  2.13492924e-04,  9.94897564e-04,  4.41351003e-04,\n",
      "        0.00000000e+00,  2.04043565e-04,  3.01312562e-03, -5.74724225e-04,\n",
      "       -1.17410114e-03,  1.69061555e-03,  8.78823339e-04, -8.96281854e-04,\n",
      "        0.00000000e+00, -4.87338111e-04,  0.00000000e+00, -3.81996389e-04,\n",
      "       -9.85123683e-04,  1.91489176e-03, -1.03957730e-03,  0.00000000e+00,\n",
      "        1.75744854e-03,  0.00000000e+00,  0.00000000e+00,  6.74855371e-04,\n",
      "        2.88090232e-04,  2.88680731e-03,  2.43305438e-03,  1.04092225e-03,\n",
      "        7.05471844e-04, -2.14088868e-04, -4.39831376e-04,  2.66512670e-03,\n",
      "        9.64334235e-04, -6.70948299e-04, -1.06312172e-03,  3.61618877e-04,\n",
      "        0.00000000e+00, -1.22180465e-03,  1.18019263e-04,  7.59297225e-04,\n",
      "       -2.33341171e-03,  5.26958320e-04,  2.85772909e-03,  0.00000000e+00,\n",
      "        1.35935459e-03, -3.88222956e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.09654607e-03, -3.57599492e-05,  0.00000000e+00,  1.20319426e-03,\n",
      "       -2.22613639e-03,  0.00000000e+00,  6.89177105e-05,  2.35321745e-03,\n",
      "        8.53637684e-05, -4.60359843e-06,  0.00000000e+00,  4.82768955e-04],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(256, 128), dtype=float32, numpy=\n",
      "array([[-1.83091339e-04,  3.90461471e-04,  0.00000000e+00, ...,\n",
      "        -1.30375061e-04, -6.37232661e-05,  0.00000000e+00],\n",
      "       [-1.79361305e-04,  2.95386038e-04,  0.00000000e+00, ...,\n",
      "        -1.97867426e-04, -1.04128361e-04,  2.01901294e-05],\n",
      "       [-2.22172457e-04,  1.90533246e-04,  0.00000000e+00, ...,\n",
      "        -3.68006120e-04, -1.92360661e-04,  5.84367590e-05],\n",
      "       ...,\n",
      "       [-1.31042170e-05,  5.27543089e-05,  0.00000000e+00, ...,\n",
      "         0.00000000e+00, -6.11308997e-06,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [-1.91982705e-04,  1.33203677e-04,  0.00000000e+00, ...,\n",
      "        -3.56901204e-04, -2.73971149e-04,  1.04340186e-04]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
      "array([-1.3595986e-02,  1.7260144e-02,  0.0000000e+00, -1.3662145e-03,\n",
      "       -2.1372713e-02,  6.8077100e-03,  0.0000000e+00,  7.1426341e-03,\n",
      "       -2.8468368e-03,  0.0000000e+00,  1.5581850e-02,  1.3428556e-02,\n",
      "        1.0175166e-02, -3.5715625e-03,  2.9143507e-02, -3.4965124e-02,\n",
      "        8.9885145e-03,  2.0498182e-03,  7.0956717e-03,  0.0000000e+00,\n",
      "        0.0000000e+00, -4.4965167e-03,  0.0000000e+00,  2.3259684e-02,\n",
      "        0.0000000e+00, -1.0119498e-02,  7.3052774e-04,  2.4046265e-03,\n",
      "        4.1140884e-02,  2.4438957e-02, -2.5809318e-02,  3.4261115e-02,\n",
      "       -1.9960878e-02, -1.0335697e-02,  0.0000000e+00, -3.7458427e-02,\n",
      "        2.3730427e-03,  6.4437338e-03, -1.2945935e-02,  0.0000000e+00,\n",
      "       -5.2399386e-04,  4.3807877e-03,  1.4252042e-02, -2.8618211e-02,\n",
      "        8.3202918e-05,  8.5656677e-04, -1.9019144e-03,  2.4144508e-02,\n",
      "       -4.7840420e-03,  2.7226081e-02, -8.1225298e-03,  8.8010998e-03,\n",
      "        3.5568245e-03,  0.0000000e+00, -1.7213437e-04, -2.3688735e-02,\n",
      "       -3.5119910e-02,  0.0000000e+00,  1.8160326e-02,  2.1981105e-02,\n",
      "        3.5222013e-02, -3.3997960e-02,  1.2572086e-02,  2.7282322e-02,\n",
      "       -1.9092210e-02, -3.9729998e-03,  0.0000000e+00,  0.0000000e+00,\n",
      "       -3.1232562e-02, -1.1948993e-02,  2.9624993e-02, -2.6391561e-03,\n",
      "       -4.5347209e-03, -4.9116794e-02,  3.2797805e-05,  0.0000000e+00,\n",
      "        4.6216017e-03, -9.2699490e-03, -1.6665978e-02, -1.6871069e-02,\n",
      "        0.0000000e+00,  0.0000000e+00, -4.0419325e-02,  1.4177384e-02,\n",
      "        4.8223506e-03,  1.2741066e-02,  0.0000000e+00,  2.6822142e-02,\n",
      "        2.9304228e-03, -2.1549845e-02,  0.0000000e+00,  0.0000000e+00,\n",
      "        2.4803536e-02,  1.0437543e-02,  0.0000000e+00,  0.0000000e+00,\n",
      "        0.0000000e+00,  0.0000000e+00, -9.8850755e-03,  0.0000000e+00,\n",
      "        0.0000000e+00,  4.2861037e-02, -3.5772782e-03, -9.5359534e-03,\n",
      "        1.0523453e-02,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "        1.6089834e-02, -1.9250615e-02, -8.8500688e-03, -2.0509634e-02,\n",
      "        4.5202584e-03, -7.6564918e-03,  0.0000000e+00, -4.5697531e-03,\n",
      "        1.7063193e-02, -9.3675777e-03,  0.0000000e+00,  5.8244867e-03,\n",
      "       -2.9270733e-02,  0.0000000e+00, -2.1134449e-02,  7.4495021e-03,\n",
      "        0.0000000e+00, -1.8802755e-02, -1.0343509e-02,  2.7306252e-03],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(128, 64), dtype=float32, numpy=\n",
      "array([[ 0.00000000e+00,  2.51502905e-04,  3.32155731e-04, ...,\n",
      "        -1.13938528e-04, -1.62567513e-03, -1.99102124e-05],\n",
      "       [ 0.00000000e+00,  2.90059543e-04,  3.95208481e-04, ...,\n",
      "        -1.60025811e-04, -1.93427503e-03, -1.80320894e-05],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 0.00000000e+00,  2.77971012e-05,  3.71067763e-05, ...,\n",
      "        -2.29854631e-05, -1.81612297e-04,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  1.31308916e-04,  1.76987043e-04, ...,\n",
      "        -9.00226078e-05, -8.66230461e-04, -3.98918382e-06],\n",
      "       [ 0.00000000e+00,  1.28278606e-08,  1.05818640e-06, ...,\n",
      "        -6.55484087e-07, -5.17909939e-06,  0.00000000e+00]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
      "array([ 0.        ,  0.1609024 ,  0.22155398, -1.012438  , -0.01877179,\n",
      "        0.23935671, -0.23757415, -0.21201496, -0.04902759, -0.743692  ,\n",
      "       -0.20126468,  0.        ,  0.2650244 , -0.73758954,  0.6064156 ,\n",
      "        0.01983001, -0.28683355,  0.6563026 ,  0.45508033,  0.12784912,\n",
      "        0.        , -0.5013296 ,  0.        ,  0.45714465,  0.        ,\n",
      "        0.09872883,  0.06878138,  0.        ,  0.24870798, -0.18094884,\n",
      "        0.50149137,  0.79375374, -0.19744772,  0.        , -0.19966213,\n",
      "        0.51474273,  0.0322538 ,  0.        ,  0.        , -0.04156009,\n",
      "       -0.03596946,  0.        , -0.04430462,  0.        ,  0.30460978,\n",
      "        0.        ,  0.26364982, -0.17241588,  0.        ,  0.6238272 ,\n",
      "        0.1497607 ,  0.        ,  0.        ,  0.        , -0.49857965,\n",
      "        0.        ,  0.25046197, -0.32080424, -0.24563013,  0.00305246,\n",
      "       -0.05801391, -0.10848375, -1.084355  , -0.00601573], dtype=float32)>, <tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n",
      "array([[0.00000000e+00],\n",
      "       [4.97103320e-04],\n",
      "       [1.05477928e-03],\n",
      "       [6.30235823e-04],\n",
      "       [1.27619307e-04],\n",
      "       [6.11596508e-04],\n",
      "       [3.46883258e-04],\n",
      "       [3.19439678e-05],\n",
      "       [1.17597774e-05],\n",
      "       [1.18680100e-03],\n",
      "       [6.55338517e-05],\n",
      "       [0.00000000e+00],\n",
      "       [1.14511204e-04],\n",
      "       [1.80075454e-04],\n",
      "       [2.90626806e-04],\n",
      "       [3.08978815e-06],\n",
      "       [2.80091976e-04],\n",
      "       [6.05082430e-04],\n",
      "       [5.18605812e-04],\n",
      "       [3.62775609e-04],\n",
      "       [0.00000000e+00],\n",
      "       [3.76598473e-04],\n",
      "       [0.00000000e+00],\n",
      "       [7.22181576e-05],\n",
      "       [0.00000000e+00],\n",
      "       [1.11164351e-04],\n",
      "       [5.52546757e-04],\n",
      "       [0.00000000e+00],\n",
      "       [3.67672073e-05],\n",
      "       [2.57083047e-05],\n",
      "       [8.08737532e-04],\n",
      "       [2.85204238e-04],\n",
      "       [4.07076041e-05],\n",
      "       [0.00000000e+00],\n",
      "       [1.46291262e-04],\n",
      "       [4.41929820e-04],\n",
      "       [1.32142770e-04],\n",
      "       [0.00000000e+00],\n",
      "       [0.00000000e+00],\n",
      "       [9.65451545e-06],\n",
      "       [3.85493302e-04],\n",
      "       [0.00000000e+00],\n",
      "       [5.61250374e-04],\n",
      "       [0.00000000e+00],\n",
      "       [1.22698097e-04],\n",
      "       [0.00000000e+00],\n",
      "       [1.96447087e-04],\n",
      "       [1.17179425e-03],\n",
      "       [0.00000000e+00],\n",
      "       [8.34809325e-04],\n",
      "       [4.73181353e-05],\n",
      "       [0.00000000e+00],\n",
      "       [0.00000000e+00],\n",
      "       [0.00000000e+00],\n",
      "       [1.00003759e-04],\n",
      "       [0.00000000e+00],\n",
      "       [6.83364851e-05],\n",
      "       [2.76186474e-05],\n",
      "       [7.19113857e-04],\n",
      "       [7.70781071e-07],\n",
      "       [6.82865793e-05],\n",
      "       [2.97232356e-04],\n",
      "       [4.54437832e-04],\n",
      "       [3.53173527e-05]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.6732621], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent(env_name='LunarLander-v2', render=False,epochs=1)\n",
    "ppo_agent.run()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
