{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        # Outputs from the actor network, an action is sampled from (Probabilities)\n",
    "        self.logits = []\n",
    "        # Outputs from the crtitics network (Values)\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) \n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collected values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards)])\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, actionspace, struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionspace = actionspace\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(self.actionspace, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "    #@tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "        #logits = self.call(observation)\n",
    "        logits = self(observation)\n",
    "    # tf.print(type(logits))\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    # tf.print(action)\n",
    "        return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "  \n",
    "    '''\n",
    "\n",
    "    def __init__(self, env_name, render=False, steps_per_epoch=1000, epochs=100, actor_structure=[256,128,64], critic_structure=[256,128,64]):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        \n",
    "        Args:\n",
    "        env_name(): String Name of the Environment Passed\n",
    "        render(): Boolean determining if env should be rendered during training\n",
    "        steps_per_epoch(): how many steps/frame the agent should take during each Epoch of training; Default=1000\n",
    "        epochs(): How many epochs of training should the agent do; Default=100\n",
    "        actor_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        critic_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        '''\n",
    "        # create environemt\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_dimensions = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # set Hyperparameters\n",
    "        self.lr = 3e-4\n",
    "        self.clip_ratio = 0.2\n",
    "        self.c_1 = 0.5\n",
    "        self.optimizer = Adam()\n",
    "        self.render = render\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.actor_struct = actor_structure\n",
    "        self.critic_struct = critic_structure\n",
    "        \n",
    "        # create models and temporary storage\n",
    "        self.actor = Actor(self.num_actions,self.actor_struct)\n",
    "        self.critic = Critic(self.critic_struct)\n",
    "        self.storage = Storage()\n",
    "\n",
    "\n",
    "    def collect_train_data(self):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        for epoch in tqdm_notebook(range(self.epochs), desc = 'Epochs'):\n",
    "\n",
    "            # Initialize values for return, length and episodes\n",
    "            sum_return = 0\n",
    "            sum_length = 0\n",
    "            num_episodes = 0\n",
    "\n",
    "            # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "            #  allows takes on action in a state and saves the information in storage object\n",
    "            for t in tqdm_notebook(range(self.steps_per_epoch), desc = 'Epoch:' + str(epoch)):\n",
    "\n",
    "                # Toggles displaying of environment\n",
    "                if self.render or epoch == self.epochs-1 and self.epochs != 1:\n",
    "                    self.env.render()\n",
    "\n",
    "                # Reshaping observation to fit as input for Actor network (policy)\n",
    "                observation = observation.reshape(1,-1)\n",
    "                \n",
    "                # Obtain action and logits for this observation by our actor\n",
    "                logits, action = self.actor.sample_action(observation)\n",
    "                \n",
    "                # Take action in environment and obtain the rewards for it\n",
    "                # Variable done represents wether agent has finished \n",
    "                # The last variable would be diagnostic information, not needed for training\n",
    "                observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "\n",
    "                # Sum up rewards over this episode and count amount of frames\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "\n",
    "                # Get the Base-Estimate from the Critics network\n",
    "                base_estimate = self.critic(observation)\n",
    "\n",
    "                # Store Variables collected in this timestep t\n",
    "                self.storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "                # Save the new state of our agent\n",
    "                observation = observation_new\n",
    "                \n",
    "                # Check if terminal state is reached in environment\n",
    "                if done:\n",
    "                    # Save information about episode\n",
    "                    self.storage.conclude_episode()\n",
    "                    # Refresh environment and reset return and length value\n",
    "                    observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "\n",
    "            # obtain all episodes saved in storage\n",
    "            # episodes, amount_episodes = self.storage.get_episodes()\n",
    "\n",
    "\n",
    "    def actor_loss_fun(self, actions, logits_old, logits_new, rewards, b_estimates_new, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "        ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates_new)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "        #l1 = np.array(l1, dtype=\"float32\")\n",
    "        #l2 = np.array(l2, dtype=\"float32\")\n",
    "        \n",
    "\n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        #actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "        l1 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l1]), dtype=tf.float32)\n",
    "        l2 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l2]), dtype=tf.float32)\n",
    "        return tf.convert_to_tensor(l1, dtype=tf.float32), tf.convert_to_tensor(l2, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape() as tape, tf.GradientTape() as tape2:\n",
    "            # print(self.actor.trainable_variables())\n",
    "            # Obtain action and logits for this state selected by policy\n",
    "            #print(f' Observation shape/type {observation}')\n",
    "            #print(f'Trainables: {self.actor.layers[0].weights}')\n",
    "\n",
    "\n",
    "            # logits_new, actions_new = sample_action(states)\n",
    "            logits_new = []\n",
    "            b_estimates_new = []\n",
    "\n",
    "            # Compute values with updated critic network\n",
    "            # b_estimates_new = critic(states)\n",
    "\n",
    "            # till we work with np arrays we need to sample each action for this by looping through it\n",
    "            for i in states:\n",
    "                logits, _ = self.actor.sample_action(i)\n",
    "                logits_new.append(logits)\n",
    "                b_estimate = self.critic(i)\n",
    "                \n",
    "                b_estimates_new.append(b_estimate)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen fÃ¼r den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()\n",
    "            #print('Weewoo')\n",
    "            #print(tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print('type critic')\n",
    "            print(type((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "\n",
    "            critic_loss = tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            #print('minimum')\n",
    "            #print(-tf.reduce_mean(tf.minimum(l1, l2)))\n",
    "            #print(type(tf.minimum(l1,l2)))\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "            #critic_loss = tf.cast(critic_loss, dtype=tf.float32)\n",
    "            #print(f'Critics loss:{type(critic_loss)}. Actor Loss {actor_loss.dtype}')\n",
    "\n",
    "            #print('Actor weights')\n",
    "            #print(print(self.actor.layers[0].weights))\n",
    "\n",
    "            #print('actor')\n",
    "            #print(actor_loss)\n",
    "            #print(type(actor_loss))\n",
    "            #print('critic')\n",
    "            #print(critic_loss)\n",
    "            #print(type(critic_loss))\n",
    "\n",
    "            actor_loss = tf.convert_to_tensor(actor_loss, dtype=tf.float32)\n",
    "\n",
    "            print(actor_loss)\n",
    "            print(critic_loss)\n",
    "            print('actor')\n",
    "            print(self.actor.trainable_variables)\n",
    "            print('critic')\n",
    "            print(self.critic.trainable_variables)\n",
    "            a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            c_gradients = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            print(a_gradients)\n",
    "            print(c_gradients)\n",
    "\n",
    "            #print(tape)\n",
    "            #print('Actor loss')\n",
    "            #print(actor_loss)\n",
    "            #print('Trainable Weights')\n",
    "            #print(self.actor.trainable_weights)\n",
    "        \n",
    "        #print(f'Gradients Actor: {a_gradients}. Gradients Critic: {c_gradients}')\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def update_policy(self, episodes, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                        [look at single one]\n",
    "        '''\n",
    "        # for epoch in training_iteratins:\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in tqdm_notebook(episodes):\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                # States\n",
    "                episode[0],\n",
    "                # Actions\n",
    "                episode[1],\n",
    "                #optimizer (Adam)\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2],\n",
    "                # Rewards\n",
    "                episode[3],\n",
    "                clip_param,\n",
    "                c_1,\n",
    "                c_2 \n",
    "            )\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "            return train_losses_actor, train_losses_critic\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # # Estimated Value of the current situtation from the critics network\n",
    "        # b_estimates = self.episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "    ## get ratio lutsch noch ARSCH, das Ding verarscht mich anders\n",
    "\n",
    "    def get_ratio_episode(self, actions, logits_old, logits_new): \n",
    "        r = []\n",
    "        for a, o, n in zip(actions, logits_old, logits_new):\n",
    "            o = tf.convert_to_tensor(o)\n",
    "            n = tf.convert_to_tensor(n)\n",
    "            #print(f'A: {a} O: {type(o)} N: {type(n)}')\n",
    "\n",
    "            #get the Logarithmic version of all logits for computational efficiency\n",
    "            log_prob_old = tf.nn.log_softmax(o)\n",
    "            log_prob_new = tf.nn.log_softmax(n)\n",
    "\n",
    "            # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "            logprobability_old = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_old, axis=1\n",
    "            )\n",
    "            logprobability_new = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_new, axis=1\n",
    "            )\n",
    "            # get the ratio of new over old prob\n",
    "            ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "            r.append(ratio)\n",
    "        return r\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.collect_train_data()\n",
    "        data, _ = self.storage.get_episodes()\n",
    "        #print(data)\n",
    "        self.update_policy(data, self.optimizer, self.clip_ratio)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4565e48c72874837859829551773e3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476618984709464f9137664678ab7181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3a2e79f1034e4f921413555f9780cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type critic\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[[7.56841362e-01 8.10806826e-03 3.20184946e+00 2.28637923e-02\n",
      "   3.89010319e-03 1.71234496e-02 1.35068750e+00 1.99532080e-02\n",
      "   5.33757843e-02 4.89102781e-01 5.82421899e-01 1.50306989e-02\n",
      "   1.99498042e-01 2.75794357e-01 3.08981824e+00 2.39089042e-01\n",
      "   3.70870143e-01 8.38835478e-01 2.01606318e-01 2.82721710e+00\n",
      "   3.48369861e+00 4.34896439e-01 3.50034070e+00 7.43860662e-01\n",
      "   2.10582781e+00 4.45663966e-02 1.45004916e+00 1.80818979e-02\n",
      "   4.69470167e+00 6.01401506e-03 7.65102565e-01]]\n",
      "\n",
      " [[7.56842613e-01 8.10820237e-03 3.20185208e+00 2.28640195e-02\n",
      "   3.89019586e-03 1.71232503e-02 1.35068583e+00 1.99534185e-02\n",
      "   5.33761382e-02 4.89101827e-01 5.82422972e-01 1.50305154e-02\n",
      "   1.99497372e-01 2.75795102e-01 3.08982086e+00 2.39088297e-01\n",
      "   3.70871067e-01 8.38834226e-01 2.01605648e-01 2.82721949e+00\n",
      "   3.48370099e+00 4.34897393e-01 3.50034308e+00 7.43859410e-01\n",
      "   2.10582995e+00 4.45667207e-02 1.45005107e+00 1.80820953e-02\n",
      "   4.69470501e+00 6.01413008e-03 7.65103817e-01]]\n",
      "\n",
      " [[7.56847262e-01 8.10868200e-03 3.20186162e+00 2.28648204e-02\n",
      "   3.89052741e-03 1.71225611e-02 1.35067987e+00 1.99541710e-02\n",
      "   5.33773564e-02 4.89098102e-01 5.82427025e-01 1.50298644e-02\n",
      "   1.99495003e-01 2.75797904e-01 3.08982944e+00 2.39085719e-01\n",
      "   3.70874316e-01 8.38829279e-01 2.01603249e-01 2.82722831e+00\n",
      "   3.48371077e+00 4.34900910e-01 3.50035286e+00 7.43854761e-01\n",
      "   2.10583758e+00 4.45678383e-02 1.45005727e+00 1.80828124e-02\n",
      "   4.69471645e+00 6.01454172e-03 7.65108526e-01]]\n",
      "\n",
      " [[7.56846488e-01 8.10860656e-03 3.20185995e+00 2.28646919e-02\n",
      "   3.89047619e-03 1.71226673e-02 1.35068047e+00 1.99540518e-02\n",
      "   5.33771627e-02 4.89098668e-01 5.82426429e-01 1.50299687e-02\n",
      "   1.99495405e-01 2.75797457e-01 3.08982849e+00 2.39086121e-01\n",
      "   3.70873749e-01 8.38830113e-01 2.01603651e-01 2.82722688e+00\n",
      "   3.48370957e+00 4.34900403e-01 3.50035167e+00 7.43855476e-01\n",
      "   2.10583663e+00 4.45676632e-02 1.45005631e+00 1.80826988e-02\n",
      "   4.69471407e+00 6.01447793e-03 7.65107751e-01]]\n",
      "\n",
      " [[7.56851017e-01 8.10906757e-03 3.20186901e+00 2.28654705e-02\n",
      "   3.89079493e-03 1.71219949e-02 1.35067475e+00 1.99547764e-02\n",
      "   5.33783436e-02 4.89095032e-01 5.82430303e-01 1.50293373e-02\n",
      "   1.99493095e-01 2.75800139e-01 3.08983707e+00 2.39083633e-01\n",
      "   3.70876938e-01 8.38825405e-01 2.01601312e-01 2.82723546e+00\n",
      "   3.48371887e+00 4.34903771e-01 3.50036097e+00 7.43851066e-01\n",
      "   2.10584402e+00 4.45687361e-02 1.45006263e+00 1.80833861e-02\n",
      "   4.69472599e+00 6.01487560e-03 7.65112281e-01]]\n",
      "\n",
      " [[7.56851494e-01 8.10912717e-03 3.20186996e+00 2.28655674e-02\n",
      "   3.89083615e-03 1.71219092e-02 1.35067403e+00 1.99548695e-02\n",
      "   5.33785038e-02 4.89094645e-01 5.82430780e-01 1.50292581e-02\n",
      "   1.99492797e-01 2.75800467e-01 3.08983874e+00 2.39083320e-01\n",
      "   3.70877326e-01 8.38824809e-01 2.01601043e-01 2.82723689e+00\n",
      "   3.48372054e+00 4.34904158e-01 3.50036216e+00 7.43850529e-01\n",
      "   2.10584450e+00 4.45688851e-02 1.45006335e+00 1.80834755e-02\n",
      "   4.69472837e+00 6.01492543e-03 7.65112817e-01]]\n",
      "\n",
      " [[7.56855249e-01 8.10951274e-03 3.20187807e+00 2.28662174e-02\n",
      "   3.89110367e-03 1.71213504e-02 1.35066891e+00 1.99554767e-02\n",
      "   5.33794910e-02 4.89091635e-01 5.82434058e-01 1.50287319e-02\n",
      "   1.99490890e-01 2.75802732e-01 3.08984637e+00 2.39081174e-01\n",
      "   3.70879889e-01 8.38820934e-01 2.01599121e-01 2.82724380e+00\n",
      "   3.48372793e+00 4.34906960e-01 3.50037026e+00 7.43846834e-01\n",
      "   2.10585093e+00 4.45697829e-02 1.45006859e+00 1.80840548e-02\n",
      "   4.69473743e+00 6.01525931e-03 7.65116572e-01]]\n",
      "\n",
      " [[7.56854236e-01 8.10940564e-03 3.20187593e+00 2.28660367e-02\n",
      "   3.89102916e-03 1.71215087e-02 1.35067022e+00 1.99553054e-02\n",
      "   5.33792078e-02 4.89092529e-01 5.82433164e-01 1.50288753e-02\n",
      "   1.99491411e-01 2.75802106e-01 3.08984423e+00 2.39081770e-01\n",
      "   3.70879143e-01 8.38821888e-01 2.01599658e-01 2.82724190e+00\n",
      "   3.48372626e+00 4.34906214e-01 3.50036788e+00 7.43847847e-01\n",
      "   2.10584903e+00 4.45695333e-02 1.45006716e+00 1.80838928e-02\n",
      "   4.69473457e+00 6.01516571e-03 7.65115440e-01]]\n",
      "\n",
      " [[7.56854653e-01 8.10944289e-03 3.20187640e+00 2.28660963e-02\n",
      "   3.89105477e-03 1.71214510e-02 1.35066986e+00 1.99553631e-02\n",
      "   5.33793122e-02 4.89092112e-01 5.82433462e-01 1.50288260e-02\n",
      "   1.99491248e-01 2.75802344e-01 3.08984470e+00 2.39081576e-01\n",
      "   3.70879471e-01 8.38821530e-01 2.01599449e-01 2.82724285e+00\n",
      "   3.48372698e+00 4.34906512e-01 3.50036907e+00 7.43847489e-01\n",
      "   2.10584974e+00 4.45696227e-02 1.45006764e+00 1.80839468e-02\n",
      "   4.69473505e+00 6.01519970e-03 7.65115976e-01]]\n",
      "\n",
      " [[7.56855369e-01 8.10951833e-03 3.20187807e+00 2.28662249e-02\n",
      "   3.89110879e-03 1.71213448e-02 1.35066891e+00 1.99554823e-02\n",
      "   5.33795059e-02 4.89091545e-01 5.82434118e-01 1.50287263e-02\n",
      "   1.99490845e-01 2.75802791e-01 3.08984637e+00 2.39081174e-01\n",
      "   3.70879948e-01 8.38820815e-01 2.01599091e-01 2.82724380e+00\n",
      "   3.48372793e+00 4.34907019e-01 3.50037026e+00 7.43846714e-01\n",
      "   2.10585093e+00 4.45697978e-02 1.45006859e+00 1.80840604e-02\n",
      "   4.69473743e+00 6.01526350e-03 7.65116572e-01]]\n",
      "\n",
      " [[7.56854534e-01 8.10943265e-03 3.20187640e+00 2.28660814e-02\n",
      "   3.89104709e-03 1.71214677e-02 1.35067010e+00 1.99553501e-02\n",
      "   5.33792786e-02 4.89092201e-01 5.82433343e-01 1.50288409e-02\n",
      "   1.99491277e-01 2.75802284e-01 3.08984423e+00 2.39081651e-01\n",
      "   3.70879412e-01 8.38821650e-01 2.01599523e-01 2.82724237e+00\n",
      "   3.48372626e+00 4.34906453e-01 3.50036836e+00 7.43847489e-01\n",
      "   2.10584950e+00 4.45695929e-02 1.45006740e+00 1.80839300e-02\n",
      "   4.69473505e+00 6.01518992e-03 7.65115857e-01]]\n",
      "\n",
      " [[7.56855607e-01 8.10955558e-03 3.20187902e+00 2.28662863e-02\n",
      "   3.89113463e-03 1.71212871e-02 1.35066843e+00 1.99555419e-02\n",
      "   5.33795953e-02 4.89091307e-01 5.82434416e-01 1.50286723e-02\n",
      "   1.99490651e-01 2.75802970e-01 3.08984685e+00 2.39080980e-01\n",
      "   3.70880187e-01 8.38820398e-01 2.01598883e-01 2.82724476e+00\n",
      "   3.48372912e+00 4.34907317e-01 3.50037146e+00 7.43846357e-01\n",
      "   2.10585165e+00 4.45698872e-02 1.45006907e+00 1.80841144e-02\n",
      "   4.69473839e+00 6.01529516e-03 7.65116990e-01]]\n",
      "\n",
      " [[7.56857634e-01 8.10975954e-03 3.20188332e+00 2.28666253e-02\n",
      "   3.89127596e-03 1.71209928e-02 1.35066557e+00 1.99558586e-02\n",
      "   5.33801243e-02 4.89089787e-01 5.82436144e-01 1.50283938e-02\n",
      "   1.99489668e-01 2.75804162e-01 3.08985090e+00 2.39079878e-01\n",
      "   3.70881557e-01 8.38818312e-01 2.01597884e-01 2.82724905e+00\n",
      "   3.48373389e+00 4.34908807e-01 3.50037551e+00 7.43844390e-01\n",
      "   2.10585499e+00 4.45703566e-02 1.45007193e+00 1.80844199e-02\n",
      "   4.69474220e+00 6.01547211e-03 7.65118897e-01]]\n",
      "\n",
      " [[7.56856978e-01 8.10968969e-03 3.20188165e+00 2.28665136e-02\n",
      "   3.89122707e-03 1.71210896e-02 1.35066664e+00 1.99557524e-02\n",
      "   5.33799492e-02 4.89090264e-01 5.82435548e-01 1.50284935e-02\n",
      "   1.99489996e-01 2.75803804e-01 3.08984947e+00 2.39080235e-01\n",
      "   3.70881110e-01 8.38819027e-01 2.01598212e-01 2.82724714e+00\n",
      "   3.48373199e+00 4.34908301e-01 3.50037360e+00 7.43845046e-01\n",
      "   2.10585356e+00 4.45701964e-02 1.45007074e+00 1.80843174e-02\n",
      "   4.69474125e+00 6.01541251e-03 7.65118361e-01]]\n",
      "\n",
      " [[7.56856978e-01 8.10968969e-03 3.20188165e+00 2.28665136e-02\n",
      "   3.89122707e-03 1.71210896e-02 1.35066664e+00 1.99557524e-02\n",
      "   5.33799492e-02 4.89090264e-01 5.82435548e-01 1.50284935e-02\n",
      "   1.99489996e-01 2.75803804e-01 3.08984947e+00 2.39080235e-01\n",
      "   3.70881110e-01 8.38819027e-01 2.01598212e-01 2.82724714e+00\n",
      "   3.48373199e+00 4.34908301e-01 3.50037360e+00 7.43845046e-01\n",
      "   2.10585356e+00 4.45701964e-02 1.45007074e+00 1.80843174e-02\n",
      "   4.69474125e+00 6.01541251e-03 7.65118361e-01]]\n",
      "\n",
      " [[7.56858528e-01 8.10983963e-03 3.20188427e+00 2.28667613e-02\n",
      "   3.89132998e-03 1.71208736e-02 1.35066473e+00 1.99559908e-02\n",
      "   5.33803366e-02 4.89089042e-01 5.82436800e-01 1.50282849e-02\n",
      "   1.99489266e-01 2.75804669e-01 3.08985257e+00 2.39079401e-01\n",
      "   3.70882154e-01 8.38817477e-01 2.01597482e-01 2.82724977e+00\n",
      "   3.48373485e+00 4.34909403e-01 3.50037718e+00 7.43843675e-01\n",
      "   2.10585618e+00 4.45705503e-02 1.45007288e+00 1.80845391e-02\n",
      "   4.69474459e+00 6.01554150e-03 7.65119731e-01]]\n",
      "\n",
      " [[7.56857634e-01 8.10975954e-03 3.20188332e+00 2.28666253e-02\n",
      "   3.89127596e-03 1.71209928e-02 1.35066557e+00 1.99558586e-02\n",
      "   5.33801243e-02 4.89089787e-01 5.82436144e-01 1.50283938e-02\n",
      "   1.99489668e-01 2.75804162e-01 3.08985090e+00 2.39079878e-01\n",
      "   3.70881557e-01 8.38818312e-01 2.01597884e-01 2.82724905e+00\n",
      "   3.48373389e+00 4.34908807e-01 3.50037551e+00 7.43844390e-01\n",
      "   2.10585499e+00 4.45703566e-02 1.45007193e+00 1.80844199e-02\n",
      "   4.69474220e+00 6.01547211e-03 7.65118897e-01]]\n",
      "\n",
      " [[7.56856859e-01 8.10968410e-03 3.20188165e+00 2.28665061e-02\n",
      "   3.89122195e-03 1.71211008e-02 1.35066664e+00 1.99557468e-02\n",
      "   5.33799306e-02 4.89090353e-01 5.82435489e-01 1.50284981e-02\n",
      "   1.99490026e-01 2.75803745e-01 3.08984947e+00 2.39080265e-01\n",
      "   3.70881051e-01 8.38819146e-01 2.01598257e-01 2.82724714e+00\n",
      "   3.48373199e+00 4.34908241e-01 3.50037360e+00 7.43845165e-01\n",
      "   2.10585356e+00 4.45701815e-02 1.45007074e+00 1.80843063e-02\n",
      "   4.69474125e+00 6.01540646e-03 7.65118241e-01]]\n",
      "\n",
      " [[7.56857157e-01 8.10970552e-03 3.20188165e+00 2.28665434e-02\n",
      "   3.89123731e-03 1.71210673e-02 1.35066652e+00 1.99557804e-02\n",
      "   5.33799827e-02 4.89090174e-01 5.82435668e-01 1.50284683e-02\n",
      "   1.99489921e-01 2.75803834e-01 3.08984995e+00 2.39080146e-01\n",
      "   3.70881170e-01 8.38818848e-01 2.01598153e-01 2.82724762e+00\n",
      "   3.48373199e+00 4.34908390e-01 3.50037432e+00 7.43845046e-01\n",
      "   2.10585403e+00 4.45702411e-02 1.45007122e+00 1.80843417e-02\n",
      "   4.69474125e+00 6.01542415e-03 7.65118361e-01]]\n",
      "\n",
      " [[7.56857395e-01 8.10972229e-03 3.20188212e+00 2.28665657e-02\n",
      "   3.89124779e-03 1.71210449e-02 1.35066628e+00 1.99558064e-02\n",
      "   5.33800200e-02 4.89090025e-01 5.82435846e-01 1.50284488e-02\n",
      "   1.99489862e-01 2.75803983e-01 3.08985043e+00 2.39080071e-01\n",
      "   3.70881349e-01 8.38818729e-01 2.01598078e-01 2.82724810e+00\n",
      "   3.48373270e+00 4.34908539e-01 3.50037479e+00 7.43844748e-01\n",
      "   2.10585427e+00 4.45702709e-02 1.45007145e+00 1.80843659e-02\n",
      "   4.69474125e+00 6.01543812e-03 7.65118599e-01]]\n",
      "\n",
      " [[7.56858766e-01 8.10987782e-03 3.20188546e+00 2.28668302e-02\n",
      "   3.89135582e-03 1.71208177e-02 1.35066426e+00 1.99560430e-02\n",
      "   5.33804260e-02 4.89088804e-01 5.82437098e-01 1.50282355e-02\n",
      "   1.99489072e-01 2.75804847e-01 3.08985353e+00 2.39079207e-01\n",
      "   3.70882332e-01 8.38817179e-01 2.01597288e-01 2.82725072e+00\n",
      "   3.48373556e+00 4.34909672e-01 3.50037766e+00 7.43843317e-01\n",
      "   2.10585690e+00 4.45706397e-02 1.45007336e+00 1.80845987e-02\n",
      "   4.69474506e+00 6.01557363e-03 7.65120149e-01]]\n",
      "\n",
      " [[7.56860018e-01 8.11000075e-03 3.20188808e+00 2.28670351e-02\n",
      "   3.89144314e-03 1.71206370e-02 1.35066271e+00 1.99562404e-02\n",
      "   5.33807427e-02 4.89087850e-01 5.82438290e-01 1.50280660e-02\n",
      "   1.99488476e-01 2.75805622e-01 3.08985543e+00 2.39078537e-01\n",
      "   3.70883256e-01 8.38815808e-01 2.01596692e-01 2.82725310e+00\n",
      "   3.48373842e+00 4.34910625e-01 3.50038052e+00 7.43842065e-01\n",
      "   2.10585880e+00 4.45709340e-02 1.45007503e+00 1.80847850e-02\n",
      "   4.69474840e+00 6.01567887e-03 7.65121281e-01]]\n",
      "\n",
      " [[7.56860375e-01 8.11003800e-03 3.20188856e+00 2.28671022e-02\n",
      "   3.89146898e-03 1.71205848e-02 1.35066199e+00 1.99563000e-02\n",
      "   5.33808321e-02 4.89087611e-01 5.82438469e-01 1.50280166e-02\n",
      "   1.99488282e-01 2.75805801e-01 3.08985662e+00 2.39078343e-01\n",
      "   3.70883435e-01 8.38815510e-01 2.01596484e-01 2.82725406e+00\n",
      "   3.48373961e+00 4.34910834e-01 3.50038123e+00 7.43841827e-01\n",
      "   2.10585952e+00 4.45710197e-02 1.45007575e+00 1.80848371e-02\n",
      "   4.69474983e+00 6.01571240e-03 7.65121639e-01]]\n",
      "\n",
      " [[7.56859004e-01 8.10989365e-03 3.20188594e+00 2.28668526e-02\n",
      "   3.89136863e-03 1.71207953e-02 1.35066402e+00 1.99560691e-02\n",
      "   5.33804595e-02 4.89088655e-01 5.82437277e-01 1.50282150e-02\n",
      "   1.99488997e-01 2.75804967e-01 3.08985353e+00 2.39079133e-01\n",
      "   3.70882511e-01 8.38816941e-01 2.01597214e-01 2.82725120e+00\n",
      "   3.48373604e+00 4.34909821e-01 3.50037837e+00 7.43843079e-01\n",
      "   2.10585713e+00 4.45706807e-02 1.45007360e+00 1.80846229e-02\n",
      "   4.69474506e+00 6.01558713e-03 7.65120268e-01]]\n",
      "\n",
      " [[7.56858230e-01 8.10982380e-03 3.20188427e+00 2.28667390e-02\n",
      "   3.89131973e-03 1.71208978e-02 1.35066473e+00 1.99559648e-02\n",
      "   5.33802845e-02 4.89089221e-01 5.82436740e-01 1.50283100e-02\n",
      "   1.99489340e-01 2.75804520e-01 3.08985257e+00 2.39079520e-01\n",
      "   3.70881975e-01 8.38817775e-01 2.01597556e-01 2.82724977e+00\n",
      "   3.48373485e+00 4.34909254e-01 3.50037718e+00 7.43843794e-01\n",
      "   2.10585618e+00 4.45705205e-02 1.45007288e+00 1.80845149e-02\n",
      "   4.69474459e+00 6.01552753e-03 7.65119493e-01]]\n",
      "\n",
      " [[7.56858110e-01 8.10980797e-03 3.20188379e+00 2.28667092e-02\n",
      "   3.89130949e-03 1.71209201e-02 1.35066509e+00 1.99559387e-02\n",
      "   5.33802472e-02 4.89089310e-01 5.82436621e-01 1.50283296e-02\n",
      "   1.99489430e-01 2.75804490e-01 3.08985186e+00 2.39079595e-01\n",
      "   3.70881915e-01 8.38817775e-01 2.01597646e-01 2.82724953e+00\n",
      "   3.48373437e+00 4.34909165e-01 3.50037646e+00 7.43843913e-01\n",
      "   2.10585570e+00 4.45704758e-02 1.45007265e+00 1.80844907e-02\n",
      "   4.69474363e+00 6.01551402e-03 7.65119493e-01]]\n",
      "\n",
      " [[7.56860137e-01 8.11001379e-03 3.20188808e+00 2.28670575e-02\n",
      "   3.89145082e-03 1.71206258e-02 1.35066271e+00 1.99562553e-02\n",
      "   5.33807799e-02 4.89087760e-01 5.82438350e-01 1.50280511e-02\n",
      "   1.99488416e-01 2.75805652e-01 3.08985543e+00 2.39078492e-01\n",
      "   3.70883316e-01 8.38815689e-01 2.01596618e-01 2.82725310e+00\n",
      "   3.48373842e+00 4.34910685e-01 3.50038052e+00 7.43841946e-01\n",
      "   2.10585880e+00 4.45709489e-02 1.45007503e+00 1.80847961e-02\n",
      "   4.69474840e+00 6.01568865e-03 7.65121400e-01]]\n",
      "\n",
      " [[7.56859243e-01 8.10992066e-03 3.20188594e+00 2.28668991e-02\n",
      "   3.89138656e-03 1.71207562e-02 1.35066378e+00 1.99561156e-02\n",
      "   5.33805303e-02 4.89088476e-01 5.82437515e-01 1.50281750e-02\n",
      "   1.99488878e-01 2.75805116e-01 3.08985400e+00 2.39078969e-01\n",
      "   3.70882660e-01 8.38816762e-01 2.01597080e-01 2.82725167e+00\n",
      "   3.48373675e+00 4.34909970e-01 3.50037885e+00 7.43842900e-01\n",
      "   2.10585737e+00 4.45707403e-02 1.45007408e+00 1.80846583e-02\n",
      "   4.69474697e+00 6.01560902e-03 7.65120506e-01]]\n",
      "\n",
      " [[7.56857514e-01 8.10974371e-03 3.20188284e+00 2.28666030e-02\n",
      "   3.89126316e-03 1.71210151e-02 1.35066605e+00 1.99558381e-02\n",
      "   5.33800907e-02 4.89089847e-01 5.82436025e-01 1.50284190e-02\n",
      "   1.99489728e-01 2.75804102e-01 3.08985090e+00 2.39079952e-01\n",
      "   3.70881498e-01 8.38818431e-01 2.01597959e-01 2.82724857e+00\n",
      "   3.48373318e+00 4.34908748e-01 3.50037551e+00 7.43844509e-01\n",
      "   2.10585475e+00 4.45703268e-02 1.45007169e+00 1.80843957e-02\n",
      "   4.69474220e+00 6.01545814e-03 7.65118897e-01]]\n",
      "\n",
      " [[7.56855011e-01 8.10947549e-03 3.20187688e+00 2.28661504e-02\n",
      "   3.89107806e-03 1.71214063e-02 1.35066962e+00 1.99554171e-02\n",
      "   5.33794016e-02 4.89091873e-01 5.82433760e-01 1.50287813e-02\n",
      "   1.99491084e-01 2.75802553e-01 3.08984542e+00 2.39081413e-01\n",
      "   3.70879710e-01 8.38821232e-01 2.01599285e-01 2.82724285e+00\n",
      "   3.48372746e+00 4.34906721e-01 3.50036907e+00 7.43847132e-01\n",
      "   2.10585022e+00 4.45696972e-02 1.45006788e+00 1.80839952e-02\n",
      "   4.69473600e+00 6.01522764e-03 7.65116215e-01]]\n",
      "\n",
      " [[7.56852150e-01 8.10919143e-03 3.20187163e+00 2.28656735e-02\n",
      "   3.89087992e-03 1.71218198e-02 1.35067320e+00 1.99549682e-02\n",
      "   5.33786602e-02 4.89094138e-01 5.82431316e-01 1.50291687e-02\n",
      "   1.99492499e-01 2.75800824e-01 3.08983970e+00 2.39082962e-01\n",
      "   3.70877683e-01 8.38824153e-01 2.01600716e-01 2.82723784e+00\n",
      "   3.48372173e+00 4.34904635e-01 3.50036383e+00 7.43849933e-01\n",
      "   2.10584569e+00 4.45690341e-02 1.45006430e+00 1.80835705e-02\n",
      "   4.69472885e+00 6.01498084e-03 7.65113413e-01]]], shape=(31, 1, 31), dtype=float32)\n",
      "tf.Tensor(8.572592, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0261722, shape=(), dtype=float32)\n",
      "actor\n",
      "[<tf.Variable 'actor/dense/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[ 0.00853969, -0.00651578,  0.01062227, ...,  0.00187801,\n",
      "        -0.00604207,  0.00813171],\n",
      "       [-0.00632645,  0.00573775, -0.00720339, ...,  0.00342469,\n",
      "         0.00533098, -0.03322322],\n",
      "       [-0.00331249,  0.01464747, -0.00736024, ..., -0.00653853,\n",
      "        -0.00816349, -0.00341983],\n",
      "       ...,\n",
      "       [ 0.00059591,  0.00435688,  0.00040311, ..., -0.01500508,\n",
      "        -0.01422067, -0.00157747],\n",
      "       [ 0.00465342,  0.01529079, -0.00512154, ...,  0.00187807,\n",
      "         0.01675389,  0.00401362],\n",
      "       [ 0.00113273,  0.00740091, -0.00344021, ...,  0.00202601,\n",
      "         0.00788292,  0.00107417]], dtype=float32)>, <tf.Variable 'actor/dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Variable 'actor/dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[-0.01355896,  0.00263778, -0.02468973, ...,  0.01369924,\n",
      "         0.00610486, -0.00365122],\n",
      "       [-0.00337117, -0.01423444, -0.01942873, ...,  0.00605642,\n",
      "         0.01199745,  0.00831584],\n",
      "       [-0.01701328,  0.01189769,  0.00649015, ..., -0.00108693,\n",
      "         0.00805624,  0.00470935],\n",
      "       ...,\n",
      "       [-0.00476423,  0.01328607, -0.01627686, ...,  0.01822136,\n",
      "         0.00158335, -0.00097592],\n",
      "       [-0.01102929,  0.0071977 , -0.0072223 , ..., -0.00680401,\n",
      "         0.01424065,  0.00307976],\n",
      "       [-0.00257237,  0.00774996, -0.00619092, ..., -0.01071037,\n",
      "        -0.00055843, -0.01247684]], dtype=float32)>, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[-1.1224198e-03,  3.5086311e-03,  1.4074416e-02, ...,\n",
      "         1.5748603e-02,  4.7776527e-03,  1.4910380e-02],\n",
      "       [ 2.8251379e-03, -1.5553749e-03,  2.7581509e-03, ...,\n",
      "        -6.7600491e-03, -1.5882140e-02,  5.4981397e-04],\n",
      "       [ 9.1308197e-03,  8.9684213e-03, -2.2343492e-04, ...,\n",
      "         3.8593456e-03,  4.3453355e-03,  2.0397162e-02],\n",
      "       ...,\n",
      "       [ 7.5027696e-05, -1.1175807e-02,  8.0059026e-04, ...,\n",
      "         1.4039454e-02,  5.1026028e-03, -4.7861505e-03],\n",
      "       [-4.3958856e-04, -2.8233668e-03,  9.5623322e-03, ...,\n",
      "        -5.2876729e-03, -1.2833004e-02, -1.2643497e-02],\n",
      "       [-1.1756892e-02, -5.2090934e-03, -7.6141581e-03, ...,\n",
      "         5.7068956e-03, -4.9277875e-03,  2.1857172e-03]], dtype=float32)>, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\n",
      "array([[-0.02158797, -0.07005644,  0.1206525 ,  0.0365369 ],\n",
      "       [ 0.18660891, -0.06472568, -0.16434792, -0.03862917],\n",
      "       [-0.2661875 , -0.11951412,  0.1785737 ,  0.10809135],\n",
      "       [ 0.240453  , -0.2750672 , -0.13723223,  0.14678413],\n",
      "       [ 0.25027835, -0.13852471, -0.12773119,  0.01944196],\n",
      "       [ 0.00147703,  0.03144774,  0.18875831,  0.22032857],\n",
      "       [ 0.21957308,  0.05811724, -0.13629265, -0.1511785 ],\n",
      "       [-0.20247859, -0.00369778,  0.2371782 ,  0.10556936],\n",
      "       [-0.2376012 , -0.17829262, -0.05658454, -0.0329048 ],\n",
      "       [ 0.23479265,  0.06495458,  0.07955492,  0.12124676],\n",
      "       [-0.11125916, -0.04413527,  0.16306004, -0.12030703],\n",
      "       [-0.18300807, -0.12298696,  0.25498873, -0.01637203],\n",
      "       [-0.2599401 , -0.14809488,  0.09497434, -0.02876723],\n",
      "       [ 0.26280648, -0.22155036,  0.227103  , -0.19129029],\n",
      "       [-0.2747692 , -0.2207333 , -0.04827406, -0.19222832],\n",
      "       [-0.13614683,  0.2253288 ,  0.1821236 ,  0.21527308],\n",
      "       [ 0.02762213, -0.0318478 ,  0.2834422 , -0.2557096 ],\n",
      "       [-0.02377769, -0.27592692,  0.23023987,  0.17955449],\n",
      "       [ 0.17044488, -0.21899578,  0.23448962,  0.22567242],\n",
      "       [-0.10451518, -0.2110897 , -0.2934195 ,  0.08735132],\n",
      "       [ 0.01416114, -0.2706916 ,  0.17301002, -0.20412892],\n",
      "       [-0.28120255,  0.11676139, -0.00532353, -0.14611827],\n",
      "       [-0.14638016,  0.1130845 , -0.20672593,  0.26536858],\n",
      "       [-0.22903329, -0.04051316, -0.00305903,  0.29281092],\n",
      "       [ 0.16146919, -0.00070664,  0.00197634, -0.2966369 ],\n",
      "       [ 0.08681753, -0.07763796,  0.2741887 , -0.24430914],\n",
      "       [ 0.21542966,  0.12947473,  0.18409666,  0.0149481 ],\n",
      "       [-0.17874232,  0.2561506 , -0.05561967, -0.03979835],\n",
      "       [ 0.11216828,  0.2116636 , -0.0645283 ,  0.15592349],\n",
      "       [-0.09696263, -0.16443184,  0.07529533, -0.09715711],\n",
      "       [ 0.25344998, -0.03239664, -0.21323244,  0.20436382],\n",
      "       [-0.25758415,  0.20445311,  0.19473568,  0.12740457],\n",
      "       [-0.07417367,  0.17925054, -0.19096658,  0.23697805],\n",
      "       [ 0.17919558,  0.2916116 ,  0.00252199,  0.10983044],\n",
      "       [-0.10092463, -0.2968236 , -0.15595104,  0.11739743],\n",
      "       [ 0.05228981, -0.15935186,  0.26877135,  0.1012589 ],\n",
      "       [-0.29042473, -0.13362022,  0.15681258,  0.21364486],\n",
      "       [ 0.12989044,  0.22284108, -0.11519355,  0.19709966],\n",
      "       [ 0.08760038, -0.27823058,  0.02493829, -0.08668673],\n",
      "       [ 0.12108457, -0.16910708,  0.10571155, -0.22515726],\n",
      "       [-0.11943579, -0.00284383,  0.22702181, -0.06079845],\n",
      "       [ 0.13620609,  0.24687487, -0.06993058, -0.04499179],\n",
      "       [-0.22452909,  0.1907573 , -0.10236576, -0.17476007],\n",
      "       [ 0.129051  , -0.21703744, -0.1347139 , -0.14070578],\n",
      "       [-0.03918433, -0.16551398, -0.08996709, -0.10164595],\n",
      "       [ 0.29064655, -0.06536901, -0.00285989, -0.02880201],\n",
      "       [-0.16466221,  0.06134015, -0.26548764,  0.11443838],\n",
      "       [-0.1457658 ,  0.00475648,  0.24825889, -0.13648535],\n",
      "       [ 0.08494809, -0.25637484,  0.1826942 , -0.04963148],\n",
      "       [-0.11487161, -0.04434252,  0.10491064, -0.26543918],\n",
      "       [ 0.00058171,  0.02626464,  0.0354214 , -0.13539733],\n",
      "       [-0.00948277, -0.27872935,  0.12518665, -0.08339965],\n",
      "       [ 0.1053969 , -0.28872246,  0.02776739,  0.2543745 ],\n",
      "       [-0.0369314 ,  0.03190628,  0.09900963, -0.00712848],\n",
      "       [-0.10224204,  0.07233664, -0.21551457,  0.06509438],\n",
      "       [ 0.18081611, -0.27546707, -0.28287494,  0.15028629],\n",
      "       [-0.1606039 ,  0.16512665,  0.0204159 , -0.07033597],\n",
      "       [ 0.28629142, -0.24938324, -0.03003052, -0.19869992],\n",
      "       [-0.15548794,  0.06056374, -0.02504247, -0.11444908],\n",
      "       [-0.23291966, -0.2887753 ,  0.02025867,  0.06195474],\n",
      "       [-0.11439279,  0.1726465 , -0.20391476, -0.07539561],\n",
      "       [ 0.22371233,  0.08779812,  0.26678282, -0.24339506],\n",
      "       [ 0.22273314,  0.28431982, -0.2908014 ,  0.2913713 ],\n",
      "       [-0.27033517, -0.11323628, -0.01478845, -0.03955027]],\n",
      "      dtype=float32)>, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "critic\n",
      "[<tf.Variable 'critic/dense_4/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[-1.3408980e-02, -1.5823049e-02,  6.5707834e-05, ...,\n",
      "        -6.5480787e-03,  8.9101363e-03, -1.2473871e-02],\n",
      "       [ 1.6671851e-02, -1.8369380e-02,  3.6949106e-03, ...,\n",
      "        -7.5373566e-03,  1.2022807e-02, -1.9623403e-02],\n",
      "       [-4.2043026e-03,  1.8890450e-02,  1.2788985e-02, ...,\n",
      "         1.2799181e-03, -2.5041648e-03,  4.5807553e-03],\n",
      "       ...,\n",
      "       [ 4.6416550e-04, -6.6181938e-03,  2.6971537e-03, ...,\n",
      "        -5.8523053e-03, -4.3949871e-03,  2.9955159e-03],\n",
      "       [ 1.6007384e-02, -7.9543330e-03, -8.2045654e-03, ...,\n",
      "         4.6768007e-03,  3.2875231e-03,  9.9104848e-03],\n",
      "       [ 5.3149909e-03,  3.4799110e-02, -5.6961719e-03, ...,\n",
      "         5.0422004e-03,  5.3465669e-03,  1.6091529e-02]], dtype=float32)>, <tf.Variable 'critic/dense_4/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Variable 'critic/dense_5/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[ 0.00551328, -0.00916497,  0.00934788, ...,  0.01719904,\n",
      "         0.00515407, -0.01563706],\n",
      "       [ 0.00504222,  0.00524142,  0.00332025, ..., -0.00273972,\n",
      "         0.00839442, -0.01056209],\n",
      "       [ 0.00225191, -0.01202353, -0.00238961, ..., -0.00741212,\n",
      "         0.015544  , -0.01414143],\n",
      "       ...,\n",
      "       [ 0.01247072,  0.00843007, -0.00070156, ..., -0.00421264,\n",
      "        -0.00740414, -0.00361787],\n",
      "       [-0.00668948,  0.00566031,  0.02304539, ..., -0.00137021,\n",
      "        -0.00396119,  0.00508378],\n",
      "       [-0.01540623,  0.01654382, -0.00572743, ..., -0.00847377,\n",
      "        -0.00299834,  0.00792635]], dtype=float32)>, <tf.Variable 'critic/dense_5/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_6/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.00309348,  0.00500346,  0.00198086, ...,  0.00438584,\n",
      "         0.00181324, -0.02150507],\n",
      "       [-0.0065799 , -0.00313326,  0.00679712, ...,  0.00126303,\n",
      "        -0.00555431,  0.01587995],\n",
      "       [ 0.01497685,  0.00539966, -0.00940852, ...,  0.00733766,\n",
      "         0.01161444, -0.00734496],\n",
      "       ...,\n",
      "       [-0.00915155,  0.01136518,  0.00503585, ..., -0.00045774,\n",
      "        -0.00315698,  0.01611267],\n",
      "       [-0.00385627,  0.00567974,  0.00490533, ..., -0.00019488,\n",
      "        -0.00846095, -0.00544845],\n",
      "       [-0.00798623, -0.01305962,  0.01153127, ...,  0.00581362,\n",
      "         0.02069303, -0.00500973]], dtype=float32)>, <tf.Variable 'critic/dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_7/kernel:0' shape=(64, 1) dtype=float32, numpy=\n",
      "array([[-0.02208054],\n",
      "       [-0.07165489],\n",
      "       [ 0.12340537],\n",
      "       [ 0.03737056],\n",
      "       [ 0.19086668],\n",
      "       [-0.06620249],\n",
      "       [-0.16809776],\n",
      "       [-0.03951055],\n",
      "       [-0.27226096],\n",
      "       [-0.12224102],\n",
      "       [ 0.18264815],\n",
      "       [ 0.11055762],\n",
      "       [ 0.24593931],\n",
      "       [-0.2813433 ],\n",
      "       [-0.1403634 ],\n",
      "       [ 0.15013322],\n",
      "       [ 0.25598884],\n",
      "       [-0.14168537],\n",
      "       [-0.13064557],\n",
      "       [ 0.01988557],\n",
      "       [ 0.00151074],\n",
      "       [ 0.03216526],\n",
      "       [ 0.19306514],\n",
      "       [ 0.22535568],\n",
      "       [ 0.22458303],\n",
      "       [ 0.05944327],\n",
      "       [-0.13940237],\n",
      "       [-0.15462786],\n",
      "       [-0.20709844],\n",
      "       [-0.00378215],\n",
      "       [ 0.24258977],\n",
      "       [ 0.10797808],\n",
      "       [-0.24302244],\n",
      "       [-0.18236063],\n",
      "       [-0.0578756 ],\n",
      "       [-0.03365555],\n",
      "       [ 0.2401498 ],\n",
      "       [ 0.06643662],\n",
      "       [ 0.08137009],\n",
      "       [ 0.12401319],\n",
      "       [-0.11379772],\n",
      "       [-0.04514229],\n",
      "       [ 0.1667805 ],\n",
      "       [-0.12305202],\n",
      "       [-0.1871837 ],\n",
      "       [-0.1257931 ],\n",
      "       [ 0.26080668],\n",
      "       [-0.01674557],\n",
      "       [-0.26587102],\n",
      "       [-0.1514739 ],\n",
      "       [ 0.09714133],\n",
      "       [-0.02942359],\n",
      "       [ 0.26880282],\n",
      "       [-0.22660537],\n",
      "       [ 0.23228472],\n",
      "       [-0.19565488],\n",
      "       [-0.2810385 ],\n",
      "       [-0.22576967],\n",
      "       [-0.0493755 ],\n",
      "       [-0.19661431],\n",
      "       [-0.13925323],\n",
      "       [ 0.23047   ],\n",
      "       [ 0.18627903],\n",
      "       [ 0.22018486]], dtype=float32)>, <tf.Variable 'critic/dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "[None, None, None, None, None, None, None, None]\n",
      "[<tf.Tensor: shape=(8, 256), dtype=float32, numpy=\n",
      "array([[-2.7973863e-05,  0.0000000e+00,  5.2720596e-05, ...,\n",
      "         0.0000000e+00,  1.1951745e-04,  0.0000000e+00],\n",
      "       [-2.7328689e-04,  0.0000000e+00,  6.8695890e-04, ...,\n",
      "         0.0000000e+00,  1.4699184e-03,  0.0000000e+00],\n",
      "       [-1.4299521e-04,  0.0000000e+00,  3.4942248e-04, ...,\n",
      "         0.0000000e+00,  7.5310492e-04,  0.0000000e+00],\n",
      "       ...,\n",
      "       [ 1.3852652e-05,  0.0000000e+00, -4.1829775e-05, ...,\n",
      "         0.0000000e+00, -8.9175046e-05,  0.0000000e+00],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)>, <tf.Tensor: shape=(256,), dtype=float32, numpy=\n",
      "array([-2.09867125e-04,  0.00000000e+00,  5.16509404e-04, -5.19293186e-04,\n",
      "       -1.00294698e-03,  0.00000000e+00,  0.00000000e+00,  7.57421076e-04,\n",
      "        3.26839683e-04,  0.00000000e+00, -4.42477030e-04, -3.19451792e-04,\n",
      "       -2.96922866e-04,  0.00000000e+00,  7.37924711e-04,  0.00000000e+00,\n",
      "        0.00000000e+00, -2.41043424e-04, -6.20186693e-05,  0.00000000e+00,\n",
      "        8.21337686e-04, -3.87460714e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00, -1.38472024e-04,  0.00000000e+00,\n",
      "        0.00000000e+00, -8.40581488e-04,  0.00000000e+00,  3.76959855e-04,\n",
      "        0.00000000e+00,  0.00000000e+00, -1.13205344e-04,  0.00000000e+00,\n",
      "       -3.35493853e-04, -2.33393017e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00, -1.22963756e-04,  0.00000000e+00,  7.88870617e-04,\n",
      "        0.00000000e+00, -2.93124467e-04,  5.93997356e-05,  0.00000000e+00,\n",
      "        7.89384503e-05,  0.00000000e+00,  5.71933575e-04, -3.12266464e-04,\n",
      "        0.00000000e+00,  0.00000000e+00, -1.49380660e-03,  5.12817351e-05,\n",
      "        0.00000000e+00, -3.71418224e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00, -3.44972359e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "       -3.76236785e-05,  3.77583550e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "       -3.37688922e-04,  0.00000000e+00,  0.00000000e+00,  1.07784770e-04,\n",
      "        7.65669334e-04,  0.00000000e+00, -1.67878607e-04,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.78237077e-04,  6.85235078e-04,  0.00000000e+00,\n",
      "       -2.18701185e-04,  0.00000000e+00,  0.00000000e+00,  9.83006903e-04,\n",
      "        0.00000000e+00, -4.16407100e-04,  7.51180341e-05, -4.30054788e-04,\n",
      "        0.00000000e+00,  0.00000000e+00, -4.13154121e-05,  0.00000000e+00,\n",
      "        1.00267316e-05,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.50727588e-04,  0.00000000e+00, -2.06708610e-05,\n",
      "        5.81509026e-04, -5.40750101e-04,  0.00000000e+00, -6.36093901e-05,\n",
      "        1.59498493e-04,  0.00000000e+00,  3.49074107e-04,  0.00000000e+00,\n",
      "        1.04100684e-04,  8.56942439e-04,  4.32925066e-04,  1.22547324e-04,\n",
      "       -3.50006769e-04,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        4.96270193e-04, -9.15779383e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "       -3.08632560e-04, -6.28617068e-04, -3.09309660e-04,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.17375923e-04, -1.96027569e-04,\n",
      "       -6.34263270e-05,  0.00000000e+00,  3.95297306e-04,  1.49825879e-03,\n",
      "        4.20439086e-04, -9.85972656e-06, -6.65460655e-04,  0.00000000e+00,\n",
      "        0.00000000e+00, -5.60662011e-04, -3.41133389e-04,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.17708254e-04,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.76114467e-04, -6.74616953e-04,\n",
      "       -1.07304496e-03,  0.00000000e+00,  4.99128073e-04, -1.19489305e-04,\n",
      "       -8.21130452e-05,  1.10427718e-05,  1.86350771e-05,  4.63709497e-04,\n",
      "       -5.72986435e-04,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "       -4.05205530e-04,  0.00000000e+00,  4.07036161e-04, -1.90046063e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        8.61133041e-04,  4.77371272e-04, -4.01481404e-04, -1.50854001e-04,\n",
      "        0.00000000e+00, -3.01872824e-05,  1.90668652e-04,  0.00000000e+00,\n",
      "        0.00000000e+00,  8.36470354e-05,  0.00000000e+00, -5.65116236e-04,\n",
      "       -3.25760891e-04,  0.00000000e+00, -6.43350068e-04,  0.00000000e+00,\n",
      "       -5.00394481e-06,  0.00000000e+00,  0.00000000e+00,  2.75161670e-04,\n",
      "        8.40014909e-05,  0.00000000e+00,  3.27318849e-04, -4.89778176e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.49371398e-04,\n",
      "       -1.40832993e-03,  2.36144697e-04, -4.89954196e-04, -6.80832367e-04,\n",
      "        0.00000000e+00, -1.57595146e-04, -4.42963152e-04, -1.71913416e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.64919521e-03,  1.16381515e-03,\n",
      "        0.00000000e+00,  5.04726486e-04, -2.13446765e-04,  0.00000000e+00,\n",
      "        0.00000000e+00, -3.93049268e-04,  0.00000000e+00, -4.97605186e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  8.84971523e-05,  0.00000000e+00,\n",
      "       -5.16023312e-04, -1.16387600e-04,  7.63240678e-04, -3.43287858e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.97966016e-04,\n",
      "        3.77617835e-04,  4.67408885e-04, -3.17956583e-04,  0.00000000e+00,\n",
      "       -1.93406508e-04,  9.56017640e-04, -1.13492366e-04, -2.25210169e-06,\n",
      "        0.00000000e+00, -4.20672528e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.01238408e-03, -3.69481451e-04, -4.38653340e-04,  8.85251036e-04,\n",
      "        0.00000000e+00,  7.58828421e-04,  1.21723613e-04,  5.98526676e-04,\n",
      "        0.00000000e+00,  1.58271566e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "       -7.43877317e-05,  0.00000000e+00,  1.10958540e-03,  0.00000000e+00],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(256, 128), dtype=float32, numpy=\n",
      "array([[-6.8311099e-05,  3.3689612e-05,  0.0000000e+00, ...,\n",
      "        -9.4478950e-05,  0.0000000e+00, -6.2671985e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [-4.6716741e-05,  2.2249036e-05,  0.0000000e+00, ...,\n",
      "        -6.2196239e-05,  0.0000000e+00, -4.0697672e-05],\n",
      "       ...,\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [-4.8712009e-05,  2.3633726e-05,  0.0000000e+00, ...,\n",
      "        -6.6055742e-05,  0.0000000e+00, -4.3433283e-05],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
      "array([-0.00522313,  0.00232419,  0.        , -0.00592813, -0.00659745,\n",
      "        0.00772769,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        , -0.00141168,  0.00576427,  0.        ,\n",
      "        0.00526635,  0.        , -0.00961788,  0.00124895,  0.        ,\n",
      "        0.        ,  0.01016646,  0.00515713,  0.        ,  0.        ,\n",
      "        0.        , -0.00293544,  0.        , -0.01617958,  0.00093125,\n",
      "       -0.0010438 ,  0.        ,  0.00163288, -0.00444735,  0.        ,\n",
      "        0.        ,  0.        ,  0.        , -0.00997084,  0.0052145 ,\n",
      "        0.00106443,  0.        , -0.00158533, -0.00134508,  0.00728126,\n",
      "        0.00074499, -0.00769997,  0.        ,  0.        ,  0.00146806,\n",
      "       -0.00155262, -0.01071666,  0.        ,  0.        , -0.00029529,\n",
      "       -0.00257416,  0.        ,  0.00065662, -0.01685748, -0.00080204,\n",
      "        0.01580214,  0.00081739,  0.        , -0.01549278,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        , -0.01590567,\n",
      "       -0.00139125,  0.        ,  0.        ,  0.        ,  0.00173034,\n",
      "        0.        ,  0.00110625, -0.00393163, -0.00108046,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.00424887,  0.00160198,  0.        ,  0.01160498,  0.        ,\n",
      "        0.00386223,  0.00430978,  0.        , -0.00059564,  0.        ,\n",
      "        0.        ,  0.00136858,  0.00255838, -0.00424231,  0.00769139,\n",
      "       -0.00525898,  0.        ,  0.        , -0.00091641,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "       -0.00905197,  0.00020765, -0.00366365, -0.00036074,  0.00231105,\n",
      "        0.00654152,  0.        , -0.00149247,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        , -0.01320705,\n",
      "       -0.0065032 ,  0.        , -0.0041912 ], dtype=float32)>, <tf.Tensor: shape=(128, 64), dtype=float32, numpy=\n",
      "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 3.6808164e-05,\n",
      "        2.9750461e-05, 3.5165536e-05],\n",
      "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 5.4901720e-05,\n",
      "        4.4374716e-05, 5.2451640e-05],\n",
      "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
      "        0.0000000e+00, 0.0000000e+00],\n",
      "       ...,\n",
      "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 5.7044195e-04,\n",
      "        4.6106384e-04, 5.4498500e-04],\n",
      "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
      "        0.0000000e+00, 0.0000000e+00],\n",
      "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.8281272e-04,\n",
      "        2.2858538e-04, 2.7019167e-04]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
      "array([ 0.        ,  0.        ,  0.        ,  0.03353896,  0.        ,\n",
      "        0.        ,  0.        ,  0.        , -0.244346  , -0.10970763,\n",
      "        0.        ,  0.09922212,  0.        , -0.23620698,  0.        ,\n",
      "        0.13474002,  0.        ,  0.        ,  0.        ,  0.0178467 ,\n",
      "        0.00135584,  0.01862406,  0.        ,  0.        ,  0.20155652,\n",
      "        0.        ,  0.        , -0.13877386,  0.        ,  0.        ,\n",
      "        0.        ,  0.        , -0.21810532,  0.        ,  0.        ,\n",
      "       -0.03020485,  0.21552722,  0.        ,  0.07302721,  0.        ,\n",
      "       -0.10213002, -0.04051384,  0.14968048,  0.        , -0.16799171,\n",
      "        0.        ,  0.        , -0.01502864,  0.        ,  0.        ,\n",
      "        0.08718142, -0.01533302,  0.        ,  0.        ,  0.        ,\n",
      "       -0.17559439,  0.        ,  0.        , -0.04431302, -0.17645544,\n",
      "       -0.02822037,  0.20683989,  0.16717982,  0.19760929], dtype=float32)>, <tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n",
      "array([[0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [2.4690304e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [2.1671304e-04],\n",
      "       [1.5617852e-04],\n",
      "       [0.0000000e+00],\n",
      "       [2.8409009e-04],\n",
      "       [0.0000000e+00],\n",
      "       [2.0670539e-05],\n",
      "       [0.0000000e+00],\n",
      "       [1.0763384e-04],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.2227907e-04],\n",
      "       [1.1488755e-04],\n",
      "       [3.2241023e-06],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.0062630e-05],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [7.8288649e-05],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [6.2350373e-05],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [2.2263906e-04],\n",
      "       [6.0451745e-05],\n",
      "       [0.0000000e+00],\n",
      "       [6.4793538e-05],\n",
      "       [0.0000000e+00],\n",
      "       [2.8115106e-04],\n",
      "       [4.7058311e-05],\n",
      "       [5.8711561e-05],\n",
      "       [0.0000000e+00],\n",
      "       [1.9067440e-05],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [5.1586609e-05],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.6191572e-05],\n",
      "       [2.1144283e-06],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [6.2827123e-05],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.6096215e-05],\n",
      "       [1.8136963e-04],\n",
      "       [1.0554862e-06],\n",
      "       [9.5852905e-05],\n",
      "       [3.1473988e-04],\n",
      "       [7.0240014e-05]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8974699], dtype=float32)>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/2511223439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mppo_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/1049264543.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/1049264543.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, episodes, optimizer, clip_param, c_1, c_2)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;31m# Compute train losses and action by chosen by policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             actor_loss, critic_loss = self.train_step(\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# States\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mepisode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/1049264543.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[1;32m     76\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0']."
     ]
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent(env_name='LunarLander-v2', render=False,epochs=1)\n",
    "ppo_agent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
