{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        # Outputs from the actor network, an action is sampled from (Probabilities)\n",
    "        self.logits = []\n",
    "        # Outputs from the crtitics network (Values)\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) \n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collected values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards)])\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(4, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "    #@tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "        #logits = self.call(observation)\n",
    "        logits = self(observation)\n",
    "    # tf.print(type(logits))\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    # tf.print(action)\n",
    "        return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "# Number of iterations\n",
    "epochs = 1\n",
    "# Leads to ~10 Episodes per epoch, then compute new parameters (smaller batching)\n",
    "steps_per_epoch = 1000 \n",
    "\n",
    "# Learning rate for actor and critic\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "\n",
    "# Parameter to decide how strongly the policy ratio gets clipped therefore how much policy (actor network)\n",
    "#  updates we allow\n",
    "# The selected 0.2 is the number proposed by the original paper by OpenAI\n",
    "clip_ratio = 0.2\n",
    "# Weighs loss of critic model\n",
    "c_1 = 0.5\n",
    "\n",
    "#\n",
    "target_kl = 0.01\n",
    "\n",
    "\n",
    "# Update weights with Adam optimizer\n",
    "optimizer = Adam()\n",
    "\n",
    "# To toggle displaying of environment\n",
    "render = False\n",
    "\n",
    "# Discount variable for rewards to whey immediate rewards stronger\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# Get dimensions of state and amount of possible actions (4 for LunarLander-v2)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage object to save observations, actions, rewards etc. during trajectory\n",
    "#storage = Storage()\n",
    "\n",
    "# initialize actor and critics model\n",
    "#observation_input = Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "# actor = Actor()\n",
    "# critic = Critic()\n",
    "\n",
    "# Initialize: observation(agent state), \n",
    "# episode return(summed rewards for singe ) and \n",
    "# episode length(amount of steps taken (=frames) before agent finished)\n",
    "# observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    ###Skizze - Not used yet\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "    Whats missing: \n",
    "    - All the FUCKING self's before variable assignment and for functions (fuck you python, even though i love you)    \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        ###Maybe pass hyperparameters?\n",
    "        '''\n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        self.storage = Storage()\n",
    "        #print(self.actor.trainable_variables())\n",
    "\n",
    "\n",
    "    def collect_train_data(self):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        for epoch in tqdm_notebook(range(epochs), desc = 'Epochs'):\n",
    "\n",
    "            # Initialize values for return, length and episodes\n",
    "            sum_return = 0\n",
    "            sum_length = 0\n",
    "            num_episodes = 0\n",
    "\n",
    "            # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "            #  allows takes on action in a state and saves the information in storage object\n",
    "            for t in tqdm_notebook(range(steps_per_epoch), desc = 'Epoch:' + str(epoch)):\n",
    "\n",
    "                # Toggles displaying of environment\n",
    "                if render or epoch == epochs-1 and epochs != 1:\n",
    "                    env.render()\n",
    "\n",
    "                # Reshaping observation to fit as input for Actor network (policy)\n",
    "                observation = observation.reshape(1,-1)\n",
    "                \n",
    "                # Obtain action and logits for this observation by our actor\n",
    "                logits, action = self.actor.sample_action(observation)\n",
    "                \n",
    "                # Take action in environment and obtain the rewards for it\n",
    "                # Variable done represents wether agent has finished \n",
    "                # The last variable would be diagnostic information, not needed for training\n",
    "                observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "                # Sum up rewards over this episode and count amount of frames\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "\n",
    "                # Get the Base-Estimate from the Critics network\n",
    "                base_estimate = self.critic(observation)\n",
    "\n",
    "                # Store Variables collected in this timestep t\n",
    "                self.storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "                # Save the new state of our agent\n",
    "                observation = observation_new\n",
    "                \n",
    "                # Check if terminal state is reached in environment\n",
    "                if done:\n",
    "                    # Save information about episode\n",
    "                    self.storage.conclude_episode()\n",
    "                    # Refresh environment and reset return and length value\n",
    "                    observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "            # obtain all episodes saved in storage\n",
    "            # episodes, amount_episodes = self.storage.get_episodes()\n",
    "\n",
    "\n",
    "    def actor_loss_fun(self, actions, logits_old, logits_new, rewards, b_estimates_new, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "        ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates_new)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "        #l1 = np.array(l1, dtype=\"float32\")\n",
    "        #l2 = np.array(l2, dtype=\"float32\")\n",
    "        \n",
    "\n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        #actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "        l1 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l1]), dtype=tf.float32)\n",
    "        l2 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l2]), dtype=tf.float32)\n",
    "        return tf.convert_to_tensor(l1, dtype=tf.float32), tf.convert_to_tensor(l2, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape() as tape, tf.GradientTape() as tape2:\n",
    "            # print(self.actor.trainable_variables())\n",
    "            # Obtain action and logits for this state selected by policy\n",
    "            #print(f' Observation shape/type {observation}')\n",
    "            #print(f'Trainables: {self.actor.layers[0].weights}')\n",
    "\n",
    "\n",
    "            # logits_new, actions_new = sample_action(states)\n",
    "            logits_new = []\n",
    "            b_estimates_new = []\n",
    "\n",
    "            # Compute values with updated critic network\n",
    "            # b_estimates_new = critic(states)\n",
    "\n",
    "            # till we work with np arrays we need to sample each action for this by looping through it\n",
    "            for i in states:\n",
    "                logits, _ = self.actor.sample_action(i)\n",
    "                logits_new.append(logits)\n",
    "                b_estimate = self.critic(i)\n",
    "                \n",
    "                b_estimates_new.append(b_estimate)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen fÃ¼r den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()\n",
    "            #print('Weewoo')\n",
    "            #print(tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print('type critic')\n",
    "            print(type((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "\n",
    "            critic_loss = tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            #print('minimum')\n",
    "            #print(-tf.reduce_mean(tf.minimum(l1, l2)))\n",
    "            #print(type(tf.minimum(l1,l2)))\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "            #critic_loss = tf.cast(critic_loss, dtype=tf.float32)\n",
    "            #print(f'Critics loss:{type(critic_loss)}. Actor Loss {actor_loss.dtype}')\n",
    "\n",
    "            #print('Actor weights')\n",
    "            #print(print(self.actor.layers[0].weights))\n",
    "\n",
    "            #print('actor')\n",
    "            #print(actor_loss)\n",
    "            #print(type(actor_loss))\n",
    "            #print('critic')\n",
    "            #print(critic_loss)\n",
    "            #print(type(critic_loss))\n",
    "\n",
    "            actor_loss = tf.convert_to_tensor(actor_loss, dtype=tf.float32)\n",
    "\n",
    "            print(actor_loss)\n",
    "            print(critic_loss)\n",
    "            print('actor')\n",
    "            print(self.actor.trainable_variables)\n",
    "            print('critic')\n",
    "            print(self.critic.trainable_variables)\n",
    "            a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            c_gradients = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            print(a_gradients)\n",
    "            print(c_gradients)\n",
    "\n",
    "            #print(tape)\n",
    "            #print('Actor loss')\n",
    "            #print(actor_loss)\n",
    "            #print('Trainable Weights')\n",
    "            #print(self.actor.trainable_weights)\n",
    "        \n",
    "        #print(f'Gradients Actor: {a_gradients}. Gradients Critic: {c_gradients}')\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def update_policy(self, episodes, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                        [look at single one]\n",
    "        '''\n",
    "        # for epoch in training_iteratins:\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in tqdm_notebook(episodes):\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                # States\n",
    "                episode[0],\n",
    "                # Actions\n",
    "                episode[1],\n",
    "                #optimizer (Adam)\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2],\n",
    "                # Rewards\n",
    "                episode[3],\n",
    "                clip_param,\n",
    "                c_1,\n",
    "                c_2 \n",
    "            )\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "            return train_losses_actor, train_losses_critic\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # # Estimated Value of the current situtation from the critics network\n",
    "        # b_estimates = self.episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "    ## get ratio lutsch noch ARSCH, das Ding verarscht mich anders\n",
    "\n",
    "    def get_ratio_episode(self, actions, logits_old, logits_new): \n",
    "        r = []\n",
    "        for a, o, n in zip(actions, logits_old, logits_new):\n",
    "            o = tf.convert_to_tensor(o)\n",
    "            n = tf.convert_to_tensor(n)\n",
    "            #print(f'A: {a} O: {type(o)} N: {type(n)}')\n",
    "\n",
    "            #get the Logarithmic version of all logits for computational efficiency\n",
    "            log_prob_old = tf.nn.log_softmax(o)\n",
    "            log_prob_new = tf.nn.log_softmax(n)\n",
    "\n",
    "            # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "            logprobability_old = tf.reduce_sum(\n",
    "                tf.one_hot(a, num_actions) * log_prob_old, axis=1\n",
    "            )\n",
    "            logprobability_new = tf.reduce_sum(\n",
    "                tf.one_hot(a, num_actions) * log_prob_new, axis=1\n",
    "            )\n",
    "            # get the ratio of new over old prob\n",
    "            ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "            r.append(ratio)\n",
    "        return r\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.collect_train_data()\n",
    "        data, _ = self.storage.get_episodes()\n",
    "        #print(data)\n",
    "        self.update_policy(data, optimizer, clip_ratio)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb583413b15344318131a2bd52be8251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280da017483941a99d73f4d3edc2ca90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4564437517489da531078ad601aa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type critic\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[[2.418201  2.491415  2.7956443 ... 4.403715  3.319935  3.0709276]]\n",
      "\n",
      " [[2.4181893 2.4914033 2.7956316 ... 4.4037304 3.3199487 3.0709407]]\n",
      "\n",
      " [[2.4182024 2.4914165 2.795646  ... 4.403713  3.3199337 3.070926 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.4181814 2.491395  2.7956233 ... 4.403742  3.319958  3.0709498]]\n",
      "\n",
      " [[2.418177  2.4913905 2.7956183 ... 4.403748  3.3199635 3.070955 ]]\n",
      "\n",
      " [[2.4181778 2.4913917 2.7956195 ... 4.4037466 3.319962  3.0709536]]], shape=(52, 1, 52), dtype=float32)\n",
      "tf.Tensor(12.281276, shape=(), dtype=float32)\n",
      "tf.Tensor(4.8927484, shape=(), dtype=float32)\n",
      "actor\n",
      "[<tf.Variable 'actor/dense/kernel:0' shape=(8, 128) dtype=float32, numpy=\n",
      "array([[-1.63800921e-02, -1.56523436e-02, -7.40791345e-03, ...,\n",
      "        -1.41428283e-03,  9.91461449e-04,  3.09268874e-03],\n",
      "       [-2.78602242e-02, -1.35148009e-02, -6.33828156e-03, ...,\n",
      "        -1.27423191e-02,  5.50283771e-03,  3.61146708e-03],\n",
      "       [-2.23480235e-03,  3.31171276e-03, -6.33148616e-03, ...,\n",
      "        -3.60199600e-03,  5.98243391e-03, -4.54733288e-03],\n",
      "       ...,\n",
      "       [-1.15284565e-04,  1.01704868e-02,  7.13728368e-05, ...,\n",
      "         1.63313504e-02,  8.45513027e-03, -1.05834287e-02],\n",
      "       [ 7.51028303e-03,  2.15990865e-03,  7.19672255e-03, ...,\n",
      "         4.51383833e-03, -5.73619828e-03,  5.78765245e-03],\n",
      "       [-1.20154815e-02,  2.52695680e-02,  2.08671018e-03, ...,\n",
      "        -1.13718761e-02, -2.88757589e-03,  6.90241344e-03]], dtype=float32)>, <tf.Variable 'actor/dense/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[-0.00803269, -0.00755734,  0.00281461, ..., -0.00289996,\n",
      "        -0.01399054, -0.01239738],\n",
      "       [-0.00997126, -0.00390268,  0.01072986, ...,  0.01263903,\n",
      "         0.00274303, -0.00694317],\n",
      "       [ 0.00753926,  0.0057579 , -0.00174976, ..., -0.00258169,\n",
      "        -0.00444449,  0.00500562],\n",
      "       ...,\n",
      "       [ 0.01215971, -0.01015533,  0.01338909, ..., -0.01340112,\n",
      "        -0.00911181, -0.00957868],\n",
      "       [ 0.01117375,  0.01645911, -0.00015771, ...,  0.00299049,\n",
      "         0.01466596,  0.00439953],\n",
      "       [ 0.00669039,  0.00507154, -0.00819705, ...,  0.01069654,\n",
      "         0.01282239, -0.00716095]], dtype=float32)>, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.01361657,  0.01444342,  0.01622957, ..., -0.00071591,\n",
      "         0.0030895 ,  0.01670394],\n",
      "       [-0.00423875, -0.00315748, -0.01855959, ..., -0.01288712,\n",
      "        -0.01097716,  0.0046248 ],\n",
      "       [-0.00877126, -0.01433462,  0.00185777, ...,  0.0126122 ,\n",
      "         0.00855011,  0.00976727],\n",
      "       ...,\n",
      "       [-0.01583707,  0.00445471, -0.01278754, ...,  0.0120641 ,\n",
      "         0.00279803,  0.02242054],\n",
      "       [ 0.01717794, -0.00272553,  0.00648627, ..., -0.01912042,\n",
      "        -0.00965603,  0.00915352],\n",
      "       [ 0.0120354 , -0.01243899,  0.01101775, ..., -0.00071088,\n",
      "         0.00749606,  0.02501313]], dtype=float32)>, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\n",
      "array([[-0.08866136, -0.27883843,  0.21831375,  0.07456532],\n",
      "       [-0.26057103, -0.17297573,  0.08374435,  0.01829883],\n",
      "       [-0.06901233,  0.13466766, -0.11715324,  0.03807408],\n",
      "       [ 0.22266573,  0.25129378,  0.20198986,  0.07525569],\n",
      "       [-0.15978846,  0.18887389, -0.15133564, -0.04595765],\n",
      "       [-0.21875896, -0.1221551 , -0.19874263,  0.08657539],\n",
      "       [-0.16946946,  0.27828848, -0.18124323,  0.04722887],\n",
      "       [ 0.14674142,  0.20180514, -0.03865913,  0.06646922],\n",
      "       [ 0.15452138, -0.09726964,  0.15949994, -0.14067788],\n",
      "       [-0.09241727, -0.19736305, -0.29537135, -0.09118237],\n",
      "       [-0.03176451, -0.00958958, -0.04649106,  0.27230364],\n",
      "       [ 0.08938798, -0.27243954,  0.2684542 ,  0.09244758],\n",
      "       [ 0.2550072 ,  0.12442201, -0.2113882 ,  0.12940007],\n",
      "       [ 0.22133005, -0.2559955 , -0.04463938,  0.22393292],\n",
      "       [ 0.15863678,  0.17518422, -0.06493729, -0.23822415],\n",
      "       [-0.2185326 ,  0.01482934, -0.11060274,  0.20738167],\n",
      "       [ 0.05545571, -0.18175264, -0.2543781 ,  0.12666413],\n",
      "       [ 0.10834956, -0.02718863, -0.00654158,  0.20151278],\n",
      "       [-0.27479497, -0.0408943 , -0.2140682 , -0.09904158],\n",
      "       [-0.00838038, -0.12688665,  0.02458426, -0.02885687],\n",
      "       [ 0.13567105, -0.1930145 ,  0.05416933, -0.18861426],\n",
      "       [ 0.11851946,  0.19364849,  0.13404053, -0.03026602],\n",
      "       [ 0.09985596, -0.11999407, -0.08788495, -0.24484058],\n",
      "       [-0.26895842, -0.22350276,  0.02136254, -0.16803406],\n",
      "       [ 0.26020503, -0.20071727, -0.16845702,  0.03397307],\n",
      "       [ 0.01317644, -0.18572676,  0.22592765,  0.17076188],\n",
      "       [ 0.02814704,  0.03195432, -0.01231137,  0.26940155],\n",
      "       [ 0.18318889,  0.0384413 , -0.08995682, -0.07394534],\n",
      "       [ 0.02738586, -0.1223301 , -0.20211464,  0.19569898],\n",
      "       [ 0.01859918,  0.02911851, -0.19670023,  0.2623127 ],\n",
      "       [-0.1855167 , -0.15258493,  0.15487137,  0.13096103],\n",
      "       [ 0.20135975, -0.1783713 , -0.12350133,  0.03085396],\n",
      "       [-0.19057508, -0.04230607, -0.10429387, -0.22345665],\n",
      "       [ 0.13837168, -0.15376408, -0.04975931, -0.18469313],\n",
      "       [ 0.2065857 ,  0.00887653, -0.27666962,  0.21269178],\n",
      "       [-0.24184854, -0.28785002,  0.2182746 ,  0.01856238],\n",
      "       [-0.05308783, -0.179622  ,  0.20419061,  0.07292292],\n",
      "       [ 0.2868412 , -0.21973756,  0.20540375, -0.26088315],\n",
      "       [-0.19169228,  0.07066464, -0.25452477,  0.00403324],\n",
      "       [-0.2234359 , -0.21529949, -0.24808659,  0.24839175],\n",
      "       [-0.21233585, -0.08565757,  0.07344678, -0.05300538],\n",
      "       [-0.13152266, -0.01565787,  0.10239437,  0.23843557],\n",
      "       [ 0.1669069 ,  0.29488516,  0.09042558, -0.24668255],\n",
      "       [-0.14868708, -0.10315025, -0.13161147,  0.15553722],\n",
      "       [ 0.24543685,  0.1424518 , -0.20946187, -0.06327555],\n",
      "       [ 0.2839226 ,  0.04985267,  0.08883417, -0.13403219],\n",
      "       [-0.04393783,  0.15881652,  0.23823214,  0.09107232],\n",
      "       [ 0.17175883, -0.02798551, -0.11835514,  0.19840065],\n",
      "       [ 0.13396782, -0.01433223,  0.20598859, -0.1043479 ],\n",
      "       [-0.00483274,  0.2256707 ,  0.18786329, -0.13234687],\n",
      "       [ 0.2934358 , -0.25038671, -0.09731893, -0.11705486],\n",
      "       [-0.1717652 , -0.11204414, -0.09522483,  0.10637039],\n",
      "       [-0.21854812, -0.25519416, -0.13856904, -0.07926817],\n",
      "       [-0.05986524,  0.08628008,  0.11967629,  0.15938154],\n",
      "       [ 0.12173775,  0.07679802, -0.1500695 ,  0.155388  ],\n",
      "       [ 0.21525037, -0.23135445, -0.07041359,  0.18416685],\n",
      "       [-0.1577814 , -0.22918323,  0.27919704,  0.00370073],\n",
      "       [ 0.22809613, -0.05775201,  0.21309894,  0.10020515],\n",
      "       [ 0.03015816, -0.11456785,  0.25603408,  0.25676334],\n",
      "       [-0.10822882,  0.24791408, -0.19739032, -0.27287233],\n",
      "       [-0.2615825 ,  0.17910641,  0.27268726, -0.10725538],\n",
      "       [-0.18997473, -0.08769579,  0.14024225,  0.08762652],\n",
      "       [ 0.13527787,  0.03279263,  0.2514047 ,  0.20941943],\n",
      "       [-0.29669392,  0.08612865,  0.26297712,  0.21619815]],\n",
      "      dtype=float32)>, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "critic\n",
      "[<tf.Variable 'critic/dense_4/kernel:0' shape=(8, 128) dtype=float32, numpy=\n",
      "array([[-0.01457976, -0.00228016,  0.01595409, ...,  0.01109221,\n",
      "        -0.00324348, -0.00295719],\n",
      "       [ 0.00468628, -0.00726179,  0.00044305, ..., -0.0053866 ,\n",
      "        -0.00107698,  0.01851011],\n",
      "       [-0.0147559 ,  0.00515697, -0.0098548 , ...,  0.00805167,\n",
      "         0.00164142,  0.00905266],\n",
      "       ...,\n",
      "       [ 0.00348952, -0.01057769, -0.01889451, ...,  0.01285833,\n",
      "        -0.01360315, -0.0239802 ],\n",
      "       [-0.00249975,  0.01684629, -0.01185575, ...,  0.00717566,\n",
      "         0.00035797,  0.00508172],\n",
      "       [-0.00635258,  0.0059159 ,  0.01017047, ..., -0.00357948,\n",
      "        -0.01670804, -0.00199479]], dtype=float32)>, <tf.Variable 'critic/dense_4/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_5/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[-0.00965523,  0.00081601,  0.00485021, ..., -0.0003093 ,\n",
      "         0.01007187, -0.00996963],\n",
      "       [-0.0114502 , -0.01212904, -0.01586909, ..., -0.01155885,\n",
      "         0.00683022,  0.006248  ],\n",
      "       [ 0.00106149,  0.00727051,  0.00338212, ..., -0.00265902,\n",
      "         0.00487504, -0.0025949 ],\n",
      "       ...,\n",
      "       [-0.00129608,  0.00443275, -0.00622101, ...,  0.01318814,\n",
      "        -0.01354124,  0.01459943],\n",
      "       [ 0.00351176,  0.00614913, -0.00385356, ..., -0.00401799,\n",
      "        -0.00203609, -0.00914995],\n",
      "       [ 0.00536312,  0.00310635,  0.00637255, ...,  0.00968645,\n",
      "         0.00556902,  0.00280026]], dtype=float32)>, <tf.Variable 'critic/dense_5/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_6/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.00194803, -0.01167338,  0.0027778 , ..., -0.00699964,\n",
      "        -0.00980446, -0.00518172],\n",
      "       [-0.01929231,  0.00192873, -0.00236816, ..., -0.00386949,\n",
      "        -0.0043223 ,  0.01056938],\n",
      "       [ 0.02712622,  0.01588193, -0.00473268, ..., -0.00095836,\n",
      "        -0.01434881, -0.004838  ],\n",
      "       ...,\n",
      "       [-0.00456963,  0.00743464,  0.00408886, ..., -0.00880102,\n",
      "         0.01009604, -0.00670597],\n",
      "       [-0.01389211, -0.01370775,  0.00107861, ..., -0.0081292 ,\n",
      "         0.00413241,  0.00086359],\n",
      "       [ 0.01220275, -0.00747103,  0.02130279, ..., -0.00189252,\n",
      "         0.00143701,  0.0052554 ]], dtype=float32)>, <tf.Variable 'critic/dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_7/kernel:0' shape=(64, 1) dtype=float32, numpy=\n",
      "array([[ 0.28500998],\n",
      "       [ 0.2173332 ],\n",
      "       [ 0.05558652],\n",
      "       [-0.08461715],\n",
      "       [ 0.16749668],\n",
      "       [-0.15596801],\n",
      "       [-0.10909027],\n",
      "       [-0.09681992],\n",
      "       [-0.1935703 ],\n",
      "       [-0.1616109 ],\n",
      "       [ 0.13417512],\n",
      "       [ 0.08387503],\n",
      "       [-0.04080936],\n",
      "       [ 0.21902514],\n",
      "       [ 0.01312837],\n",
      "       [-0.2531189 ],\n",
      "       [-0.05994707],\n",
      "       [-0.20211676],\n",
      "       [ 0.24667096],\n",
      "       [ 0.29025978],\n",
      "       [-0.03689095],\n",
      "       [ 0.05229327],\n",
      "       [ 0.18030033],\n",
      "       [-0.17627907],\n",
      "       [-0.24857754],\n",
      "       [-0.00808111],\n",
      "       [ 0.02134326],\n",
      "       [-0.06904766],\n",
      "       [ 0.14928427],\n",
      "       [-0.00270987],\n",
      "       [ 0.22735852],\n",
      "       [-0.18763143],\n",
      "       [ 0.07364994],\n",
      "       [ 0.1605855 ],\n",
      "       [ 0.06262949],\n",
      "       [-0.29902577],\n",
      "       [ 0.24346411],\n",
      "       [ 0.07987767],\n",
      "       [ 0.06566551],\n",
      "       [-0.09352484],\n",
      "       [-0.1767147 ],\n",
      "       [-0.30262   ],\n",
      "       [ 0.18843684],\n",
      "       [-0.11760767],\n",
      "       [-0.14546736],\n",
      "       [-0.13190973],\n",
      "       [-0.21497774],\n",
      "       [ 0.26706505],\n",
      "       [ 0.08567971],\n",
      "       [-0.25979728],\n",
      "       [ 0.18249696],\n",
      "       [ 0.10548654],\n",
      "       [-0.2761642 ],\n",
      "       [-0.01569995],\n",
      "       [ 0.05453524],\n",
      "       [ 0.02416179],\n",
      "       [ 0.06385815],\n",
      "       [ 0.06132838],\n",
      "       [ 0.05404621],\n",
      "       [-0.06681104],\n",
      "       [ 0.17381072],\n",
      "       [-0.20072365],\n",
      "       [-0.13495158],\n",
      "       [ 0.05725995]], dtype=float32)>, <tf.Variable 'critic/dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "[None, None, None, None, None, None, None, None]\n",
      "[<tf.Tensor: shape=(8, 128), dtype=float32, numpy=\n",
      "array([[-1.89452180e-07,  0.00000000e+00,  8.59248246e-07, ...,\n",
      "        -9.00331365e-09,  1.12949451e-07, -8.10458118e-07],\n",
      "       [-2.22423259e-05,  0.00000000e+00,  1.36343966e-04, ...,\n",
      "        -1.45622344e-05,  3.73950788e-05, -1.09221866e-04],\n",
      "       [-3.11148881e-07,  0.00000000e+00,  3.47630316e-06, ...,\n",
      "        -4.41589123e-07,  1.49114749e-06, -2.02461865e-06],\n",
      "       ...,\n",
      "       [-1.84500976e-07,  0.00000000e+00, -1.61887590e-06, ...,\n",
      "         1.88267109e-08, -1.26594284e-06, -6.45072021e-07],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
      "array([-1.47236769e-05,  0.00000000e+00,  9.06726273e-05,  1.86673191e-04,\n",
      "        8.02650684e-05,  2.06195164e-05, -1.03272061e-04,  0.00000000e+00,\n",
      "        0.00000000e+00, -3.70315029e-05,  0.00000000e+00, -0.00000000e+00,\n",
      "       -2.62841604e-05,  1.14470022e-04, -8.80622756e-05,  0.00000000e+00,\n",
      "        3.97653457e-05, -0.00000000e+00,  0.00000000e+00,  5.51671037e-05,\n",
      "       -9.32578841e-05, -0.00000000e+00,  7.67308211e-06,  1.15683426e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.63519102e-05,\n",
      "        4.43834433e-05,  0.00000000e+00, -6.00131789e-05, -7.44533900e-05,\n",
      "       -6.90017914e-05,  0.00000000e+00, -2.11683764e-05, -0.00000000e+00,\n",
      "        9.14848169e-06, -0.00000000e+00,  3.80250058e-05, -0.00000000e+00,\n",
      "       -8.83473092e-07,  0.00000000e+00,  1.13661343e-04,  0.00000000e+00,\n",
      "        0.00000000e+00, -8.06175085e-05,  3.05936192e-05,  8.83848697e-05,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
      "       -7.44047575e-05,  0.00000000e+00,  0.00000000e+00, -2.34878753e-05,\n",
      "        1.86064834e-04,  0.00000000e+00,  9.12206306e-05,  1.48928730e-05,\n",
      "       -1.49032048e-05,  0.00000000e+00,  4.67455138e-05, -3.85323392e-06,\n",
      "       -7.26454164e-05,  0.00000000e+00, -0.00000000e+00, -1.58605690e-05,\n",
      "        1.10196532e-04,  1.48190185e-04, -0.00000000e+00,  1.24536673e-04,\n",
      "        0.00000000e+00,  4.74506087e-05, -1.48324239e-06,  4.38670359e-05,\n",
      "        0.00000000e+00,  6.26752490e-06,  0.00000000e+00,  1.87486585e-05,\n",
      "        0.00000000e+00, -0.00000000e+00,  1.08269614e-05,  0.00000000e+00,\n",
      "       -0.00000000e+00,  4.10809043e-05, -0.00000000e+00,  1.56544556e-05,\n",
      "       -6.13890006e-05,  0.00000000e+00, -1.32183704e-04, -3.83194856e-05,\n",
      "       -6.12233125e-05,  1.94308632e-05, -3.31786123e-06,  0.00000000e+00,\n",
      "       -4.80024901e-05, -0.00000000e+00, -8.07048273e-05,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00, -5.10035206e-05,  0.00000000e+00,\n",
      "        3.46030470e-06, -6.40701619e-05,  0.00000000e+00, -1.03089278e-05,\n",
      "        1.13051319e-04, -0.00000000e+00,  5.35996478e-05,  1.52732136e-05,\n",
      "        0.00000000e+00,  0.00000000e+00, -3.45096632e-05,  0.00000000e+00,\n",
      "       -2.86259801e-05, -1.36117460e-05, -1.20645767e-04, -4.63457181e-05,\n",
      "        4.35395923e-05, -6.54713731e-05, -0.00000000e+00, -0.00000000e+00,\n",
      "        0.00000000e+00, -1.01669448e-05,  2.52116261e-05, -7.23403209e-05],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 1.7467928e-06,  0.0000000e+00, -7.3500855e-06, ...,\n",
      "        -8.5146439e-07, -1.7483570e-06, -1.6602131e-07],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 3.9843550e-07,  0.0000000e+00, -8.3994820e-07, ...,\n",
      "        -1.5185458e-07, -2.6042892e-07, -1.1156618e-07],\n",
      "       ...,\n",
      "       [ 1.7244291e-07,  0.0000000e+00, -6.6809406e-08, ...,\n",
      "        -2.8690641e-08, -5.5807693e-08, -9.3479109e-09],\n",
      "       [ 3.8293547e-07,  0.0000000e+00, -4.0939767e-07, ...,\n",
      "        -9.4347918e-08, -1.8524332e-07, -5.8300074e-08],\n",
      "       [ 4.2564179e-06,  0.0000000e+00, -3.0688356e-05, ...,\n",
      "        -2.2828367e-06, -6.3893231e-06,  6.3638174e-07]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
      "array([ 1.96465378e-04,  0.00000000e+00, -1.10235007e-03,  5.00881695e-04,\n",
      "       -0.00000000e+00, -1.17823447e-03, -0.00000000e+00, -9.86382831e-04,\n",
      "        0.00000000e+00,  1.35356186e-05,  0.00000000e+00,  2.32721423e-03,\n",
      "       -0.00000000e+00,  2.52933416e-04, -0.00000000e+00, -0.00000000e+00,\n",
      "        9.60765523e-04, -1.81445954e-04,  0.00000000e+00, -0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  7.33645924e-04,  0.00000000e+00,\n",
      "       -7.48246908e-04,  0.00000000e+00, -6.20725448e-04,  1.21285685e-03,\n",
      "        0.00000000e+00,  7.04174396e-04, -7.52050983e-05,  1.07556094e-04,\n",
      "        0.00000000e+00,  1.43645750e-03, -6.31026051e-04, -6.21239233e-05,\n",
      "        2.68848933e-04, -6.24239270e-04,  2.78690830e-04,  2.05606082e-03,\n",
      "       -0.00000000e+00, -0.00000000e+00, -7.19511416e-04, -0.00000000e+00,\n",
      "       -0.00000000e+00,  8.32560356e-04,  0.00000000e+00, -3.65280255e-04,\n",
      "        9.11411422e-04, -8.14042869e-04, -0.00000000e+00,  1.87044172e-03,\n",
      "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  2.78791966e-04,\n",
      "        0.00000000e+00, -0.00000000e+00, -4.06198029e-04,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00, -1.33257723e-04,  1.25168426e-05,\n",
      "        7.55882298e-04,  3.89293244e-04, -0.00000000e+00,  0.00000000e+00,\n",
      "        6.19952858e-04, -2.17691151e-04,  1.19488419e-03,  3.46377754e-04,\n",
      "       -6.04545814e-04, -1.32299354e-03,  2.55759980e-04,  0.00000000e+00,\n",
      "       -0.00000000e+00, -2.37256725e-04,  1.14735514e-04,  0.00000000e+00,\n",
      "        5.01018403e-05,  1.33624853e-04, -1.10332483e-04, -1.89560786e-04,\n",
      "       -3.95569630e-04, -0.00000000e+00, -1.17180287e-03,  0.00000000e+00,\n",
      "       -0.00000000e+00, -6.57338416e-04, -1.86653597e-06,  6.26556750e-04,\n",
      "        1.69724564e-03, -1.59318524e-03, -8.77995393e-04, -4.87421872e-04,\n",
      "        0.00000000e+00,  1.78559567e-04,  2.65468581e-04, -0.00000000e+00,\n",
      "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -1.33019325e-03,\n",
      "        9.99947661e-04,  0.00000000e+00, -8.41836794e-04,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.96449822e-04,  1.26290732e-04,\n",
      "       -0.00000000e+00,  0.00000000e+00, -7.41143827e-04,  0.00000000e+00,\n",
      "        3.96808435e-04,  2.24295305e-04, -0.00000000e+00,  5.39439905e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.02191916e-04,  1.66327995e-03,\n",
      "        2.12402985e-04, -9.76198571e-05, -2.42735638e-04,  7.90170816e-06],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(128, 64), dtype=float32, numpy=\n",
      "array([[-8.9149339e-08,  0.0000000e+00, -1.7387114e-08, ...,\n",
      "         0.0000000e+00,  0.0000000e+00, -1.7910553e-08],\n",
      "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [-2.8439334e-05,  0.0000000e+00, -1.9423037e-06, ...,\n",
      "         0.0000000e+00,  8.5656247e-06, -3.6434503e-07],\n",
      "       ...,\n",
      "       [-2.6042107e-05,  0.0000000e+00, -1.4568824e-06, ...,\n",
      "         0.0000000e+00,  8.6102145e-06, -1.9271030e-07],\n",
      "       [-1.2503584e-05,  0.0000000e+00, -1.2450250e-06, ...,\n",
      "         0.0000000e+00,  3.4849782e-06, -2.3377629e-07],\n",
      "       [-3.6085374e-05,  0.0000000e+00, -2.3630027e-06, ...,\n",
      "         0.0000000e+00,  1.1204030e-05, -3.5154440e-07]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
      "array([-0.02789001, -0.        , -0.0021969 ,  0.00541382, -0.0163906 ,\n",
      "        0.01174004,  0.01067517,  0.00947443,  0.01894206,  0.        ,\n",
      "       -0.        , -0.        ,  0.00399345, -0.01978435, -0.        ,\n",
      "        0.02476927,  0.0058662 ,  0.01977839, -0.0241383 , -0.        ,\n",
      "        0.00361001, -0.00511722, -0.        ,  0.00696573,  0.        ,\n",
      "        0.00079079, -0.00184756,  0.        , -0.01292283,  0.00026518,\n",
      "       -0.        ,  0.        , -0.00415815, -0.        , -0.00153231,\n",
      "        0.02926154, -0.        , -0.        , -0.00642578,  0.        ,\n",
      "        0.01729264,  0.        , -0.01843972,  0.01150865,  0.01423489,\n",
      "        0.01290819,  0.00080906, -0.02613398, -0.        ,  0.02542279,\n",
      "       -0.01785847, -0.01032252,  0.02702439,  0.00153634, -0.00533661,\n",
      "       -0.00236438, -0.        , -0.00600137, -0.00122034,  0.00653788,\n",
      "       -0.01700847,  0.        ,  0.00812666, -0.000431  ], dtype=float32)>, <tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n",
      "array([[-1.50259552e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-4.33481461e-07],\n",
      "       [-7.08245125e-07],\n",
      "       [-9.85516090e-06],\n",
      "       [-9.64227866e-07],\n",
      "       [-7.94403240e-06],\n",
      "       [-1.20356672e-05],\n",
      "       [-2.79618575e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.19996448e-05],\n",
      "       [-1.94656036e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.31181496e-05],\n",
      "       [-7.07328718e-06],\n",
      "       [-1.36040971e-05],\n",
      "       [-4.69447605e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.10523115e-05],\n",
      "       [-8.70802069e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-2.54808015e-07],\n",
      "       [ 0.00000000e+00],\n",
      "       [-9.48303204e-06],\n",
      "       [-5.12271072e-07],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.94383438e-06],\n",
      "       [-7.20852449e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 0.00000000e+00],\n",
      "       [-2.31820053e-07],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.19160646e-07],\n",
      "       [-7.19135824e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 0.00000000e+00],\n",
      "       [-2.32116713e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-4.56430871e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-1.38250161e-05],\n",
      "       [-9.58824603e-06],\n",
      "       [-5.23523886e-06],\n",
      "       [-4.96193024e-06],\n",
      "       [-8.12177881e-09],\n",
      "       [-1.24608578e-05],\n",
      "       [ 0.00000000e+00],\n",
      "       [-8.79208983e-06],\n",
      "       [-6.86588055e-06],\n",
      "       [-1.27878075e-05],\n",
      "       [-1.32745918e-05],\n",
      "       [-1.18996991e-06],\n",
      "       [-1.26616542e-05],\n",
      "       [-6.57148303e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-7.78178389e-07],\n",
      "       [-2.18527120e-07],\n",
      "       [-3.25812834e-06],\n",
      "       [-6.42401210e-06],\n",
      "       [ 0.00000000e+00],\n",
      "       [-7.35097274e-07],\n",
      "       [-1.64069203e-08]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.09785625], dtype=float32)>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'actor/dense/kernel:0' shape=(8, 128) dtype=float32, numpy=\narray([[-1.63800921e-02, -1.56523436e-02, -7.40791345e-03, ...,\n        -1.41428283e-03,  9.91461449e-04,  3.09268874e-03],\n       [-2.78602242e-02, -1.35148009e-02, -6.33828156e-03, ...,\n        -1.27423191e-02,  5.50283771e-03,  3.61146708e-03],\n       [-2.23480235e-03,  3.31171276e-03, -6.33148616e-03, ...,\n        -3.60199600e-03,  5.98243391e-03, -4.54733288e-03],\n       ...,\n       [-1.15284565e-04,  1.01704868e-02,  7.13728368e-05, ...,\n         1.63313504e-02,  8.45513027e-03, -1.05834287e-02],\n       [ 7.51028303e-03,  2.15990865e-03,  7.19672255e-03, ...,\n         4.51383833e-03, -5.73619828e-03,  5.78765245e-03],\n       [-1.20154815e-02,  2.52695680e-02,  2.08671018e-03, ...,\n        -1.13718761e-02, -2.88757589e-03,  6.90241344e-03]], dtype=float32)>), (None, <tf.Variable 'actor/dense/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\narray([[-0.00803269, -0.00755734,  0.00281461, ..., -0.00289996,\n        -0.01399054, -0.01239738],\n       [-0.00997126, -0.00390268,  0.01072986, ...,  0.01263903,\n         0.00274303, -0.00694317],\n       [ 0.00753926,  0.0057579 , -0.00174976, ..., -0.00258169,\n        -0.00444449,  0.00500562],\n       ...,\n       [ 0.01215971, -0.01015533,  0.01338909, ..., -0.01340112,\n        -0.00911181, -0.00957868],\n       [ 0.01117375,  0.01645911, -0.00015771, ...,  0.00299049,\n         0.01466596,  0.00439953],\n       [ 0.00669039,  0.00507154, -0.00819705, ...,  0.01069654,\n         0.01282239, -0.00716095]], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[ 0.01361657,  0.01444342,  0.01622957, ..., -0.00071591,\n         0.0030895 ,  0.01670394],\n       [-0.00423875, -0.00315748, -0.01855959, ..., -0.01288712,\n        -0.01097716,  0.0046248 ],\n       [-0.00877126, -0.01433462,  0.00185777, ...,  0.0126122 ,\n         0.00855011,  0.00976727],\n       ...,\n       [-0.01583707,  0.00445471, -0.01278754, ...,  0.0120641 ,\n         0.00279803,  0.02242054],\n       [ 0.01717794, -0.00272553,  0.00648627, ..., -0.01912042,\n        -0.00965603,  0.00915352],\n       [ 0.0120354 , -0.01243899,  0.01101775, ..., -0.00071088,\n         0.00749606,  0.02501313]], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\narray([[-0.08866136, -0.27883843,  0.21831375,  0.07456532],\n       [-0.26057103, -0.17297573,  0.08374435,  0.01829883],\n       [-0.06901233,  0.13466766, -0.11715324,  0.03807408],\n       [ 0.22266573,  0.25129378,  0.20198986,  0.07525569],\n       [-0.15978846,  0.18887389, -0.15133564, -0.04595765],\n       [-0.21875896, -0.1221551 , -0.19874263,  0.08657539],\n       [-0.16946946,  0.27828848, -0.18124323,  0.04722887],\n       [ 0.14674142,  0.20180514, -0.03865913,  0.06646922],\n       [ 0.15452138, -0.09726964,  0.15949994, -0.14067788],\n       [-0.09241727, -0.19736305, -0.29537135, -0.09118237],\n       [-0.03176451, -0.00958958, -0.04649106,  0.27230364],\n       [ 0.08938798, -0.27243954,  0.2684542 ,  0.09244758],\n       [ 0.2550072 ,  0.12442201, -0.2113882 ,  0.12940007],\n       [ 0.22133005, -0.2559955 , -0.04463938,  0.22393292],\n       [ 0.15863678,  0.17518422, -0.06493729, -0.23822415],\n       [-0.2185326 ,  0.01482934, -0.11060274,  0.20738167],\n       [ 0.05545571, -0.18175264, -0.2543781 ,  0.12666413],\n       [ 0.10834956, -0.02718863, -0.00654158,  0.20151278],\n       [-0.27479497, -0.0408943 , -0.2140682 , -0.09904158],\n       [-0.00838038, -0.12688665,  0.02458426, -0.02885687],\n       [ 0.13567105, -0.1930145 ,  0.05416933, -0.18861426],\n       [ 0.11851946,  0.19364849,  0.13404053, -0.03026602],\n       [ 0.09985596, -0.11999407, -0.08788495, -0.24484058],\n       [-0.26895842, -0.22350276,  0.02136254, -0.16803406],\n       [ 0.26020503, -0.20071727, -0.16845702,  0.03397307],\n       [ 0.01317644, -0.18572676,  0.22592765,  0.17076188],\n       [ 0.02814704,  0.03195432, -0.01231137,  0.26940155],\n       [ 0.18318889,  0.0384413 , -0.08995682, -0.07394534],\n       [ 0.02738586, -0.1223301 , -0.20211464,  0.19569898],\n       [ 0.01859918,  0.02911851, -0.19670023,  0.2623127 ],\n       [-0.1855167 , -0.15258493,  0.15487137,  0.13096103],\n       [ 0.20135975, -0.1783713 , -0.12350133,  0.03085396],\n       [-0.19057508, -0.04230607, -0.10429387, -0.22345665],\n       [ 0.13837168, -0.15376408, -0.04975931, -0.18469313],\n       [ 0.2065857 ,  0.00887653, -0.27666962,  0.21269178],\n       [-0.24184854, -0.28785002,  0.2182746 ,  0.01856238],\n       [-0.05308783, -0.179622  ,  0.20419061,  0.07292292],\n       [ 0.2868412 , -0.21973756,  0.20540375, -0.26088315],\n       [-0.19169228,  0.07066464, -0.25452477,  0.00403324],\n       [-0.2234359 , -0.21529949, -0.24808659,  0.24839175],\n       [-0.21233585, -0.08565757,  0.07344678, -0.05300538],\n       [-0.13152266, -0.01565787,  0.10239437,  0.23843557],\n       [ 0.1669069 ,  0.29488516,  0.09042558, -0.24668255],\n       [-0.14868708, -0.10315025, -0.13161147,  0.15553722],\n       [ 0.24543685,  0.1424518 , -0.20946187, -0.06327555],\n       [ 0.2839226 ,  0.04985267,  0.08883417, -0.13403219],\n       [-0.04393783,  0.15881652,  0.23823214,  0.09107232],\n       [ 0.17175883, -0.02798551, -0.11835514,  0.19840065],\n       [ 0.13396782, -0.01433223,  0.20598859, -0.1043479 ],\n       [-0.00483274,  0.2256707 ,  0.18786329, -0.13234687],\n       [ 0.2934358 , -0.25038671, -0.09731893, -0.11705486],\n       [-0.1717652 , -0.11204414, -0.09522483,  0.10637039],\n       [-0.21854812, -0.25519416, -0.13856904, -0.07926817],\n       [-0.05986524,  0.08628008,  0.11967629,  0.15938154],\n       [ 0.12173775,  0.07679802, -0.1500695 ,  0.155388  ],\n       [ 0.21525037, -0.23135445, -0.07041359,  0.18416685],\n       [-0.1577814 , -0.22918323,  0.27919704,  0.00370073],\n       [ 0.22809613, -0.05775201,  0.21309894,  0.10020515],\n       [ 0.03015816, -0.11456785,  0.25603408,  0.25676334],\n       [-0.10822882,  0.24791408, -0.19739032, -0.27287233],\n       [-0.2615825 ,  0.17910641,  0.27268726, -0.10725538],\n       [-0.18997473, -0.08769579,  0.14024225,  0.08762652],\n       [ 0.13527787,  0.03279263,  0.2514047 ,  0.20941943],\n       [-0.29669392,  0.08612865,  0.26297712,  0.21619815]],\n      dtype=float32)>), (None, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mh:\\FPSSample-master\\bchlr\\IANNWTF-Final\\CustomPPO_loss_function_commented.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000011?line=0'>1</a>\u001b[0m ppo_agent \u001b[39m=\u001b[39m Agent()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000011?line=1'>2</a>\u001b[0m ppo_agent\u001b[39m.\u001b[39;49mrun()\n",
      "\u001b[1;32mh:\\FPSSample-master\\bchlr\\IANNWTF-Final\\CustomPPO_loss_function_commented.ipynb Cell 11'\u001b[0m in \u001b[0;36mAgent.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=368'>369</a>\u001b[0m data, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mget_episodes()\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=369'>370</a>\u001b[0m \u001b[39m#print(data)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=370'>371</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_policy(data, optimizer, clip_ratio)\n",
      "\u001b[1;32mh:\\FPSSample-master\\bchlr\\IANNWTF-Final\\CustomPPO_loss_function_commented.ipynb Cell 11'\u001b[0m in \u001b[0;36mAgent.update_policy\u001b[1;34m(self, episodes, optimizer, clip_param, c_1, c_2)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=243'>244</a>\u001b[0m \u001b[39m# Iterate over all finished episodes from collected training data\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=244'>245</a>\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m tqdm_notebook(episodes):\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=245'>246</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=246'>247</a>\u001b[0m     \u001b[39m# Update parameters\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=247'>248</a>\u001b[0m     \u001b[39m# Compute train losses and action by chosen by policy\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=248'>249</a>\u001b[0m     actor_loss, critic_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=249'>250</a>\u001b[0m         \u001b[39m# States\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=250'>251</a>\u001b[0m         episode[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=251'>252</a>\u001b[0m         \u001b[39m# Actions\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=252'>253</a>\u001b[0m         episode[\u001b[39m1\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=253'>254</a>\u001b[0m         \u001b[39m#optimizer (Adam)\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=254'>255</a>\u001b[0m         optimizer,\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=255'>256</a>\u001b[0m         \u001b[39m# Logits\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=256'>257</a>\u001b[0m         episode[\u001b[39m2\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=257'>258</a>\u001b[0m         \u001b[39m# Rewards\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=258'>259</a>\u001b[0m         episode[\u001b[39m3\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=259'>260</a>\u001b[0m         clip_param,\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=260'>261</a>\u001b[0m         c_1,\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=261'>262</a>\u001b[0m         c_2 \n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=262'>263</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=263'>264</a>\u001b[0m     train_losses_actor\u001b[39m.\u001b[39mappend(actor_loss)\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=264'>265</a>\u001b[0m     train_losses_critic\u001b[39m.\u001b[39mappend(critic_loss)\n",
      "\u001b[1;32mh:\\FPSSample-master\\bchlr\\IANNWTF-Final\\CustomPPO_loss_function_commented.ipynb Cell 11'\u001b[0m in \u001b[0;36mAgent.train_step\u001b[1;34m(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=199'>200</a>\u001b[0m     \u001b[39mprint\u001b[39m(c_gradients)\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=201'>202</a>\u001b[0m     \u001b[39m#print(tape)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=202'>203</a>\u001b[0m     \u001b[39m#print('Actor loss')\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=203'>204</a>\u001b[0m     \u001b[39m#print(actor_loss)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=208'>209</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=209'>210</a>\u001b[0m \u001b[39m# Update parameters\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=210'>211</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(a_gradients, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mtrainable_variables))\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=211'>212</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(c_gradients, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mtrainable_variables))\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CustomPPO_loss_function_commented.ipynb#ch0000010?line=215'>216</a>\u001b[0m \u001b[39mreturn\u001b[39;00m actor_loss, critic_loss\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_gym\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:633\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=591'>592</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_gradients\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=592'>593</a>\u001b[0m                     grads_and_vars,\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=593'>594</a>\u001b[0m                     name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=594'>595</a>\u001b[0m                     experimental_aggregate_gradients\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=595'>596</a>\u001b[0m   \u001b[39m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=596'>597</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=597'>598</a>\u001b[0m \u001b[39m  This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=630'>631</a>\u001b[0m \u001b[39m    RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=631'>632</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=632'>633</a>\u001b[0m   grads_and_vars \u001b[39m=\u001b[39m optimizer_utils\u001b[39m.\u001b[39;49mfilter_empty_gradients(grads_and_vars)\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=633'>634</a>\u001b[0m   var_list \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m (_, v) \u001b[39min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=635'>636</a>\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name):\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=636'>637</a>\u001b[0m     \u001b[39m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_gym\\lib\\site-packages\\keras\\optimizer_v2\\utils.py:73\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=70'>71</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=71'>72</a>\u001b[0m   variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m---> <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=72'>73</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=73'>74</a>\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=75'>76</a>\u001b[0m   logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=76'>77</a>\u001b[0m       (\u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the loss. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=77'>78</a>\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mIf you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using `model.compile()`, did you forget to provide a `loss`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=78'>79</a>\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39margument?\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/keras/optimizer_v2/utils.py?line=79'>80</a>\u001b[0m       ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]))\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'actor/dense/kernel:0' shape=(8, 128) dtype=float32, numpy=\narray([[-1.63800921e-02, -1.56523436e-02, -7.40791345e-03, ...,\n        -1.41428283e-03,  9.91461449e-04,  3.09268874e-03],\n       [-2.78602242e-02, -1.35148009e-02, -6.33828156e-03, ...,\n        -1.27423191e-02,  5.50283771e-03,  3.61146708e-03],\n       [-2.23480235e-03,  3.31171276e-03, -6.33148616e-03, ...,\n        -3.60199600e-03,  5.98243391e-03, -4.54733288e-03],\n       ...,\n       [-1.15284565e-04,  1.01704868e-02,  7.13728368e-05, ...,\n         1.63313504e-02,  8.45513027e-03, -1.05834287e-02],\n       [ 7.51028303e-03,  2.15990865e-03,  7.19672255e-03, ...,\n         4.51383833e-03, -5.73619828e-03,  5.78765245e-03],\n       [-1.20154815e-02,  2.52695680e-02,  2.08671018e-03, ...,\n        -1.13718761e-02, -2.88757589e-03,  6.90241344e-03]], dtype=float32)>), (None, <tf.Variable 'actor/dense/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\narray([[-0.00803269, -0.00755734,  0.00281461, ..., -0.00289996,\n        -0.01399054, -0.01239738],\n       [-0.00997126, -0.00390268,  0.01072986, ...,  0.01263903,\n         0.00274303, -0.00694317],\n       [ 0.00753926,  0.0057579 , -0.00174976, ..., -0.00258169,\n        -0.00444449,  0.00500562],\n       ...,\n       [ 0.01215971, -0.01015533,  0.01338909, ..., -0.01340112,\n        -0.00911181, -0.00957868],\n       [ 0.01117375,  0.01645911, -0.00015771, ...,  0.00299049,\n         0.01466596,  0.00439953],\n       [ 0.00669039,  0.00507154, -0.00819705, ...,  0.01069654,\n         0.01282239, -0.00716095]], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[ 0.01361657,  0.01444342,  0.01622957, ..., -0.00071591,\n         0.0030895 ,  0.01670394],\n       [-0.00423875, -0.00315748, -0.01855959, ..., -0.01288712,\n        -0.01097716,  0.0046248 ],\n       [-0.00877126, -0.01433462,  0.00185777, ...,  0.0126122 ,\n         0.00855011,  0.00976727],\n       ...,\n       [-0.01583707,  0.00445471, -0.01278754, ...,  0.0120641 ,\n         0.00279803,  0.02242054],\n       [ 0.01717794, -0.00272553,  0.00648627, ..., -0.01912042,\n        -0.00965603,  0.00915352],\n       [ 0.0120354 , -0.01243899,  0.01101775, ..., -0.00071088,\n         0.00749606,  0.02501313]], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\narray([[-0.08866136, -0.27883843,  0.21831375,  0.07456532],\n       [-0.26057103, -0.17297573,  0.08374435,  0.01829883],\n       [-0.06901233,  0.13466766, -0.11715324,  0.03807408],\n       [ 0.22266573,  0.25129378,  0.20198986,  0.07525569],\n       [-0.15978846,  0.18887389, -0.15133564, -0.04595765],\n       [-0.21875896, -0.1221551 , -0.19874263,  0.08657539],\n       [-0.16946946,  0.27828848, -0.18124323,  0.04722887],\n       [ 0.14674142,  0.20180514, -0.03865913,  0.06646922],\n       [ 0.15452138, -0.09726964,  0.15949994, -0.14067788],\n       [-0.09241727, -0.19736305, -0.29537135, -0.09118237],\n       [-0.03176451, -0.00958958, -0.04649106,  0.27230364],\n       [ 0.08938798, -0.27243954,  0.2684542 ,  0.09244758],\n       [ 0.2550072 ,  0.12442201, -0.2113882 ,  0.12940007],\n       [ 0.22133005, -0.2559955 , -0.04463938,  0.22393292],\n       [ 0.15863678,  0.17518422, -0.06493729, -0.23822415],\n       [-0.2185326 ,  0.01482934, -0.11060274,  0.20738167],\n       [ 0.05545571, -0.18175264, -0.2543781 ,  0.12666413],\n       [ 0.10834956, -0.02718863, -0.00654158,  0.20151278],\n       [-0.27479497, -0.0408943 , -0.2140682 , -0.09904158],\n       [-0.00838038, -0.12688665,  0.02458426, -0.02885687],\n       [ 0.13567105, -0.1930145 ,  0.05416933, -0.18861426],\n       [ 0.11851946,  0.19364849,  0.13404053, -0.03026602],\n       [ 0.09985596, -0.11999407, -0.08788495, -0.24484058],\n       [-0.26895842, -0.22350276,  0.02136254, -0.16803406],\n       [ 0.26020503, -0.20071727, -0.16845702,  0.03397307],\n       [ 0.01317644, -0.18572676,  0.22592765,  0.17076188],\n       [ 0.02814704,  0.03195432, -0.01231137,  0.26940155],\n       [ 0.18318889,  0.0384413 , -0.08995682, -0.07394534],\n       [ 0.02738586, -0.1223301 , -0.20211464,  0.19569898],\n       [ 0.01859918,  0.02911851, -0.19670023,  0.2623127 ],\n       [-0.1855167 , -0.15258493,  0.15487137,  0.13096103],\n       [ 0.20135975, -0.1783713 , -0.12350133,  0.03085396],\n       [-0.19057508, -0.04230607, -0.10429387, -0.22345665],\n       [ 0.13837168, -0.15376408, -0.04975931, -0.18469313],\n       [ 0.2065857 ,  0.00887653, -0.27666962,  0.21269178],\n       [-0.24184854, -0.28785002,  0.2182746 ,  0.01856238],\n       [-0.05308783, -0.179622  ,  0.20419061,  0.07292292],\n       [ 0.2868412 , -0.21973756,  0.20540375, -0.26088315],\n       [-0.19169228,  0.07066464, -0.25452477,  0.00403324],\n       [-0.2234359 , -0.21529949, -0.24808659,  0.24839175],\n       [-0.21233585, -0.08565757,  0.07344678, -0.05300538],\n       [-0.13152266, -0.01565787,  0.10239437,  0.23843557],\n       [ 0.1669069 ,  0.29488516,  0.09042558, -0.24668255],\n       [-0.14868708, -0.10315025, -0.13161147,  0.15553722],\n       [ 0.24543685,  0.1424518 , -0.20946187, -0.06327555],\n       [ 0.2839226 ,  0.04985267,  0.08883417, -0.13403219],\n       [-0.04393783,  0.15881652,  0.23823214,  0.09107232],\n       [ 0.17175883, -0.02798551, -0.11835514,  0.19840065],\n       [ 0.13396782, -0.01433223,  0.20598859, -0.1043479 ],\n       [-0.00483274,  0.2256707 ,  0.18786329, -0.13234687],\n       [ 0.2934358 , -0.25038671, -0.09731893, -0.11705486],\n       [-0.1717652 , -0.11204414, -0.09522483,  0.10637039],\n       [-0.21854812, -0.25519416, -0.13856904, -0.07926817],\n       [-0.05986524,  0.08628008,  0.11967629,  0.15938154],\n       [ 0.12173775,  0.07679802, -0.1500695 ,  0.155388  ],\n       [ 0.21525037, -0.23135445, -0.07041359,  0.18416685],\n       [-0.1577814 , -0.22918323,  0.27919704,  0.00370073],\n       [ 0.22809613, -0.05775201,  0.21309894,  0.10020515],\n       [ 0.03015816, -0.11456785,  0.25603408,  0.25676334],\n       [-0.10822882,  0.24791408, -0.19739032, -0.27287233],\n       [-0.2615825 ,  0.17910641,  0.27268726, -0.10725538],\n       [-0.18997473, -0.08769579,  0.14024225,  0.08762652],\n       [ 0.13527787,  0.03279263,  0.2514047 ,  0.20941943],\n       [-0.29669392,  0.08612865,  0.26297712,  0.21619815]],\n      dtype=float32)>), (None, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent()\n",
    "ppo_agent.run()\n",
    "# ppo_agent.collect_train_data()\n",
    "# data = storage.get_episodes()\n",
    "# #print(data)\n",
    "# print(ppo_agent.update_policy(data[0], actor, critic, optimizer, clip_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
