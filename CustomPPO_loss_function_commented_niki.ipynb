{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        # Outputs from the actor network, an action is sampled from (Probabilities)\n",
    "        self.logits = []\n",
    "        # Outputs from the crtitics network (Values)\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) \n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collected values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards)])\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, actionspace, struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        self.actionspace = actionspace\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(self.actionspace, activation=\"softmax\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "    #@tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "        #logits = self.call(observation)\n",
    "        logits = self(observation)\n",
    "    # tf.print(type(logits))\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    # tf.print(action)\n",
    "        return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,struct):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(struct[0], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[1], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(struct[2], activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "  \n",
    "    '''\n",
    "\n",
    "    def __init__(self, env_name, render=False, steps_per_epoch=1000, epochs=100, actor_structure=[256,128,64], critic_structure=[256,128,64]):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        \n",
    "        Args:\n",
    "        env_name(): String Name of the Environment Passed\n",
    "        render(): Boolean determining if env should be rendered during training\n",
    "        steps_per_epoch(): how many steps/frame the agent should take during each Epoch of training; Default=1000\n",
    "        epochs(): How many epochs of training should the agent do; Default=100\n",
    "        actor_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        critic_structure(): Define the Structure of the NN, Default: [256,128,64] (Can only take List of len 3)\n",
    "        '''\n",
    "        # create environemt\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_dimensions = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # set Hyperparameters\n",
    "        self.lr = 3e-4\n",
    "        self.clip_ratio = 0.2\n",
    "        self.c_1 = 0.5\n",
    "        self.optimizer = Adam()\n",
    "        self.render = render\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.actor_struct = actor_structure\n",
    "        self.critic_struct = critic_structure\n",
    "        \n",
    "        # create models and temporary storage\n",
    "        self.actor = Actor(self.num_actions,self.actor_struct)\n",
    "        self.critic = Critic(self.critic_struct)\n",
    "        self.storage = Storage()\n",
    "\n",
    "\n",
    "    def collect_train_data(self, epoch):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        \n",
    "\n",
    "        # Initialize values for return, length and episodes\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "\n",
    "        # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "        #  allows takes on action in a state and saves the information in storage object\n",
    "        for t in tqdm_notebook(range(self.steps_per_epoch), desc = 'Epoch:' + str(epoch)):\n",
    "\n",
    "            # Toggles displaying of environment\n",
    "            if self.render or epoch == self.epochs-1 and self.epochs != 1:\n",
    "                self.env.render()\n",
    "\n",
    "            # Reshaping observation to fit as input for Actor network (policy)\n",
    "            observation = observation.reshape(1,-1)\n",
    "            \n",
    "            # Obtain action and logits for this observation by our actor\n",
    "            logits, action = self.actor.sample_action(observation)\n",
    "            \n",
    "            # Take action in environment and obtain the rewards for it\n",
    "            # Variable done represents wether agent has finished \n",
    "            # The last variable would be diagnostic information, not needed for training\n",
    "            observation_new, reward, done, _ = self.env.step(action[0].numpy())\n",
    "\n",
    "            # Sum up rewards over this episode and count amount of frames\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the Base-Estimate from the Critics network\n",
    "            base_estimate = self.critic(observation)\n",
    "\n",
    "            # Store Variables collected in this timestep t\n",
    "            self.storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "            # Save the new state of our agent\n",
    "            observation = observation_new\n",
    "            \n",
    "            # Check if terminal state is reached in environment\n",
    "            if done:\n",
    "                # Save information about episode\n",
    "                self.storage.conclude_episode()\n",
    "                # Refresh environment and reset return and length value\n",
    "                observation, episode_return, episode_length = self.env.reset(), 0, 0\n",
    "\n",
    "        # obtain all episodes saved in storage\n",
    "        # episodes, amount_episodes = self.storage.get_episodes()\n",
    "\n",
    "\n",
    "    def actor_loss_fun(self, actions, logits_old, logits_new, rewards, b_estimates_new, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "        ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates_new)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "        #l1 = np.array(l1, dtype=\"float32\")\n",
    "        #l2 = np.array(l2, dtype=\"float32\")\n",
    "        \n",
    "\n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        #actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "        l1 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l1]), dtype=tf.float32)\n",
    "        l2 = tf.convert_to_tensor(np.array([tf.convert_to_tensor(l, dtype=tf.float32) for l in l2]), dtype=tf.float32)\n",
    "        return tf.convert_to_tensor(l1, dtype=tf.float32), tf.convert_to_tensor(l2, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape() as tape, tf.GradientTape() as tape2:\n",
    "            # print(self.actor.trainable_variables())\n",
    "            # Obtain action and logits for this state selected by policy\n",
    "            #print(f' Observation shape/type {observation}')\n",
    "            #print(f'Trainables: {self.actor.layers[0].weights}')\n",
    "\n",
    "\n",
    "            # logits_new, actions_new = sample_action(states)\n",
    "            logits_new = []\n",
    "            b_estimates_new = []\n",
    "\n",
    "            # Compute values with updated critic network\n",
    "            # b_estimates_new = critic(states)\n",
    "\n",
    "            # till we work with np arrays we need to sample each action for this by looping through it\n",
    "            for i in states:\n",
    "                logits, _ = self.actor.sample_action(i)\n",
    "                logits_new.append(logits)\n",
    "                b_estimate = self.critic(i)\n",
    "                \n",
    "                b_estimates_new.append(b_estimate)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen für den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()\n",
    "            #print('Weewoo')\n",
    "            #print(tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print('type critic')\n",
    "            print(type((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            print((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "\n",
    "            critic_loss = tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "            #actor_loss = entropy * self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            l1,l2 = self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            #print('minimum')\n",
    "            #print(-tf.reduce_mean(tf.minimum(l1, l2)))\n",
    "            #print(type(tf.minimum(l1,l2)))\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "            #critic_loss = tf.cast(critic_loss, dtype=tf.float32)\n",
    "            #print(f'Critics loss:{type(critic_loss)}. Actor Loss {actor_loss.dtype}')\n",
    "\n",
    "            #print('Actor weights')\n",
    "            #print(print(self.actor.layers[0].weights))\n",
    "\n",
    "            #print('actor')\n",
    "            #print(actor_loss)\n",
    "            #print(type(actor_loss))\n",
    "            #print('critic')\n",
    "            #print(critic_loss)\n",
    "            #print(type(critic_loss))\n",
    "\n",
    "            actor_loss = tf.convert_to_tensor(actor_loss, dtype=tf.float32)\n",
    "\n",
    "            print(actor_loss)\n",
    "            print(critic_loss)\n",
    "            print('actor')\n",
    "            print(self.actor.trainable_variables)\n",
    "            print('critic')\n",
    "            print(self.critic.trainable_variables)\n",
    "            a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            c_gradients = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            print(a_gradients)\n",
    "            print(c_gradients)\n",
    "\n",
    "            #print(tape)\n",
    "            #print('Actor loss')\n",
    "            #print(actor_loss)\n",
    "            #print('Trainable Weights')\n",
    "            #print(self.actor.trainable_weights)\n",
    "        \n",
    "        #print(f'Gradients Actor: {a_gradients}. Gradients Critic: {c_gradients}')\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def update_policy(self, episodes, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                        [look at single one]\n",
    "        '''\n",
    "        # for epoch in training_iteratins:\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in tqdm_notebook(episodes):\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                # States\n",
    "                episode[0],\n",
    "                # Actions\n",
    "                episode[1],\n",
    "                #optimizer (Adam)\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2],\n",
    "                # Rewards\n",
    "                episode[3],\n",
    "                clip_param,\n",
    "                c_1,\n",
    "                c_2 \n",
    "            )\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "            return train_losses_actor, train_losses_critic\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # # Estimated Value of the current situtation from the critics network\n",
    "        # b_estimates = self.episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "    ## get ratio lutsch noch ARSCH, das Ding verarscht mich anders\n",
    "\n",
    "    def get_ratio_episode(self, actions, logits_old, logits_new): \n",
    "        r = []\n",
    "        for a, o, n in zip(actions, logits_old, logits_new):\n",
    "            o = tf.convert_to_tensor(o)\n",
    "            n = tf.convert_to_tensor(n)\n",
    "            #print(f'A: {a} O: {type(o)} N: {type(n)}')\n",
    "\n",
    "            #get the Logarithmic version of all logits for computational efficiency\n",
    "            log_prob_old = tf.nn.log_softmax(o)\n",
    "            log_prob_new = tf.nn.log_softmax(n)\n",
    "\n",
    "            # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "            logprobability_old = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_old, axis=1\n",
    "            )\n",
    "            logprobability_new = tf.reduce_sum(\n",
    "                tf.one_hot(a, self.num_actions) * log_prob_new, axis=1\n",
    "            )\n",
    "            # get the ratio of new over old prob\n",
    "            ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "            r.append(ratio)\n",
    "        return r\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for epoch in tqdm_notebook(range(self.epochs), desc = 'Epochs'):\n",
    "            self.collect_train_data((epoch))\n",
    "            data, _ = self.storage.get_episodes()\n",
    "            #print(data)\n",
    "            self.update_policy(data, self.optimizer, self.clip_ratio)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7a6fb4944241279638b52f14283715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2a5ad7f10a48b580216a7a1ef922ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045a6b3a48114ff68c7ef9150f360b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type critic\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([], shape=(0,), dtype=float32)\n",
      "tf.Tensor(-0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "actor\n",
      "[<tf.Variable 'actor/dense/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[ 0.01409981, -0.00539116, -0.01662573, ..., -0.02715505,\n",
      "        -0.00596736, -0.00566574],\n",
      "       [ 0.017676  , -0.02006602, -0.00523527, ...,  0.00446293,\n",
      "        -0.00959453, -0.00962588],\n",
      "       [-0.01050518,  0.01976851, -0.00151063, ..., -0.02515877,\n",
      "        -0.01699403, -0.01757865],\n",
      "       ...,\n",
      "       [ 0.00649009, -0.02016316,  0.00346045, ...,  0.00526325,\n",
      "         0.00712228,  0.00175894],\n",
      "       [ 0.00473584,  0.01230994, -0.0120965 , ..., -0.00683025,\n",
      "        -0.01681167,  0.00422599],\n",
      "       [ 0.0093918 ,  0.00931567, -0.01264456, ...,  0.01324879,\n",
      "         0.00887312,  0.00499208]], dtype=float32)>, <tf.Variable 'actor/dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Variable 'actor/dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[ 0.00911615,  0.00925266,  0.00542157, ..., -0.0107984 ,\n",
      "         0.01475171,  0.00476226],\n",
      "       [ 0.00394174,  0.01690874,  0.01824152, ..., -0.00351864,\n",
      "        -0.0034775 , -0.00402917],\n",
      "       [ 0.00909558, -0.00517064,  0.00469003, ...,  0.00526381,\n",
      "         0.01094522,  0.01314575],\n",
      "       ...,\n",
      "       [-0.0087939 , -0.0036935 ,  0.01334018, ...,  0.01319817,\n",
      "         0.00571792,  0.00338481],\n",
      "       [ 0.01431413, -0.00025267,  0.00085282, ..., -0.00228589,\n",
      "        -0.00487391, -0.00585384],\n",
      "       [-0.00528682,  0.0058673 ,  0.00218209, ...,  0.00769526,\n",
      "        -0.00887826,  0.00853908]], dtype=float32)>, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.01156385, -0.00489789,  0.0095145 , ..., -0.00210382,\n",
      "         0.00465855, -0.01194742],\n",
      "       [-0.00794091, -0.0045555 ,  0.00298881, ...,  0.00508386,\n",
      "         0.01481146,  0.01121011],\n",
      "       [ 0.02377547,  0.01628162, -0.00860299, ..., -0.02047621,\n",
      "        -0.01476801, -0.01352108],\n",
      "       ...,\n",
      "       [-0.01091764, -0.00724662, -0.01506384, ...,  0.00036503,\n",
      "         0.01875769,  0.00337663],\n",
      "       [-0.01679011,  0.00047654, -0.00632949, ..., -0.00358079,\n",
      "         0.00304895,  0.00736671],\n",
      "       [-0.0103805 , -0.00578691,  0.00592329, ..., -0.00297016,\n",
      "         0.01780151,  0.00680956]], dtype=float32)>, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\n",
      "array([[-0.02158797, -0.07005644,  0.1206525 ,  0.0365369 ],\n",
      "       [ 0.18660891, -0.06472568, -0.16434792, -0.03862917],\n",
      "       [-0.2661875 , -0.11951412,  0.1785737 ,  0.10809135],\n",
      "       [ 0.240453  , -0.2750672 , -0.13723223,  0.14678413],\n",
      "       [ 0.25027835, -0.13852471, -0.12773119,  0.01944196],\n",
      "       [ 0.00147703,  0.03144774,  0.18875831,  0.22032857],\n",
      "       [ 0.21957308,  0.05811724, -0.13629265, -0.1511785 ],\n",
      "       [-0.20247859, -0.00369778,  0.2371782 ,  0.10556936],\n",
      "       [-0.2376012 , -0.17829262, -0.05658454, -0.0329048 ],\n",
      "       [ 0.23479265,  0.06495458,  0.07955492,  0.12124676],\n",
      "       [-0.11125916, -0.04413527,  0.16306004, -0.12030703],\n",
      "       [-0.18300807, -0.12298696,  0.25498873, -0.01637203],\n",
      "       [-0.2599401 , -0.14809488,  0.09497434, -0.02876723],\n",
      "       [ 0.26280648, -0.22155036,  0.227103  , -0.19129029],\n",
      "       [-0.2747692 , -0.2207333 , -0.04827406, -0.19222832],\n",
      "       [-0.13614683,  0.2253288 ,  0.1821236 ,  0.21527308],\n",
      "       [ 0.02762213, -0.0318478 ,  0.2834422 , -0.2557096 ],\n",
      "       [-0.02377769, -0.27592692,  0.23023987,  0.17955449],\n",
      "       [ 0.17044488, -0.21899578,  0.23448962,  0.22567242],\n",
      "       [-0.10451518, -0.2110897 , -0.2934195 ,  0.08735132],\n",
      "       [ 0.01416114, -0.2706916 ,  0.17301002, -0.20412892],\n",
      "       [-0.28120255,  0.11676139, -0.00532353, -0.14611827],\n",
      "       [-0.14638016,  0.1130845 , -0.20672593,  0.26536858],\n",
      "       [-0.22903329, -0.04051316, -0.00305903,  0.29281092],\n",
      "       [ 0.16146919, -0.00070664,  0.00197634, -0.2966369 ],\n",
      "       [ 0.08681753, -0.07763796,  0.2741887 , -0.24430914],\n",
      "       [ 0.21542966,  0.12947473,  0.18409666,  0.0149481 ],\n",
      "       [-0.17874232,  0.2561506 , -0.05561967, -0.03979835],\n",
      "       [ 0.11216828,  0.2116636 , -0.0645283 ,  0.15592349],\n",
      "       [-0.09696263, -0.16443184,  0.07529533, -0.09715711],\n",
      "       [ 0.25344998, -0.03239664, -0.21323244,  0.20436382],\n",
      "       [-0.25758415,  0.20445311,  0.19473568,  0.12740457],\n",
      "       [-0.07417367,  0.17925054, -0.19096658,  0.23697805],\n",
      "       [ 0.17919558,  0.2916116 ,  0.00252199,  0.10983044],\n",
      "       [-0.10092463, -0.2968236 , -0.15595104,  0.11739743],\n",
      "       [ 0.05228981, -0.15935186,  0.26877135,  0.1012589 ],\n",
      "       [-0.29042473, -0.13362022,  0.15681258,  0.21364486],\n",
      "       [ 0.12989044,  0.22284108, -0.11519355,  0.19709966],\n",
      "       [ 0.08760038, -0.27823058,  0.02493829, -0.08668673],\n",
      "       [ 0.12108457, -0.16910708,  0.10571155, -0.22515726],\n",
      "       [-0.11943579, -0.00284383,  0.22702181, -0.06079845],\n",
      "       [ 0.13620609,  0.24687487, -0.06993058, -0.04499179],\n",
      "       [-0.22452909,  0.1907573 , -0.10236576, -0.17476007],\n",
      "       [ 0.129051  , -0.21703744, -0.1347139 , -0.14070578],\n",
      "       [-0.03918433, -0.16551398, -0.08996709, -0.10164595],\n",
      "       [ 0.29064655, -0.06536901, -0.00285989, -0.02880201],\n",
      "       [-0.16466221,  0.06134015, -0.26548764,  0.11443838],\n",
      "       [-0.1457658 ,  0.00475648,  0.24825889, -0.13648535],\n",
      "       [ 0.08494809, -0.25637484,  0.1826942 , -0.04963148],\n",
      "       [-0.11487161, -0.04434252,  0.10491064, -0.26543918],\n",
      "       [ 0.00058171,  0.02626464,  0.0354214 , -0.13539733],\n",
      "       [-0.00948277, -0.27872935,  0.12518665, -0.08339965],\n",
      "       [ 0.1053969 , -0.28872246,  0.02776739,  0.2543745 ],\n",
      "       [-0.0369314 ,  0.03190628,  0.09900963, -0.00712848],\n",
      "       [-0.10224204,  0.07233664, -0.21551457,  0.06509438],\n",
      "       [ 0.18081611, -0.27546707, -0.28287494,  0.15028629],\n",
      "       [-0.1606039 ,  0.16512665,  0.0204159 , -0.07033597],\n",
      "       [ 0.28629142, -0.24938324, -0.03003052, -0.19869992],\n",
      "       [-0.15548794,  0.06056374, -0.02504247, -0.11444908],\n",
      "       [-0.23291966, -0.2887753 ,  0.02025867,  0.06195474],\n",
      "       [-0.11439279,  0.1726465 , -0.20391476, -0.07539561],\n",
      "       [ 0.22371233,  0.08779812,  0.26678282, -0.24339506],\n",
      "       [ 0.22273314,  0.28431982, -0.2908014 ,  0.2913713 ],\n",
      "       [-0.27033517, -0.11323628, -0.01478845, -0.03955027]],\n",
      "      dtype=float32)>, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "critic\n",
      "[<tf.Variable 'critic/dense_4/kernel:0' shape=(8, 256) dtype=float32, numpy=\n",
      "array([[ 0.00294245,  0.00428662, -0.01163544, ..., -0.00626715,\n",
      "         0.00049932,  0.00503913],\n",
      "       [-0.00511775,  0.00323089,  0.00105189, ..., -0.02240095,\n",
      "         0.00274174,  0.00151338],\n",
      "       [ 0.00704541,  0.00602873, -0.00592248, ..., -0.01692653,\n",
      "        -0.0166349 , -0.01037321],\n",
      "       ...,\n",
      "       [-0.01051044, -0.01761666, -0.00079895, ...,  0.00652473,\n",
      "        -0.00758824,  0.00616234],\n",
      "       [-0.02680599,  0.01107133,  0.00819033, ...,  0.00106212,\n",
      "        -0.00442503, -0.01255303],\n",
      "       [ 0.00734191,  0.00565885,  0.00620341, ..., -0.00026292,\n",
      "        -0.01039371,  0.00135   ]], dtype=float32)>, <tf.Variable 'critic/dense_4/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Variable 'critic/dense_5/kernel:0' shape=(256, 128) dtype=float32, numpy=\n",
      "array([[ 0.01240714, -0.01966183, -0.00255988, ..., -0.01881894,\n",
      "         0.00152434,  0.00142511],\n",
      "       [ 0.00166085,  0.00317524,  0.00417432, ..., -0.00795913,\n",
      "        -0.01270629, -0.00758577],\n",
      "       [ 0.00067061,  0.00569347, -0.00859336, ..., -0.00365907,\n",
      "        -0.00257207,  0.01345988],\n",
      "       ...,\n",
      "       [ 0.01528348, -0.00874469,  0.01091201, ..., -0.00068626,\n",
      "         0.0081007 , -0.01833302],\n",
      "       [-0.0050212 ,  0.00912008, -0.01004629, ..., -0.0079803 ,\n",
      "        -0.01706826,  0.00223423],\n",
      "       [ 0.01190888,  0.02462259,  0.00035919, ..., -0.00019514,\n",
      "         0.01759888, -0.00024219]], dtype=float32)>, <tf.Variable 'critic/dense_5/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_6/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[ 0.01009845,  0.00108124, -0.01260212, ...,  0.00682111,\n",
      "         0.00454671, -0.00632705],\n",
      "       [ 0.01330411,  0.00065581,  0.00316291, ..., -0.00536338,\n",
      "         0.00415047,  0.00859067],\n",
      "       [ 0.00272355, -0.00832687, -0.02301566, ...,  0.00222655,\n",
      "        -0.01712744, -0.02114185],\n",
      "       ...,\n",
      "       [ 0.01399587,  0.00461308,  0.00272376, ..., -0.00577711,\n",
      "         0.00102412,  0.00627429],\n",
      "       [-0.00838651, -0.01035589, -0.01022623, ...,  0.01502043,\n",
      "        -0.02216665,  0.01323725],\n",
      "       [-0.01662559,  0.00476329, -0.00314504, ..., -0.01293724,\n",
      "        -0.01216751, -0.00911987]], dtype=float32)>, <tf.Variable 'critic/dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'critic/dense_7/kernel:0' shape=(64, 1) dtype=float32, numpy=\n",
      "array([[-0.02208054],\n",
      "       [-0.07165489],\n",
      "       [ 0.12340537],\n",
      "       [ 0.03737056],\n",
      "       [ 0.19086668],\n",
      "       [-0.06620249],\n",
      "       [-0.16809776],\n",
      "       [-0.03951055],\n",
      "       [-0.27226096],\n",
      "       [-0.12224102],\n",
      "       [ 0.18264815],\n",
      "       [ 0.11055762],\n",
      "       [ 0.24593931],\n",
      "       [-0.2813433 ],\n",
      "       [-0.1403634 ],\n",
      "       [ 0.15013322],\n",
      "       [ 0.25598884],\n",
      "       [-0.14168537],\n",
      "       [-0.13064557],\n",
      "       [ 0.01988557],\n",
      "       [ 0.00151074],\n",
      "       [ 0.03216526],\n",
      "       [ 0.19306514],\n",
      "       [ 0.22535568],\n",
      "       [ 0.22458303],\n",
      "       [ 0.05944327],\n",
      "       [-0.13940237],\n",
      "       [-0.15462786],\n",
      "       [-0.20709844],\n",
      "       [-0.00378215],\n",
      "       [ 0.24258977],\n",
      "       [ 0.10797808],\n",
      "       [-0.24302244],\n",
      "       [-0.18236063],\n",
      "       [-0.0578756 ],\n",
      "       [-0.03365555],\n",
      "       [ 0.2401498 ],\n",
      "       [ 0.06643662],\n",
      "       [ 0.08137009],\n",
      "       [ 0.12401319],\n",
      "       [-0.11379772],\n",
      "       [-0.04514229],\n",
      "       [ 0.1667805 ],\n",
      "       [-0.12305202],\n",
      "       [-0.1871837 ],\n",
      "       [-0.1257931 ],\n",
      "       [ 0.26080668],\n",
      "       [-0.01674557],\n",
      "       [-0.26587102],\n",
      "       [-0.1514739 ],\n",
      "       [ 0.09714133],\n",
      "       [-0.02942359],\n",
      "       [ 0.26880282],\n",
      "       [-0.22660537],\n",
      "       [ 0.23228472],\n",
      "       [-0.19565488],\n",
      "       [-0.2810385 ],\n",
      "       [-0.22576967],\n",
      "       [-0.0493755 ],\n",
      "       [-0.19661431],\n",
      "       [-0.13925323],\n",
      "       [ 0.23047   ],\n",
      "       [ 0.18627903],\n",
      "       [ 0.22018486]], dtype=float32)>, <tf.Variable 'critic/dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "[None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/2511223439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mppo_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/1926255560.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/1926255560.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, episodes, optimizer, clip_param, c_1, c_2)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;31m# Compute train losses and action by chosen by policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             actor_loss, critic_loss = self.train_step(\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# States\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mepisode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_54769/1926255560.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[1;32m     76\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0']."
     ]
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent(env_name='LunarLander-v2', render=False,epochs=1)\n",
    "ppo_agent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
