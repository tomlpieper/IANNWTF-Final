{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        # Outputs from the actor network, an action is sampled from (Probabilities)\n",
    "        self.logits = []\n",
    "        # Outputs from the crtitics network (Values)\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) \n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collected values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards)])\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(4, activation=\"softmax\")\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "#@tf.function\n",
    "def sample_action(observation):\n",
    "    '''\n",
    "    Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "    Args:\n",
    "    observation(): Representation of actors state. Same as x in the call function. \n",
    "    '''\n",
    "    # Output of softmax function\n",
    "    logits = actor(observation)\n",
    "   # tf.print(type(logits))\n",
    "    # Sample action from the Softmax output of the network\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "   # tf.print(action)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "# Number of iterations\n",
    "epochs = 1\n",
    "# Leads to ~10 Episodes per epoch, then compute new parameters (smaller batching)\n",
    "steps_per_epoch = 1000 \n",
    "\n",
    "# Learning rate for actor and critic\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "\n",
    "# Parameter to decide how strongly the policy ratio gets clipped therefore how much policy (actor network)\n",
    "#  updates we allow\n",
    "# The selected 0.2 is the number proposed by the original paper by OpenAI\n",
    "clip_ratio = 0.2\n",
    "# Weighs loss of critic model\n",
    "c_1 = 0.5\n",
    "\n",
    "#\n",
    "target_kl = 0.01\n",
    "\n",
    "\n",
    "# Update weights with Adam optimizer\n",
    "optimizer = Adam()\n",
    "\n",
    "# To toggle displaying of environment\n",
    "render = False\n",
    "\n",
    "# Discount variable for rewards to whey immediate rewards stronger\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# Get dimensions of state and amount of possible actions (4 for LunarLander-v2)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage object to save observations, actions, rewards etc. during trajectory\n",
    "storage = Storage()\n",
    "\n",
    "# initialize actor and critics model\n",
    "observation_input = Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# Initialize: observation(agent state), \n",
    "# episode return(summed rewards for singe ) and \n",
    "# episode length(amount of steps taken (=frames) before agent finished)\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    ###Skizze - Not used yet\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "    Whats missing: \n",
    "    - All the FUCKING self's before variable assignment and for functions (fuck you python, even though i love you)    \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        ###Maybe pass hyperparameters?\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "\n",
    "    def collect_train_data(self):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Initialize values for return, length and episodes\n",
    "            sum_return = 0\n",
    "            sum_length = 0\n",
    "            num_episodes = 0\n",
    "\n",
    "            # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "            #  allows takes on action in a state and saves the information in storage object\n",
    "            for t in tqdm(range(steps_per_epoch)):\n",
    "\n",
    "                # Toggles displaying of environment\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                # Reshaping observation to fit as input for Actor network (policy)\n",
    "                observation = observation.reshape(1,-1)\n",
    "\n",
    "                # Obtain action and logits for this observation by our actor\n",
    "                logits, action = sample_action(observation=observation)\n",
    "                \n",
    "                # Take action in environment and obtain the rewards for it\n",
    "                # Variable done represents wether agent has finished \n",
    "                # The last variable would be diagnostic information, not needed for training\n",
    "                observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "                # Sum up rewards over this episode and count amount of frames\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "\n",
    "                # Get the Base-Estimate from the Critics network\n",
    "                base_estimate = critic(observation)\n",
    "\n",
    "                # Store Variables collected in this timestep t\n",
    "                storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "                # Save the new state of our agent\n",
    "                observation = observation_new\n",
    "                \n",
    "                # Check if terminal state is reached in environment\n",
    "                if done:\n",
    "                    # Save information about episode\n",
    "                    storage.conclude_episode()\n",
    "                    # Refresh environment and reset return and length value\n",
    "                    observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "            # obtain all episodes saved in storage\n",
    "            episodes, amount_episodes = storage.get_episodes()\n",
    "\n",
    "\n",
    "    def update_policy(episodes, actor, critic, actor_loss_fun, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                    [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                    [look at single one]\n",
    "        '''\n",
    "\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in episodes[0]:\n",
    "\n",
    "            # All states in a single episode\n",
    "            for train_state_idx in range(len(episode[0])):\n",
    "\n",
    "                # Start policy iteration from same state as training data\n",
    "                #if train_state_idx == 0:\n",
    "\n",
    "                # Update parameters\n",
    "                # Compute train losses and action by chosen by policy\n",
    "                actor_loss, critic_loss = train_step(\n",
    "                    actor,\n",
    "                    critic,\n",
    "                    # State\n",
    "                    episode[0][train_state_idx],\n",
    "                    actor_loss_fun,\n",
    "                    optimizer,\n",
    "                    # Logits\n",
    "                    episode[2][train_state_idx],\n",
    "                    # Reward\n",
    "                    episode[3][train_state_idx],\n",
    "                    clip_param,\n",
    "                    c_1,\n",
    "                    c_2 \n",
    "                )\n",
    "                train_losses_actor.append(actor_loss)\n",
    "                train_losses_critic.append(critic_loss)\n",
    "\n",
    "\n",
    "    def train_step(actor, critic, state, actor_loss_fun, optimizer, train_logits, train_rewards, clip_param, c_1, c_2):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Obtain action and logits for this state selected by policy\n",
    "            logits, action = sample_action(observation=state.reshape(1,-1))\n",
    "\n",
    "            # Take computed action in environment to get new state & reward\n",
    "            new_state, reward, _, _ = env.step(action[0].numpy())\n",
    "\n",
    "            # Compute baseline estimate (value) for new state\n",
    "            b_estimate = critic(new_state.reshape(1,-1))\n",
    "\n",
    "            entropy = c_2 * np.mean(-(logits * train_logits))\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimate, discounted_reward(train_rewards)).numpy()\n",
    "            actor_loss = entropy * actor_loss_fun(train_logits, logits, reward, b_estimate, clip_param)\n",
    "\n",
    "            a_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "            c_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.apply_gradients(zip(a_gradients, actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, critic.trainable_variables))\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "\n",
    "    def actor_loss_fun(logits_old, logits_new, rewards, b_estimates, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates():\n",
    "        clip_param():\n",
    "\n",
    "        Whats missing:\n",
    "        - Entropy\n",
    "        '''\n",
    "\n",
    "        ratio = get_ratio(logits_old, logits_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = get_advantage(rewards, b_estimates)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "\n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        return (np.mean(np.min(l1, l2)))\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # Estimated Value of the current situtation from the critics network\n",
    "        b_estimates = episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "    def get_ratio(action, logits_old, logits_new):\n",
    "\n",
    "        #get the Logarithmic version of all logits for computational efficiency\n",
    "        log_prob_old = tf.nn.log_softmax(logits_old)\n",
    "        log_prob_new = tf.nn.log_softmax(logits_new)\n",
    "\n",
    "        # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "        logprobability_old = tf.reduce_sum(\n",
    "            tf.one_hot(action, num_actions) * log_prob_old, axis=1\n",
    "        )\n",
    "        logprobability_new = tf.reduce_sum(\n",
    "            tf.one_hot(action, num_actions) * log_prob_new, axis=1\n",
    "        )\n",
    "        # get the ratio of new over old prob\n",
    "        ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "\n",
    "        print(ratio)\n",
    "        return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.65it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data collection.\n",
    "-> Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "'''\n",
    "\n",
    "\n",
    "episodes_total = 0\n",
    "# Iteration of whole training process\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Initialize values for return, length and episodes\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "    #  allows takes on action in a state and saves the information in storage object\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "\n",
    "        # Toggles displaying of environment\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Reshaping observation to fit as input for Actor network (policy)\n",
    "        observation = observation.reshape(1,-1)\n",
    "\n",
    "        # Obtain action and logits for this observation by our actor\n",
    "        logits, action = sample_action(observation=observation)\n",
    "        \n",
    "        # Take action in environment and obtain the rewards for it\n",
    "        # Variable done represents wether agent has finished \n",
    "        # The last variable would be diagnostic information, not needed for training\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "        # Sum up rewards over this episode and count amount of frames\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the Base-Estimate from the Critics network\n",
    "        base_estimate = critic(observation)\n",
    "\n",
    "        # Store Variables collected in this timestep t\n",
    "        storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "        # Save the new state of our agent\n",
    "        observation = observation_new\n",
    "        \n",
    "        # Check if terminal state is reached in environment\n",
    "        if done:\n",
    "            # Save information about episode\n",
    "            storage.conclude_episode()\n",
    "            # Refresh environment and reset return and length value\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # obtain all episodes saved in storage\n",
    "    episodes, amount_episodes = storage.get_episodes()\n",
    "\n",
    "  \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes [episode][particular values // 0: Observations, 1: actions, 2: logits, 3, rewards, 4: BaselineEstimates from Critics]\n",
    "#print(episodes[0][4])\n",
    "#print(f'Number of Episodes = {amount_episodes}')\n",
    "\n",
    "\n",
    "\n",
    "### Advantagefunction\n",
    "\n",
    "\n",
    "# for i in b_estimates:\n",
    "#   print(i.numpy())\n",
    "\n",
    "# Discounted sum of rewards\n",
    "# Saves list of all rewards in new variable \n",
    "#rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MIGHT NOT WORK\n",
    "#  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "#  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "# \n",
    "#  ###\n",
    "\n",
    "def discounted_reward(rewards, gamma = 0.99):\n",
    "    '''\n",
    "    weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "    Args:\n",
    "    rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "    gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "    '''\n",
    "    # To select the next reward\n",
    "    i = 0\n",
    "    discounted_rewards = []\n",
    "\n",
    "    # Iterates through every reward and appends a discounted version to the output\n",
    "    for r in rewards:\n",
    "        disc = 0\n",
    "        for t in rewards[i:-1]:\n",
    "            discount_t = gamma ** t\n",
    "            disc += t * discount_t\n",
    "        i += 1\n",
    "        discounted_rewards.append(disc)\n",
    "\n",
    "    # returns list of discounted rewards.\n",
    "    return discounted_rewards\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (Temp/ipykernel_11172/2303987738.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User1\\AppData\\Local\\Temp/ipykernel_11172/2303987738.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def get_advantage(rewards, gamma = 0.99, b_estimates):\u001b[0m\n\u001b[1;37m                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def get_advantage(rewards, b_estimates, gamma = 0.99):\n",
    "    '''\n",
    "    Computes Advantage for action in state.\n",
    "\n",
    "    Args:\n",
    "    rewards(float?): Reward for action.\n",
    "    b_estimates(float): Baseline Estimates.\n",
    "    gamma(float): Discount factor.\n",
    "    \n",
    "    '''\n",
    "    # Saves list of all rewards in new variable \n",
    "    #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "    # Get discounted sum of rewards \n",
    "    disc_sum = discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "    # Estimated Value of the current situtation from the critics network\n",
    "    b_estimates = episodes[0][4] \n",
    "\n",
    "    # Convert lists to np arrays and flatten\n",
    "    disc_sum_np = np.array(disc_sum)\n",
    "    b_estimates_np = np.array(b_estimates)\n",
    "    b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "    # substract arrays to obtain advantages\n",
    "    advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogProbs and Ratio Computation\n",
    "\n",
    "We need the ratio of probabilities for an action at state t of the 'new' model vs the old model (maybe because of entropy there is a difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#creating oneHot vector with size actions space and getting the log for the probability of choosing this action\n",
    "\n",
    "#print(f'log old: {logits_old}')\n",
    "#print(f'log new: {logits_new}')\n",
    "\n",
    "\n",
    "### this function currently only takes one single action and 2 sets of logits and computes the ratio of that\n",
    "\n",
    "def get_ratio(action, logits_old, logits_new):\n",
    "\n",
    "    #get the Logarithmic version of all logits for computational efficiency\n",
    "    log_prob_old = tf.nn.log_softmax(logits_old)\n",
    "    log_prob_new = tf.nn.log_softmax(logits_new)\n",
    "\n",
    "    # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "    logprobability_old = tf.reduce_sum(\n",
    "        tf.one_hot(action, num_actions) * log_prob_old, axis=1\n",
    "    )\n",
    "    logprobability_new = tf.reduce_sum(\n",
    "        tf.one_hot(action, num_actions) * log_prob_new, axis=1\n",
    "    )\n",
    "    # get the ratio of new over old prob\n",
    "    ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "\n",
    "    print(ratio)\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using one Episode as example to get the prob ratio of old vs new\n",
    "logits_old = episodes[0][2][0]\n",
    "action = episodes[0][1][0].numpy()\n",
    "obs = episodes[0][0]\n",
    "logits_new = []\n",
    "for i in obs:\n",
    "    tensor = tf.convert_to_tensor(i)\n",
    "    new, _ = sample_action(tensor)\n",
    "    logits_new.append(new)\n",
    "\n",
    "logits_new = logits_new[0]\n",
    "\n",
    "get_ratio(action, logits_old, logits_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(episodes, actor, critic, actor_loss_fun, optimizer, clip_param, c_1):\n",
    "    '''\n",
    "    Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "    Args: \n",
    "    episodes(list): Contains all information on one episode in the following order:\n",
    "                    [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "    actor(object): Object of the actor model.\n",
    "    critic(object): Object of the critic model.\n",
    "    actor_loss(function): Clipped objective function for PPO.\n",
    "    optimizer(object): Optimizer used to train actor.\n",
    "    clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "    c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "\n",
    "\n",
    "    Information stored as:\n",
    "    storage.episodes[different episodes]\n",
    "                [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                [look at single one]\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Save network loss\n",
    "    train_losses_actor = []\n",
    "    train_losses_critic = []\n",
    "     \n",
    "    # Iterate over all finished episodes from collected training data\n",
    "    for episode in episodes[0]:\n",
    "\n",
    "        # All states in a single episode\n",
    "        for train_state_idx in range(len(episode[0])):\n",
    "\n",
    "            # Start policy iteration from same state as training data\n",
    "            #if train_state_idx == 0:\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = train_step(\n",
    "                actor,\n",
    "                critic,\n",
    "                # State\n",
    "                episode[0][train_state_idx],\n",
    "                actor_loss_fun,\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2][train_state_idx],\n",
    "                # Reward\n",
    "                episode[3][train_state_idx],\n",
    "                clip_param,\n",
    "                c_1  \n",
    "            )\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "        # Update to evaluate next policy step\n",
    "        #state = new_state\n",
    "\n",
    "\n",
    "\n",
    "def train_step(actor, critic, state, actor_loss_fun, optimizer, train_logits, train_rewards, clip_param, c_1):\n",
    "    '''\n",
    "    Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "    Args:\n",
    "    model(object): Object of the actor model.\n",
    "    input(list): contains floats describing the actors state.\n",
    "    loss_function(function): Clipped objective function for PPO.\n",
    "    optimizer(object): Optimizer used to train actor.\n",
    "    train_logits():\n",
    "    train_rewards():\n",
    "    clip_param():\n",
    "    c_1():\n",
    "    '''\n",
    "\n",
    "    # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Obtain action and logits for this state selected by policy\n",
    "        logits, action = sample_action(observation=state.reshape(1,-1))\n",
    "\n",
    "        # Take computed action in environment to get new state & reward\n",
    "        new_state, reward, _, _ = env.step(action[0].numpy())\n",
    "\n",
    "        # Compute baseline estimate (value) for new state\n",
    "        b_estimate = critic(new_state.reshape(1,-1))\n",
    "\n",
    "\n",
    "        # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "        #  which represents an estimate based on rewards collected during training\n",
    "        critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimate, discounted_reward(train_rewards)).numpy()\n",
    "        actor_loss = actor_loss_fun(train_logits, logits, reward, b_estimate, clip_param)\n",
    "\n",
    "        a_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "        c_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.apply_gradients(zip(a_gradients, actor.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(c_gradients, critic.trainable_variables))\n",
    "\n",
    "    return actor_loss, critic_loss\n",
    "\n",
    "\n",
    "def actor_loss_fun(logits_old, logits_new, rewards, b_estimates, clip_param):\n",
    "    '''\n",
    "    Computes loss for Actor Network output.\n",
    "\n",
    "    Args:\n",
    "    logits_old():\n",
    "    logits_new():\n",
    "    reward():\n",
    "    b_estimates():\n",
    "    clip_param():\n",
    "\n",
    "\n",
    "    Whats missing:\n",
    "    - Entropy\n",
    "    '''\n",
    "\n",
    "    ratio = get_ratio(logits_old, logits_new)\n",
    "\n",
    "    ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "    advantage = get_advantage(rewards, b_estimates)\n",
    "    \n",
    "    # Unclipped value\n",
    "    l1 = ratio * advantage\n",
    "    # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "    l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "\n",
    "    # Compute minimum of both and take the mean to return float loss\n",
    "    return (np.mean(np.min(l1, l2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14340/3921470302.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing range\n",
    "'''\n",
    "\n",
    "#print(np.clip(100, 0.8, 1.2))\n",
    "\n",
    "\n",
    "state = [1,2,3]\n",
    "\n",
    "for i in range(state):\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "print(storage.episodes[0][0][0])\n",
    "\n",
    "'''\n",
    "storage.episodes[different episodes]\n",
    "            [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "            [look at single one]\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
