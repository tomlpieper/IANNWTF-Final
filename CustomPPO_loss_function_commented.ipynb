{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "\n",
    "\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        # Outputs from the actor network, an action is sampled from (Probabilities)\n",
    "        self.logits = []\n",
    "        # Outputs from the crtitics network (Values)\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) \n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collected values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards)])\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(4, activation=\"softmax\")\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "    #@tf.function\n",
    "    def sample_action(self,observation):\n",
    "        '''\n",
    "        Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "        Args:\n",
    "        observation(): Representation of actors state. Same as x in the call function. \n",
    "        '''\n",
    "        # Output of softmax function\n",
    "        #logits = self.call(observation)\n",
    "        logits = self(observation)\n",
    "    # tf.print(type(logits))\n",
    "        # Sample action from the Softmax output of the network\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    # tf.print(action)\n",
    "        return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            \n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust Hyperparameters\n",
    "'''\n",
    "\n",
    "# Number of iterations\n",
    "epochs = 1\n",
    "# Leads to ~10 Episodes per epoch, then compute new parameters (smaller batching)\n",
    "steps_per_epoch = 1000 \n",
    "\n",
    "# Learning rate for actor and critic\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "\n",
    "# Parameter to decide how strongly the policy ratio gets clipped therefore how much policy (actor network)\n",
    "#  updates we allow\n",
    "# The selected 0.2 is the number proposed by the original paper by OpenAI\n",
    "clip_ratio = 0.2\n",
    "# Weighs loss of critic model\n",
    "c_1 = 0.5\n",
    "\n",
    "#\n",
    "target_kl = 0.01\n",
    "\n",
    "\n",
    "# Update weights with Adam optimizer\n",
    "optimizer = Adam()\n",
    "\n",
    "# To toggle displaying of environment\n",
    "render = False\n",
    "\n",
    "# Discount variable for rewards to whey immediate rewards stronger\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# Get dimensions of state and amount of possible actions (4 for LunarLander-v2)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage object to save observations, actions, rewards etc. during trajectory\n",
    "#storage = Storage()\n",
    "\n",
    "# initialize actor and critics model\n",
    "#observation_input = Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "# actor = Actor()\n",
    "# critic = Critic()\n",
    "\n",
    "# Initialize: observation(agent state), \n",
    "# episode return(summed rewards for singe ) and \n",
    "# episode length(amount of steps taken (=frames) before agent finished)\n",
    "# observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    ###Skizze - Not used yet\n",
    "\n",
    "    Currently contains:\n",
    "    - Collects data\n",
    "    - Training process (iterator, updater, actor loss fun)\n",
    "    - get advantage function\n",
    "    - dicount rewards function\n",
    "    - Get ratio function\n",
    "\n",
    "    Whats missing: \n",
    "    - All the FUCKING self's before variable assignment and for functions (fuck you python, even though i love you)    \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' \n",
    "        Initialize Parameters.\n",
    "        ###Maybe pass hyperparameters?\n",
    "        '''\n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "        self.storage = Storage()\n",
    "        #print(self.actor.trainable_variables())\n",
    "\n",
    "\n",
    "    def collect_train_data(self):\n",
    "        '''\n",
    "        Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "        -> Data collection\n",
    "        '''\n",
    "        observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "        episodes_total = 0\n",
    "        # Iteration of whole training process\n",
    "        # for epoch in tqdm(range(epochs), desc = 'Epochs'):\n",
    "\n",
    "            # Initialize values for return, length and episodes\n",
    "            # sum_return = 0\n",
    "            # sum_length = 0\n",
    "            # num_episodes = 0\n",
    "\n",
    "        # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "        #  allows takes on action in a state and saves the information in storage object\n",
    "        for t in tqdm(range(steps_per_epoch)):\n",
    "\n",
    "            # Toggles displaying of environment\n",
    "            # if render or epoch == epochs-1 and epochs != 1:\n",
    "            #     env.render()\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Reshaping observation to fit as input for Actor network (policy)\n",
    "            observation = observation.reshape(1,-1)\n",
    "            \n",
    "            # Obtain action and logits for this observation by our actor\n",
    "            logits, action = self.actor.sample_action(observation)\n",
    "            \n",
    "            # Take action in environment and obtain the rewards for it\n",
    "            # Variable done represents wether agent has finished \n",
    "            # The last variable would be diagnostic information, not needed for training\n",
    "            observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "            # Sum up rewards over this episode and count amount of frames\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the Base-Estimate from the Critics network\n",
    "            base_estimate = self.critic(observation)\n",
    "\n",
    "            # Store Variables collected in this timestep t\n",
    "            self.storage.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "            # Save the new state of our agent\n",
    "            observation = observation_new\n",
    "            \n",
    "            # Check if terminal state is reached in environment\n",
    "            if done:\n",
    "                # Save information about episode\n",
    "                self.storage.conclude_episode()\n",
    "                # Refresh environment and reset return and length value\n",
    "                observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "        # obtain all episodes saved in storage\n",
    "        # episodes, amount_episodes = self.storage.get_episodes()\n",
    "\n",
    "\n",
    "    def update_policy(self, episodes, optimizer, clip_param, c_1 = 1, c_2=0.01):\n",
    "        '''\n",
    "        Update policy with the collected data (Parameter updates for actor)\n",
    "\n",
    "        Args: \n",
    "        episodes(list): Contains all information on one episode in the following order:\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, summed rewards]\n",
    "        actor(object): Object of the actor model.\n",
    "        critic(object): Object of the critic model.\n",
    "        actor_loss(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        clip_param(float): Hyperparameter to decide values to clip ratio between.\n",
    "        c_1(float): hyperparameter to determine how strongly loss of the critic network should be weighed\n",
    "        c_2(float): hyperparameter to determine how strongly entropy should be weighed\n",
    "\n",
    "\n",
    "        Information stored as:\n",
    "        storage.episodes[different episodes]\n",
    "                        [observations, actions, logits, rewards, BaselineEstimate, sum(self.rewards)]\n",
    "                        [look at single one]\n",
    "        '''\n",
    "        # for epoch in training_iteratins:\n",
    "        # Save network loss\n",
    "        train_losses_actor = []\n",
    "        train_losses_critic = []\n",
    "        \n",
    "        # Iterate over all finished episodes from collected training data\n",
    "        for episode in tqdm(episodes):\n",
    "\n",
    "            # Update parameters\n",
    "            # Compute train losses and action by chosen by policy\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                # States\n",
    "                episode[0],\n",
    "                # Actions\n",
    "                episode[1],\n",
    "                #optimizer (Adam)\n",
    "                optimizer,\n",
    "                # Logits\n",
    "                episode[2],\n",
    "                # Rewards\n",
    "                episode[3],\n",
    "                clip_param,\n",
    "                c_1,\n",
    "                c_2 \n",
    "            )\n",
    "\n",
    "            train_losses_actor.append(actor_loss)\n",
    "            train_losses_critic.append(critic_loss)\n",
    "\n",
    "            return train_losses_actor, train_losses_critic\n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2):\n",
    "        '''\n",
    "        Updates actor network parameters and returns the loss to evaluate performance.\n",
    "\n",
    "        Args:\n",
    "        model(object): Object of the actor model.\n",
    "        input(list): contains floats describing the actors state.\n",
    "        loss_function(function): Clipped objective function for PPO.\n",
    "        optimizer(object): Optimizer used to train actor.\n",
    "        train_logits():\n",
    "        train_rewards():\n",
    "        clip_param():\n",
    "        c_1(): \n",
    "        c_2():\n",
    "        '''\n",
    "\n",
    "        \n",
    "\n",
    "        # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        with tf.GradientTape() as tape, tf.GradientTape() as tape2:\n",
    "            # print(self.actor.trainable_variables())\n",
    "            # Obtain action and logits for this state selected by policy\n",
    "            #print(f' Observation shape/type {observation}')\n",
    "            #print(f'Trainables: {self.actor.layers[0].weights}')\n",
    "\n",
    "\n",
    "            # logits_new, actions_new = sample_action(states)\n",
    "            logits_new = []\n",
    "            b_estimates_new = []\n",
    "\n",
    "            # Compute values with updated critic network\n",
    "            # b_estimates_new = critic(states)\n",
    "\n",
    "            # till we work with np arrays we need to sample each action for this by looping through it\n",
    "            for i in states:\n",
    "                logits, _ = self.actor.sample_action(i)\n",
    "                logits_new.append(logits)\n",
    "                b_estimate = self.critic(i)\n",
    "                \n",
    "                b_estimates_new.append(b_estimate)\n",
    "\n",
    "            # Compute & weigh entropy \n",
    "            #entropy = c_2 * np.mean(-(logits_new * train_logits))   # <----- DOESNT WORK YET Musste ich erstmal rausnehmen für den Rest vom Debugging\n",
    "            # entropy = 0.01\n",
    "\n",
    "            # Computes MSE between output of the critics network (value) the discounted sum of rewards\n",
    "            #  which represents an estimate based on rewards collected during training\n",
    "            # critic_loss = c_1 * tf.keras.losses.MeanSquaredError(b_estimates_new, self.discounted_reward(train_rewards)).numpy()\n",
    "            #print('Weewoo')\n",
    "            #print(tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2))\n",
    "            critic_loss = tf.reduce_mean((np.array(train_rewards) - tf.convert_to_tensor(b_estimates_new, dtype=tf.float32)) ** 2)\n",
    "            actor_loss = self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            #actor_loss = self.actor_loss_fun(actions, train_logits, logits_new, train_rewards, b_estimates_new, clip_param)\n",
    "            #actor_loss = tf.convert_to_tensor(actor_loss)\n",
    "            #critic_loss = tf.cast(critic_loss, dtype=tf.float32)\n",
    "            #print(f'Critics loss:{type(critic_loss)}. Actor Loss {actor_loss.dtype}')\n",
    "            \n",
    "            print(\"gradient_fun\")\n",
    "            print(type(actor_loss))\n",
    "           # print('Actor weights')\n",
    "           # print(self.critic.layers[0].weights)\n",
    "\n",
    "            #a_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            c_gradients = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            \n",
    "            #actor_loss = 0\n",
    "            #print(tape)\n",
    "            #print('Actor loss')\n",
    "            #print(actor_loss)\n",
    "            #print('Trainable Weights')\n",
    "            #print(self.actor.trainable_weights)\n",
    "        \n",
    "        #print(f'Gradients Actor: {a_gradients}. Gradients Critic: {c_gradients}')\n",
    "\n",
    "        # Update parameters\n",
    "        #optimizer.apply_gradients(zip(a_gradients, self.actor.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(c_gradients, self.critic.trainable_variables))\n",
    "\n",
    "        #print(\"updated weights\")\n",
    "        #print(self.critic.layers[0].weights)\n",
    "\n",
    "        \n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "\n",
    "    def actor_loss_fun(self, actions, logits_old, logits_new, rewards, b_estimates_new, clip_param):\n",
    "        '''\n",
    "        Computes loss for Actor Network output.\n",
    "\n",
    "        Args:\n",
    "        logits_old():\n",
    "        logits_new():\n",
    "        reward():\n",
    "        b_estimates_new():\n",
    "        clip_param():\n",
    "        '''\n",
    "        \n",
    "        ratio = self.get_ratio_episode(actions, logits_old, logits_new)\n",
    "\n",
    "        ### FIND OUT WHICH: SINGLE OR MULTIPLE ELEMENTS ARE WANTED AND ADJUST EITHER IN GET_ADV OR THE UPPER TWO FUNCTIONS\n",
    "        advantage = self.get_advantage(rewards, b_estimates_new)\n",
    "        \n",
    "        # Unclipped value\n",
    "        l1 = ratio * advantage\n",
    "        # Clipped ratio between values determined by Hyperparam and multiplied by advantage (see objective function)\n",
    "        l2 = np.clip(ratio, a_min=1 - clip_param, a_max=1 + clip_param) * advantage\n",
    "        l1 = tf.convert_to_tensor(l1)\n",
    "        l2 = tf.convert_to_tensor(l2)\n",
    "        \n",
    "        # Compute minimum of both and take the mean to return float loss\n",
    "        actor_loss = -tf.reduce_mean(tf.minimum(l1, l2))\n",
    "        print(\"loss_fun\")\n",
    "        #print(type(tf.convert_to_tensor(np.array(actor_loss), dtype=tf.float32)))\n",
    "        print(np.array(actor_loss))\n",
    "        return tf.convert_to_tensor(np.array(actor_loss), dtype=tf.float32)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    def get_advantage(self, rewards, b_estimates, gamma = 0.99):\n",
    "        '''\n",
    "        Computes Advantage for action in state.\n",
    "\n",
    "        Args:\n",
    "        rewards(float): Reward for action.\n",
    "        gamma(float): Discount factor.\n",
    "        b_estimates(float): Baseline Estimates.\n",
    "        \n",
    "        '''\n",
    "        # Saves list of all rewards in new variable \n",
    "        #rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "        # Get discounted sum of rewards \n",
    "        disc_sum = self.discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "        # # Estimated Value of the current situtation from the critics network\n",
    "        # b_estimates = self.episodes[0][4] \n",
    "\n",
    "        # Convert lists to np arrays and flatten\n",
    "        disc_sum_np = np.array(disc_sum)\n",
    "        b_estimates_np = np.array(b_estimates)\n",
    "        b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "        # substract arrays to obtain advantages\n",
    "        advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "\n",
    "     ### MIGHT NOT WORK\n",
    "    #  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "    #  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "    #  ###\n",
    "    def discounted_reward(self, rewards, gamma = 0.99):\n",
    "        '''\n",
    "        weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "        Args:\n",
    "        rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "        gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "        '''\n",
    "        # To select the next reward\n",
    "        i = 0\n",
    "        discounted_rewards = []\n",
    "\n",
    "        # Iterates through every reward and appends a discounted version to the output\n",
    "        for r in rewards:\n",
    "            disc = 0\n",
    "            for t in rewards[i:-1]:\n",
    "                discount_t = gamma ** t\n",
    "                disc += t * discount_t\n",
    "            i += 1\n",
    "            discounted_rewards.append(disc)\n",
    "\n",
    "        # returns list of discounted rewards.\n",
    "        return discounted_rewards   \n",
    "\n",
    "\n",
    "\n",
    "    ## get ratio lutsch noch ARSCH, das Ding verarscht mich anders\n",
    "\n",
    "    def get_ratio_episode(self, actions, logits_old, logits_new): \n",
    "        r = []\n",
    "        for a, o, n in zip(actions, logits_old, logits_new):\n",
    "            o = tf.convert_to_tensor(o)\n",
    "            n = tf.convert_to_tensor(n)\n",
    "            #print(f'A: {a} O: {type(o)} N: {type(n)}')\n",
    "\n",
    "            #get the Logarithmic version of all logits for computational efficiency\n",
    "            log_prob_old = tf.nn.log_softmax(o)\n",
    "            log_prob_new = tf.nn.log_softmax(n)\n",
    "\n",
    "            # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "            logprobability_old = tf.reduce_sum(\n",
    "                tf.one_hot(a, num_actions) * log_prob_old, axis=1\n",
    "            )\n",
    "            logprobability_new = tf.reduce_sum(\n",
    "                tf.one_hot(a, num_actions) * log_prob_new, axis=1\n",
    "            )\n",
    "            # get the ratio of new over old prob\n",
    "            ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "            r.append(ratio)\n",
    "        return r\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        c_loss = []\n",
    "        a_loss = []\n",
    "        save_epochs = []\n",
    "\n",
    "        for epoch in tqdm(range(epochs), desc=str(epochs)):\n",
    "            self.collect_train_data()\n",
    "            data, _ = self.storage.get_episodes()\n",
    "            #print(data)\n",
    "            a, c = self.update_policy(data, optimizer, clip_ratio)\n",
    "            a_loss.append(a)\n",
    "            c_loss.append(c)\n",
    "            save_epochs.append(epoch)\n",
    "\n",
    "        return a_loss, c_loss, save_epochs\n",
    "\n",
    "\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 277.34it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "1:   0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_fun\n",
      "17.674249368990846\n",
      "gradient_fun\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'actor/dense/kernel:0' shape=(8, 128) dtype=float32, numpy=\narray([[-0.00540234,  0.00503504, -0.00397553, ..., -0.00462557,\n        -0.00591265,  0.02421263],\n       [ 0.00168411,  0.00219151, -0.0221698 , ..., -0.00685579,\n        -0.00442842,  0.00893392],\n       [-0.0008146 , -0.00633603,  0.00081152, ..., -0.00448825,\n         0.00757394,  0.01312999],\n       ...,\n       [ 0.00897591, -0.00092317, -0.01236722, ...,  0.00358165,\n         0.0049873 ,  0.01273157],\n       [ 0.00232917,  0.01662414,  0.00691048, ...,  0.00230937,\n         0.01516791,  0.00615546],\n       [-0.00347868, -0.01057736, -0.01041433, ..., -0.00336184,\n        -0.00307776,  0.00679514]], dtype=float32)>), (None, <tf.Variable 'actor/dense/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\narray([[-0.00181471,  0.00046002, -0.00394883, ..., -0.01144408,\n        -0.00889392, -0.00744953],\n       [-0.00879102, -0.0114245 , -0.01561856, ..., -0.01409598,\n         0.00270666, -0.00794675],\n       [ 0.0146388 ,  0.01565737, -0.00444501, ...,  0.00061687,\n         0.00112723, -0.00103347],\n       ...,\n       [ 0.01030375, -0.00128284, -0.02208928, ...,  0.00526952,\n        -0.00614633,  0.01031822],\n       [ 0.00018099, -0.00272611,  0.01435435, ...,  0.00870736,\n         0.01108824,  0.00820426],\n       [-0.01041555, -0.0084445 , -0.00974746, ...,  0.01206642,\n        -0.01883854, -0.01615048]], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[-1.09523432e-02,  4.70806984e-03,  7.09796464e-03, ...,\n         1.40006014e-03,  3.79924290e-03,  2.05443054e-03],\n       [ 1.07491072e-02, -7.25436769e-03, -1.21000791e-02, ...,\n         1.30753489e-02, -1.21214315e-02, -4.26769536e-03],\n       [ 6.49184873e-03, -2.41763308e-03,  8.49148352e-03, ...,\n        -2.85155363e-02, -3.89457028e-03,  1.21120000e-02],\n       ...,\n       [ 1.90021992e-02, -4.86763820e-05, -1.64333195e-03, ...,\n         8.27024691e-03,  1.41660161e-02,  3.21729574e-03],\n       [-1.17920861e-02, -2.36707600e-03, -3.88719083e-04, ...,\n         1.42745010e-03, -5.17856702e-03, -3.52891535e-03],\n       [ 2.00320361e-03,  4.54213005e-03,  2.70186737e-03, ...,\n         1.54422307e-02, -2.54795537e-03,  5.33705624e-03]], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\narray([[ 0.10019085,  0.26521456,  0.10242036, -0.12537979],\n       [-0.03524053,  0.12316877, -0.07485355,  0.11988726],\n       [ 0.04753539, -0.06178357,  0.18550104,  0.06659004],\n       [-0.17818199, -0.21180172, -0.25707546, -0.13141543],\n       [ 0.2916727 ,  0.22136998,  0.13578764,  0.29403394],\n       [-0.03515705,  0.05960038,  0.21002966,  0.05615267],\n       [ 0.20811003,  0.05324852, -0.2395409 ,  0.11401558],\n       [-0.28650543, -0.26055574, -0.18886843, -0.24259748],\n       [-0.13776734,  0.24908608,  0.10414386,  0.2849648 ],\n       [ 0.10801578, -0.2896709 , -0.01962474, -0.24921958],\n       [-0.06124009,  0.09348625,  0.1074993 ,  0.02223209],\n       [-0.24522507, -0.24593002,  0.2188201 ,  0.28615296],\n       [-0.01250002, -0.21772787, -0.29578006, -0.0722232 ],\n       [-0.06776956,  0.16380182,  0.12931445, -0.21060711],\n       [-0.16907415, -0.05005662, -0.16986237, -0.27887213],\n       [-0.07729758, -0.00632033,  0.08661166, -0.10217781],\n       [-0.04928248,  0.11700663, -0.26074654,  0.07165664],\n       [-0.27479434,  0.17639944,  0.2460553 ,  0.23038161],\n       [ 0.01591706, -0.05723694,  0.03052974, -0.00324976],\n       [-0.15525274, -0.13775864, -0.10672189, -0.2573229 ],\n       [-0.09051545, -0.01326051,  0.09552008, -0.28087127],\n       [-0.12062664,  0.26510602,  0.16747132,  0.15986452],\n       [-0.14615998, -0.23450094,  0.04131392,  0.2441178 ],\n       [ 0.16423157,  0.11882815, -0.19333687,  0.13138887],\n       [ 0.11745301,  0.11219111, -0.09304574, -0.12135121],\n       [-0.04947837,  0.18959534, -0.06760716,  0.05848396],\n       [ 0.18167734, -0.1683485 , -0.22654846,  0.29406047],\n       [-0.19773422, -0.21731561, -0.01272631, -0.05474482],\n       [-0.06799328, -0.29387137, -0.22914794,  0.04203963],\n       [ 0.2026591 , -0.09881304,  0.26556903,  0.12053812],\n       [-0.13360585, -0.1742622 , -0.26650888,  0.13277128],\n       [ 0.25753957,  0.1612364 ,  0.14067405,  0.07968518],\n       [-0.21761484, -0.14705111,  0.03937584, -0.08019558],\n       [-0.29118535, -0.12227471,  0.03761998, -0.13383155],\n       [ 0.01602522,  0.06648189,  0.00551185, -0.27443486],\n       [ 0.21630871,  0.12444106,  0.08403394,  0.21860462],\n       [ 0.2925511 ,  0.06447589, -0.2122508 , -0.19275849],\n       [-0.21858141,  0.17051485,  0.18435636, -0.1318497 ],\n       [ 0.25399786,  0.01577881,  0.11870018,  0.2092517 ],\n       [-0.12461323,  0.22905809,  0.02524012, -0.2577904 ],\n       [ 0.11325249,  0.05704927, -0.18438074,  0.10700837],\n       [-0.17039998,  0.25746214,  0.2213614 ,  0.11570764],\n       [-0.08480884,  0.27224904,  0.1564329 , -0.18379462],\n       [-0.09342675,  0.14960486,  0.25265092,  0.2426433 ],\n       [-0.18687803, -0.25093842,  0.25302124,  0.0559465 ],\n       [ 0.19995785,  0.08335134,  0.28469396, -0.22185978],\n       [ 0.11453846, -0.18357316,  0.25201792,  0.19564697],\n       [-0.26780993,  0.18415198,  0.15758312,  0.22869569],\n       [-0.14326121,  0.05526415,  0.23584718, -0.20082363],\n       [ 0.12953654,  0.09828642, -0.25494877,  0.11612895],\n       [-0.233812  ,  0.02414134, -0.14084458,  0.0164344 ],\n       [-0.29199284, -0.18788296,  0.07764602,  0.07311583],\n       [ 0.22685993, -0.1112511 , -0.00064561,  0.23306245],\n       [ 0.09487113, -0.27447337, -0.01611161, -0.18587357],\n       [ 0.05438673,  0.00916237,  0.18493044, -0.06180233],\n       [ 0.28093153, -0.22755796,  0.07602295,  0.20633197],\n       [-0.02245906, -0.20460689, -0.03476879,  0.17667252],\n       [-0.05727731,  0.06297761,  0.05765486,  0.29494208],\n       [-0.03085488, -0.22136106,  0.16634336,  0.13138065],\n       [-0.19744909,  0.23656225,  0.23342359,  0.20696998],\n       [-0.21084295,  0.13469201,  0.19871047, -0.13744639],\n       [-0.02686024, -0.13753031, -0.18523604, -0.16640987],\n       [-0.19464374,  0.08038983, -0.22795442,  0.17523634],\n       [ 0.2520337 , -0.07866251, -0.23814398, -0.03437087]],\n      dtype=float32)>), (None, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2572/448988756.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mppo_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# ppo_agent.collect_train_data()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# data = storage.get_episodes()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# #print(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2572/1955199920.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_episodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m#print(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m             \u001b[0ma_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[0mc_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2572/1955199920.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[1;34m(self, episodes, optimizer, clip_param, c_1, c_2)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m# Compute train losses and action by chosen by policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             actor_loss, critic_loss = self.train_step(\n\u001b[0m\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# States\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mepisode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2572/1955199920.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, states, actions, optimizer, train_logits, train_rewards, clip_param, c_1, c_2)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m     \"\"\"\n\u001b[1;32m--> 633\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\u001b[0m\u001b[0;32m     74\u001b[0m                      f\"Provided `grads_and_vars` is {grads_and_vars}.\")\n\u001b[0;32m     75\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['actor/dense/kernel:0', 'actor/dense/bias:0', 'actor/dense_1/kernel:0', 'actor/dense_1/bias:0', 'actor/dense_2/kernel:0', 'actor/dense_2/bias:0', 'actor/dense_3/kernel:0', 'actor/dense_3/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'actor/dense/kernel:0' shape=(8, 128) dtype=float32, numpy=\narray([[-0.00540234,  0.00503504, -0.00397553, ..., -0.00462557,\n        -0.00591265,  0.02421263],\n       [ 0.00168411,  0.00219151, -0.0221698 , ..., -0.00685579,\n        -0.00442842,  0.00893392],\n       [-0.0008146 , -0.00633603,  0.00081152, ..., -0.00448825,\n         0.00757394,  0.01312999],\n       ...,\n       [ 0.00897591, -0.00092317, -0.01236722, ...,  0.00358165,\n         0.0049873 ,  0.01273157],\n       [ 0.00232917,  0.01662414,  0.00691048, ...,  0.00230937,\n         0.01516791,  0.00615546],\n       [-0.00347868, -0.01057736, -0.01041433, ..., -0.00336184,\n        -0.00307776,  0.00679514]], dtype=float32)>), (None, <tf.Variable 'actor/dense/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/kernel:0' shape=(128, 128) dtype=float32, numpy=\narray([[-0.00181471,  0.00046002, -0.00394883, ..., -0.01144408,\n        -0.00889392, -0.00744953],\n       [-0.00879102, -0.0114245 , -0.01561856, ..., -0.01409598,\n         0.00270666, -0.00794675],\n       [ 0.0146388 ,  0.01565737, -0.00444501, ...,  0.00061687,\n         0.00112723, -0.00103347],\n       ...,\n       [ 0.01030375, -0.00128284, -0.02208928, ...,  0.00526952,\n        -0.00614633,  0.01031822],\n       [ 0.00018099, -0.00272611,  0.01435435, ...,  0.00870736,\n         0.01108824,  0.00820426],\n       [-0.01041555, -0.0084445 , -0.00974746, ...,  0.01206642,\n        -0.01883854, -0.01615048]], dtype=float32)>), (None, <tf.Variable 'actor/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[-1.09523432e-02,  4.70806984e-03,  7.09796464e-03, ...,\n         1.40006014e-03,  3.79924290e-03,  2.05443054e-03],\n       [ 1.07491072e-02, -7.25436769e-03, -1.21000791e-02, ...,\n         1.30753489e-02, -1.21214315e-02, -4.26769536e-03],\n       [ 6.49184873e-03, -2.41763308e-03,  8.49148352e-03, ...,\n        -2.85155363e-02, -3.89457028e-03,  1.21120000e-02],\n       ...,\n       [ 1.90021992e-02, -4.86763820e-05, -1.64333195e-03, ...,\n         8.27024691e-03,  1.41660161e-02,  3.21729574e-03],\n       [-1.17920861e-02, -2.36707600e-03, -3.88719083e-04, ...,\n         1.42745010e-03, -5.17856702e-03, -3.52891535e-03],\n       [ 2.00320361e-03,  4.54213005e-03,  2.70186737e-03, ...,\n         1.54422307e-02, -2.54795537e-03,  5.33705624e-03]], dtype=float32)>), (None, <tf.Variable 'actor/dense_2/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor/dense_3/kernel:0' shape=(64, 4) dtype=float32, numpy=\narray([[ 0.10019085,  0.26521456,  0.10242036, -0.12537979],\n       [-0.03524053,  0.12316877, -0.07485355,  0.11988726],\n       [ 0.04753539, -0.06178357,  0.18550104,  0.06659004],\n       [-0.17818199, -0.21180172, -0.25707546, -0.13141543],\n       [ 0.2916727 ,  0.22136998,  0.13578764,  0.29403394],\n       [-0.03515705,  0.05960038,  0.21002966,  0.05615267],\n       [ 0.20811003,  0.05324852, -0.2395409 ,  0.11401558],\n       [-0.28650543, -0.26055574, -0.18886843, -0.24259748],\n       [-0.13776734,  0.24908608,  0.10414386,  0.2849648 ],\n       [ 0.10801578, -0.2896709 , -0.01962474, -0.24921958],\n       [-0.06124009,  0.09348625,  0.1074993 ,  0.02223209],\n       [-0.24522507, -0.24593002,  0.2188201 ,  0.28615296],\n       [-0.01250002, -0.21772787, -0.29578006, -0.0722232 ],\n       [-0.06776956,  0.16380182,  0.12931445, -0.21060711],\n       [-0.16907415, -0.05005662, -0.16986237, -0.27887213],\n       [-0.07729758, -0.00632033,  0.08661166, -0.10217781],\n       [-0.04928248,  0.11700663, -0.26074654,  0.07165664],\n       [-0.27479434,  0.17639944,  0.2460553 ,  0.23038161],\n       [ 0.01591706, -0.05723694,  0.03052974, -0.00324976],\n       [-0.15525274, -0.13775864, -0.10672189, -0.2573229 ],\n       [-0.09051545, -0.01326051,  0.09552008, -0.28087127],\n       [-0.12062664,  0.26510602,  0.16747132,  0.15986452],\n       [-0.14615998, -0.23450094,  0.04131392,  0.2441178 ],\n       [ 0.16423157,  0.11882815, -0.19333687,  0.13138887],\n       [ 0.11745301,  0.11219111, -0.09304574, -0.12135121],\n       [-0.04947837,  0.18959534, -0.06760716,  0.05848396],\n       [ 0.18167734, -0.1683485 , -0.22654846,  0.29406047],\n       [-0.19773422, -0.21731561, -0.01272631, -0.05474482],\n       [-0.06799328, -0.29387137, -0.22914794,  0.04203963],\n       [ 0.2026591 , -0.09881304,  0.26556903,  0.12053812],\n       [-0.13360585, -0.1742622 , -0.26650888,  0.13277128],\n       [ 0.25753957,  0.1612364 ,  0.14067405,  0.07968518],\n       [-0.21761484, -0.14705111,  0.03937584, -0.08019558],\n       [-0.29118535, -0.12227471,  0.03761998, -0.13383155],\n       [ 0.01602522,  0.06648189,  0.00551185, -0.27443486],\n       [ 0.21630871,  0.12444106,  0.08403394,  0.21860462],\n       [ 0.2925511 ,  0.06447589, -0.2122508 , -0.19275849],\n       [-0.21858141,  0.17051485,  0.18435636, -0.1318497 ],\n       [ 0.25399786,  0.01577881,  0.11870018,  0.2092517 ],\n       [-0.12461323,  0.22905809,  0.02524012, -0.2577904 ],\n       [ 0.11325249,  0.05704927, -0.18438074,  0.10700837],\n       [-0.17039998,  0.25746214,  0.2213614 ,  0.11570764],\n       [-0.08480884,  0.27224904,  0.1564329 , -0.18379462],\n       [-0.09342675,  0.14960486,  0.25265092,  0.2426433 ],\n       [-0.18687803, -0.25093842,  0.25302124,  0.0559465 ],\n       [ 0.19995785,  0.08335134,  0.28469396, -0.22185978],\n       [ 0.11453846, -0.18357316,  0.25201792,  0.19564697],\n       [-0.26780993,  0.18415198,  0.15758312,  0.22869569],\n       [-0.14326121,  0.05526415,  0.23584718, -0.20082363],\n       [ 0.12953654,  0.09828642, -0.25494877,  0.11612895],\n       [-0.233812  ,  0.02414134, -0.14084458,  0.0164344 ],\n       [-0.29199284, -0.18788296,  0.07764602,  0.07311583],\n       [ 0.22685993, -0.1112511 , -0.00064561,  0.23306245],\n       [ 0.09487113, -0.27447337, -0.01611161, -0.18587357],\n       [ 0.05438673,  0.00916237,  0.18493044, -0.06180233],\n       [ 0.28093153, -0.22755796,  0.07602295,  0.20633197],\n       [-0.02245906, -0.20460689, -0.03476879,  0.17667252],\n       [-0.05727731,  0.06297761,  0.05765486,  0.29494208],\n       [-0.03085488, -0.22136106,  0.16634336,  0.13138065],\n       [-0.19744909,  0.23656225,  0.23342359,  0.20696998],\n       [-0.21084295,  0.13469201,  0.19871047, -0.13744639],\n       [-0.02686024, -0.13753031, -0.18523604, -0.16640987],\n       [-0.19464374,  0.08038983, -0.22795442,  0.17523634],\n       [ 0.2520337 , -0.07866251, -0.23814398, -0.03437087]],\n      dtype=float32)>), (None, <tf.Variable 'actor/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "\n",
    "ppo_agent = Agent()\n",
    "actor_loss, critic_loss, save_epochs = ppo_agent.run()\n",
    "# ppo_agent.collect_train_data()\n",
    "# data = storage.get_episodes()\n",
    "# #print(data)\n",
    "# print(ppo_agent.update_policy(data[0], actor, critic, optimizer, clip_ratio))\n",
    "\n",
    "plot = 1\n",
    "if plot:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('Critic & Actor Loss')\n",
    "    axs[0].plot(save_epochs, critic_loss)\n",
    "    # For actor loss later\n",
    "    axs[1].plot(save_epochs, critic_loss)\n",
    "'''\n",
    "Nikis idea what the problem is:\n",
    "- Actor_loss needs to be eager tensor dtpye float32 shit\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
