{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self):\n",
    "        # init creating a storage for all observation variables during trajectory\n",
    "        self.observations = []\n",
    "        # create arrays for chosen actions and rewards\n",
    "        self.actions = []\n",
    "        self.logits = []\n",
    "        self.rewards = []\n",
    "        self.BaselineEstimate = []\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self,observation, action, logits, reward, BaselineEstimate):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) # value of critics network\n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        # append all already stored values to finished episodes\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate, \n",
    "             sum(self.rewards)]) # get the return of the whole episode\n",
    "             \n",
    "        # empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        # create arrays for chosen actions and rewards\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "\n",
    "    # return array of finished trajectories stored in self.episodes and the amount of episodes\n",
    "    def get_episodes(self):\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "\n",
    "        self.l = [\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(4, activation=\"softmax\")\n",
    "        ]\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "#@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "   # tf.print(type(logits))\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "   # tf.print(action)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Hyperparameters\n",
    "epochs = 1\n",
    "steps_per_epoch = 1000 # ~10 Episodes per epoch, then compute new parameters (smaller batching)\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "clip_ratio = 0.2\n",
    "target_kl = 0.01\n",
    "optimizer = Adam()\n",
    "\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 12:37:46.563900: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-04 12:37:46.564041: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# define environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# get observation_dims and amount of possible actions (1 for CartPole-v1)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage for observations, actions, rewards etc during trajectory\n",
    "T = Storage()\n",
    "\n",
    "# init the actor and critics model\n",
    "observation_input = Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 289.34it/s]\n"
     ]
    }
   ],
   "source": [
    "episodes_total = 0\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        observation = observation.reshape(1,-1)\n",
    "\n",
    "        # obtain action and logits for this observation by our actor\n",
    "        logits, action = sample_action(observation=observation)\n",
    "        \n",
    "        # make a step in environment and obtain the rewards for it\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "        # sum up rewards over this episode and count amount of frames\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # get the Base-Estimate from the Critics network\n",
    "        base_estimate = critic(observation)\n",
    "\n",
    "        # store Variables collected in this step\n",
    "        T.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "        #update the observations\n",
    "        observation = observation_new\n",
    "        # check if terminal state is reached in env, if so save episode and refresh storage, reset env\n",
    "        if done:\n",
    "            T.conclude_episode()\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # obtain all episodes saved in storage\n",
    "    episodes, amount_episodes = T.get_episodes()\n",
    "\n",
    "  \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Episodes = 11\n",
      "[-0.4264440701853971, -1.5051301517936964, -0.8757260133700584, -0.5711668046340606, -0.6118232836923028, -1.6563027111441204, -2.238994327738594, -0.06036102388392692, -2.4638427059447165, -0.9674875484260212, -1.989685212016069, -0.042337950427308896, -2.5407700434440685, -2.672685094051586, -0.6773556384971176, -1.9671823165295337, -0.11061789709731215, -0.3620920355987767, -0.06086519084712336, -2.2888636484981633, -0.17861734669699444, 0.12173367041177244, -0.9404595347056954, -2.3109151863718, -1.5396380127457292, -2.6744072578578355, -0.7022746271788833, -0.58683251490251, -2.5657968516759113, -1.587873237786198, -0.3701425921136934, -0.31221755825171615, 0.0735138751271063, -1.0249967457178002, -1.9926045140670385, -0.024850375548622877, -1.003259948603727, -0.02721960076485061, 0.24028436410896006, -0.6333837462898089, -1.9373512580493457, -1.2214220652484926, -0.7644795585776194, -1.8295434699770567, -1.63987780976355, -0.5757665794678815, -0.5728373252731558, -1.6451459206845402, -0.7543756967084505, -0.2761710475093537, 0.46675940514549663, 0.1008890761099906, 0.6621777626891412, 0.5684900669366766, 1.6651585541961935, 0.9762537307915136, 0.01362156146626603, -1.1610461038504514, 0.5954251835284776, -0.04319401684813329, 0.188310982642588, 0.9281731963963875, 0.1777286960212905, 1.2126425994261762, 1.3195033561300147, 1.4871962709428044, 1.5791768222601104, 1.6157704350706876, -0.12593934949754157, -0.05683005510647263, 1.4273675323393331, 0.7710029679377044, -0.027958605063786307, -0.1440877983575308, 1.1504605535717485, 2.7318659022702265, 3.2369511829970294, 0.309565498389901]\n"
     ]
    }
   ],
   "source": [
    "# episodes [episode][particular values // 0: Observations, 1: actions, 2: logits, 3, rewards, 4: BaselineEstimates from Critics]\n",
    "#print(episodes[0][4])\n",
    "print(f'Number of Episodes = {amount_episodes}')\n",
    "\n",
    "\n",
    "\n",
    "### Advantagefunction\n",
    "\n",
    "# estimated Value of the current situtation from the critics network\n",
    "b_estimates = episodes[0][4] \n",
    "# for i in b_estimates:\n",
    "#   print(i.numpy())\n",
    "\n",
    "# Discounted sum of rewards\n",
    "print(episodes[0][3]) \n",
    "rewards = episodes[0][3]\n",
    "gamma = 0.99\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward(rewards, gamma):\n",
    "    i = 0\n",
    "    discounted_rewards = []\n",
    "    for r in rewards:\n",
    "        disc = 0\n",
    "        for t in rewards[i:-1]:\n",
    "            discount_t = gamma ** t\n",
    "            disc += t * discount_t\n",
    "        i += 1\n",
    "        discounted_rewards.append(disc)\n",
    "    return discounted_rewards\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.33844649e+01 -3.29561888e+01 -3.14281182e+01 -3.05446514e+01\n",
      " -2.99701960e+01 -2.93545985e+01 -2.76704920e+01 -2.53805426e+01\n",
      " -2.53201444e+01 -2.27945299e+01 -2.18175885e+01 -1.97877132e+01\n",
      " -1.97453579e+01 -1.71388695e+01 -1.43934193e+01 -1.37114347e+01\n",
      " -1.17049739e+01 -1.15942338e+01 -1.12308213e+01 -1.11699179e+01\n",
      " -8.82778895e+00 -8.64884978e+00 -8.77043395e+00 -7.82104144e+00\n",
      " -5.45582340e+00 -3.89217724e+00 -1.14490734e+00 -4.37659831e-01\n",
      "  1.52645697e-01  2.78546998e+00  4.39888832e+00  4.77041108e+00\n",
      "  5.08361042e+00  5.01015137e+00  6.04576325e+00  8.07867514e+00\n",
      "  8.10353345e+00  9.11696173e+00  9.14419087e+00  8.90448750e+00\n",
      "  9.54191713e+00  1.15173614e+01  1.27538679e+01  1.35242456e+01\n",
      "  1.53877402e+01  1.70548694e+01  1.76339792e+01  1.82101259e+01\n",
      "  1.98827011e+01  2.06428202e+01  2.09197574e+01  2.04551855e+01\n",
      "  2.03543975e+01  1.96966149e+01  1.91313666e+01  1.74938412e+01\n",
      "  1.65271210e+01  1.65135034e+01  1.76881785e+01  1.70963082e+01\n",
      "  1.71395228e+01  1.69515645e+01  1.60320118e+01  1.58546017e+01\n",
      "  1.46566507e+01  1.33545325e+01  1.18894029e+01  1.03350941e+01\n",
      "  8.74535210e+00  8.87145185e+00  8.92831488e+00  7.52127974e+00\n",
      "  6.75623000e+00  6.78419679e+00  6.92849328e+00  5.79126048e+00\n",
      "  3.13338156e+00  4.40497752e-05]\n"
     ]
    }
   ],
   "source": [
    "# get discounted sum of rewards \n",
    "disc_sum = discounted_reward(rewards, gamma)\n",
    "\n",
    "# convert lists to np arrays and flatten\n",
    "disc_sum_np = np.array(disc_sum)\n",
    "b_estimates_np = np.array(b_estimates)\n",
    "b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "# substract arrays to obtain advantages\n",
    "advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "print(advantages)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogProbs and Ratio Computation\n",
    "\n",
    "We need the ratio of probabilities for an action at state t of the 'new' model vs the old model (maybe because of entropy there is a difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 6\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_3107/2711087504.py:72: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array_ops =  np.array(batch_obs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#creating oneHot vector with size actions space and getting the log for the probability of choosing this action\n",
    "\n",
    "# print(f'log old: {logits_old}')\n",
    "# print(f'log new: {logits_new}')\n",
    "\n",
    "\n",
    "### this function currently only takes one single action and 2 sets of logits and computes the ratio of that\n",
    "\n",
    "# def get_ratio(action, logits_old, logits_new):\n",
    "\n",
    "#     #get the Logarithmic version of all logits for computational efficiency\n",
    "#     log_prob_old = tf.nn.log_softmax(logits_old)\n",
    "#     log_prob_new = tf.nn.log_softmax(logits_new)\n",
    "\n",
    "#     # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "#     logprobability_old = tf.reduce_sum(\n",
    "#         tf.one_hot(action, num_actions) * log_prob_old, axis=1\n",
    "#     )\n",
    "#     logprobability_new = tf.reduce_sum(\n",
    "#         tf.one_hot(action, num_actions) * log_prob_new, axis=1\n",
    "#     )\n",
    "#     # get the ratio of new over old prob\n",
    "#     ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "\n",
    "#     print(ratio)\n",
    "\n",
    "\n",
    "\n",
    "def get_ratio_episode(actions, logits_old, logits_new):\n",
    "    r = []\n",
    "    for a, o, n in zip(actions, logits_old, logits_new):\n",
    "        #get the Logarithmic version of all logits for computational efficiency\n",
    "        log_prob_old = tf.nn.log_softmax(o)\n",
    "        log_prob_new = tf.nn.log_softmax(n)\n",
    "\n",
    "        # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "        logprobability_old = tf.reduce_sum(\n",
    "            tf.one_hot(a, num_actions) * log_prob_old, axis=1\n",
    "        )\n",
    "        logprobability_new = tf.reduce_sum(\n",
    "            tf.one_hot(a, num_actions) * log_prob_new, axis=1\n",
    "        )\n",
    "        # get the ratio of new over old prob\n",
    "        ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "        r.append(ratio)\n",
    "    return r\n",
    "        \n",
    "\n",
    "\n",
    "## second test with multiple actions taken by the agent\n",
    "obs = episodes[0][0]\n",
    "logits_new = []\n",
    "for i in obs:\n",
    "    tensor = tf.convert_to_tensor(i)\n",
    "    new, _ = sample_action(tensor)\n",
    "    logits_new.append(new) \n",
    "\n",
    "episode_actions = episodes[0][1]\n",
    "episode_logits_old = episodes[0][2]\n",
    "# print(len(logits_new), len(episode_logits_old), len(episode_actions))\n",
    "\n",
    "\n",
    "# episode_1 = get_ratio_episode(episode_actions, episode_logits_old, logits_new)\n",
    "# print(len(episode_1))\n",
    "# print(f'Length fo Episode: {len(episode_1)}. Episode Ratios: {episode_1}')\n",
    "\n",
    "batch_obs = episodes[:][0]\n",
    "print(type(batch_obs), len(batch_obs))\n",
    "nones = np.full_like(batch_obs,None)\n",
    "#print(nones)\n",
    "\n",
    "array_ops =  np.array(batch_obs)\n",
    "for i in range(len(array_ops)):\n",
    "    nones[i] = episodes[i][0]\n",
    "print(type(nones))\n",
    "\n",
    "array_ops = np.array(array_ops)\n",
    "#print(f'Obs_episode_array: {array_ops}')\n",
    "def get_ratio_batch(actions, logits_old, logits_new):\n",
    "    print('bla')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
