{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "class Trajectory_Storage:\n",
    "    # T for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # T initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "\n",
    "        self.l = [\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(2, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.02))\n",
    "        ]\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "#@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "   # tf.print(type(logits))\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "   # tf.print(action)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.02))\n",
    "        ]\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Policy Function\n",
    "\n",
    "Training the Actor Model Using the typical PPO-Clipping Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    # print(\"Policy grads: \")\n",
    "    # print(policy_grads)\n",
    "    # print(\"Actor Variables:\")\n",
    "    # print([actor.trainable_variables])\n",
    "    # print(type(policy_grads), type(actor.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "        \n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    optimizer_2.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Hyperparameters\n",
    "epochs = 10\n",
    "steps_per_epoch = 1000\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "clip_ratio = 0.2\n",
    "target_kl = 0.01\n",
    "optimizer = Adam()\n",
    "optimizer_2 = Adam()\n",
    "\n",
    "render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# define environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# get observation_dims and amount of possible actions (1 for CartPole-v1)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage for observations, actions, rewards etc during trajectory\n",
    "T = Trajectory_Storage(observation_dimensions=observation_dimensions, size=steps_per_epoch)\n",
    "\n",
    "# init the actor and critics model\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "mean_returns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 336.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "[<tf.Variable 'critic/dense_3/kernel:0' shape=(4, 64) dtype=float32>, <tf.Variable 'critic/dense_3/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'critic/dense_4/kernel:0' shape=(64, 64) dtype=float32>, <tf.Variable 'critic/dense_4/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'critic/dense_5/kernel:0' shape=(64, 1) dtype=float32>, <tf.Variable 'critic/dense_5/bias:0' shape=(1,) dtype=float32>]\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "[<tf.Variable 'critic/dense_3/kernel:0' shape=(4, 64) dtype=float32>, <tf.Variable 'critic/dense_3/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'critic/dense_4/kernel:0' shape=(64, 64) dtype=float32>, <tf.Variable 'critic/dense_4/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'critic/dense_5/kernel:0' shape=(64, 1) dtype=float32>, <tf.Variable 'critic/dense_5/bias:0' shape=(1,) dtype=float32>]\n",
      " Epoch: 1. Mean Return: 20.833333333333332. Mean Length: 20.833333333333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 402.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 2. Mean Return: 21.27659574468085. Mean Length: 21.27659574468085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 403.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 3. Mean Return: 41.666666666666664. Mean Length: 41.666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 400.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 4. Mean Return: 55.55555555555556. Mean Length: 55.55555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 372.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 5. Mean Return: 100.0. Mean Length: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 362.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 6. Mean Return: 333.3333333333333. Mean Length: 333.3333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 259.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 7. Mean Return: 142.85714285714286. Mean Length: 142.85714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 333.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 8. Mean Return: 200.0. Mean Length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 297.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 9. Mean Return: 166.66666666666666. Mean Length: 166.66666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 364.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 10. Mean Return: 166.66666666666666. Mean Length: 166.66666666666666\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "     # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        #print(observation)\n",
    "        observation = observation.reshape(1, -1)\n",
    "        \n",
    "        #print(observation)\n",
    "        logits, action = sample_action(observation)\n",
    "        #print(logits)\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "        #print(observation)\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        T.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            T.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = T.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )\n",
    "    mean_returns.append(sum_return/num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(epochs), mean_returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [03:15<02:10,  3.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mh:\\FPSSample-master\\bchlr\\IANNWTF-Final\\CartPole_PPO_Custom.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CartPole_PPO_Custom.ipynb#ch0000015?line=12'>13</a>\u001b[0m reward_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CartPole_PPO_Custom.ipynb#ch0000015?line=14'>15</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CartPole_PPO_Custom.ipynb#ch0000015?line=16'>17</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CartPole_PPO_Custom.ipynb#ch0000015?line=18'>19</a>\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/FPSSample-master/bchlr/IANNWTF-Final/CartPole_PPO_Custom.ipynb#ch0000015?line=20'>21</a>\u001b[0m     logits, action \u001b[39m=\u001b[39m sample_action(observation)\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_gym\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/core.py?line=284'>285</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/core.py?line=285'>286</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_gym\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/core.py?line=284'>285</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/core.py?line=285'>286</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_gym\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:260\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/envs/classic_control/cartpole.py?line=257'>258</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/envs/classic_control/cartpole.py?line=258'>259</a>\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/envs/classic_control/cartpole.py?line=259'>260</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/envs/classic_control/cartpole.py?line=260'>261</a>\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    <a href='file:///c%3A/Users/nikla/.conda/envs/env_gym/lib/site-packages/gym/envs/classic_control/cartpole.py?line=262'>263</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test the trained variable \n",
    "\n",
    "test_length = 100\n",
    "passed = []\n",
    "\n",
    "observation= env.reset()\n",
    "reward_sum = 0\n",
    "done \n",
    "for i in tqdm(range(test_length)):\n",
    "    observation = env.reset()\n",
    "    terminal = False\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "\n",
    "    while not terminal:\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "        observation = observation.reshape(1, -1)\n",
    "            \n",
    "        logits, action = sample_action(observation)\n",
    "        \n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        reward_sum += reward\n",
    "        terminal = done \n",
    "        observation = observation_new\n",
    "   \n",
    "   \n",
    "    passed.append(reward_sum)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Average reward over 100 episodes {sum(passed)} / {test_length}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
